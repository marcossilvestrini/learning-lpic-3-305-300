<h1><a name="readme-top"></a></h1>

[![Create Release](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/release.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/release.yml)[![Translate README](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/translate.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/translate.yml)[![Generate HTML and PDF](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/generate-html.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/generate-html.yml)[![Deploy Webpage](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/deploy-webpage.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/deploy-webpage.yml)[![Generate GitBook Docs](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/generate-docs.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/generate-docs.yml)[![PSScriptAnalyzer](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/powershell.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/powershell.yml)[![Slack Notification](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/slack.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/slack.yml)

* * *

[![MIT License][license-shield]][license-url][![Forks][forks-shield]][forks-url][![Stargazers][stars-shield]][stars-url][![Contributors][contributors-shield]][contributors-url][![Issues][issues-shield]][issues-url]

## [![LinkedIn][linkedin-shield]][linkedin-url]

# ğŸ“š Aprendendo Lpic-3 305-300

[![English](https://img.shields.io/badge/lang-English-blue?logo=readme&logoColor=white)](./README.md)[![PortuguÃªs](https://img.shields.io/badge/lang-PortuguÃªs-green?logo=readme&logoColor=white)](README_pt.md)

![LPIC3-305-300](images/lpic3-305-300.jpg)

<p align="center">
<strong>Explore the docs Â»</strong></a>
    <br />
    <a href="https://marcossilvestrini.github.io/learning-lpic-3-305-300/">Web Site</a>
    -
    <a href="https://github.com/marcossilvestrini/learning-lpic-3-305-300">Code Page</a>
    -
    <a href="https://skynet-8.gitbook.io/learning-lpic-3-305-300">Gitbook</a>
    -
    <a href="https://github.com/marcossilvestrini/learning-lpic-3-305-300/issues">Report Bug</a>
    -
    <a href="https://github.com/marcossilvestrini/learning-lpic-3-305-300/issues">Request Feature</a>
</p>

* * *

## Resumo Resumo

<details>
  <summary><b>TABLE OF CONTENT</b></summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#installation">installation</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#roadmap">Roadmap</a></li>
    <li><a href="#freedoms">Four Essential Freedoms</a></li>
    <li>
      <a href="#topic-351">Topic 351: Full Virtualization</a>
      <ul>
        <li><a href="#topic-351.1">351.1 Virtualization Concepts and Theory </a></li>
        <li><a href="#topic-351.2">351.2 Xen</a></li>
        <li><a href="#topic-351.3">351.3 QEMU</a></li>
        <li><a href="#topic-351.4">351.4 Libvirt Virtual Machine</a></li>
        <li><a href="#topic-351.5">351.5 Virtual Machine Disk Image Management</a></li>
      </ul>
    </li>
    <li>
      <a href="#topic-352">Topic 352: Container Virtualization</a>
      <ul>
        <li><a href="#topic-352.1">352.1 Container Virtualization Concepts</a></li>
        <li><a href="#topic-352.2">352.2 LXC</a></li>
        <li><a href="#topic-352.3">352.3 Docker</a></li>
        <li><a href="#topic-352.4">352.4 Container Orchestration Platforms</a></li>
      </ul>
    </li>
    <li>
      <a href="#topic-353">Topic 353: VM Deployment and Provisioning</a>
      <ul>
        <li><a href="#topic-353.1">353.1 Cloud Management Tools</a></li>
        <li><a href="#topic-353.2">353.2 Packer</a></li>
        <li><a href="#topic-353.3">353.3 cloud-init</a></li>
        <li><a href="#topic-353.4">353.4 Vagrant</a></li>
      </ul>
    </li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
    <li><a href="#acknowledgments">Acknowledgments</a></li>
  </ol>
</details><br>

* * *

<a name="about-the-project"></a>

## ğŸ“– Sobre o projeto

> Este projeto tem como objetivo ajudar estudantes ou profissionais a aprender os principais conceitos de gnulinux
> e software livre
> Algumas distribuiÃ§Ãµes de Gnulinux como Debian e RPM serÃ£o cobertas
> InstalaÃ§Ã£o e configuraÃ§Ã£o de alguns pacotes tambÃ©m serÃ£o cobertas
> Ao fazer isso, vocÃª pode dar a toda a comunidade a chance de se beneficiar de suas mudanÃ§as.
> O acesso ao cÃ³digo -fonte Ã© uma condiÃ§Ã£o prÃ©via para isso.
> Use o Vagrant para mÃ¡quinas UP e execute laboratÃ³rios e pratique o conteÃºdo deste artigo.
> Eu publiquei na pasta Vagrant um VagrantFile com o que Ã© necessÃ¡rio
> para vocÃª fazer upload de um ambiente para estudos

* * *

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<a name="getting-started"></a>

## ğŸš€ IntroduÃ§Ã£o

Para iniciar o aprendizado, consulte a documentaÃ§Ã£o acima.

<a name="prerequisites"></a>

### PrÃ© -requisitos

-   [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
-   [VMware WorkStation](https://blogs.vmware.com/workstation/2024/05/vmware-workstation-pro-now-available-free-for-personal-use.html)
-   [UtilitÃ¡rio VMware Vagrant](https://developer.hashicorp.com/vagrant/install/vmware)
-   [Vagabundo](https://developer.hashicorp.com/vagrant/install)

<a name="installation"></a>

### ğŸ’¾ InstalaÃ§Ã£o

Clone o repo

```sh
git clone https://github.com/marcossilvestrini/learning-lpic-3-305-300.git
cd learning-lpic-3-305-300
```

Personalize um modelo_VagrantFile-Topic-xxx_. Este arquivo contÃ©m uma configuraÃ§Ã£o VMS para laboratÃ³rios. Exemplo:

-   Arquivo[VagrantFile-Topic-351](vagrant/Vagrantfile-topic-351)
    -   vm.clone_directory = "&lt;your_driver_letter>:\\`<folder>`\\&lt;para_machine>\\#{Vm_name} -instance-1 "
        Exemplo: vm.clone_directory = "e:\\Servidores\\VMware\\#{Vm_name} -instance-1 "
    -   vm.vmx["Memsize"]= ""
    -   vm.vmx[â€œNumVCPusâ€"]= ""
    -   vm.vmx["CPUID.CRERSESCOUT"]= ""

Personalize a configuraÃ§Ã£o de rede em arquivos[Configs/Network](configs/network/).

* * *

<a name="usage"></a>

## ğŸ“ Uso

Use este repositÃ³rio para obter aprendizado sobre o exame LPIC-3 305-300

### â¬†ï¸â¬‡ï¸ para cima e para baixo

Mudar a_VagrantFile-Topic-xxx_modelo e cÃ³pia para um novo arquivo com nome_VagrantFile_

```sh
cd vagrant && vagrant up
cd vagrant && vagrant destroy -f
```

### ğŸ”„ Para reiniciar VMs

```sh
cd vagrant && vagrant reload
```

**Importante:**_Se vocÃª reiniciar VMs sem vagÃ£o, a pasta compartilhada nÃ£o Ã© montada apÃ³s a inicializaÃ§Ã£o._

### ğŸ’» Use PowerShell para cima e para baixo

Se vocÃª usa a plataforma Windows, crio um script PowerShell para VMs para cima e para baixo.

```powershell
vagrant/up.ps1
vagrant/destroy.ps1
```

### ğŸ—ºï¸ Esquema de infraestrutura TÃ³pico 351

![topic-351](images/infraestructure-topic-351.png)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="roadmap"></a>

## Roteiro Roteiro

-   [x] Criar repositÃ³rio
-   [x] Crie scripts para provisioning laboratÃ³rios
-   [x] Crie exemplos sobre o tÃ³pico 351
-   [x] Crie exemplos sobre o tÃ³pico 352
-   [ ] Crie exemplos sobre o tÃ³pico 353
-   [ ] Carregue o ITEXAM simulado

* * *

<a name="freedoms"></a>

## ğŸ—½ Quatro liberdades essenciais

> 0.a liberdade para executar o programa como desejar, para qualquer finalidade (liberdade 0).
> 1.A liberdade para estudar como o programa funciona e muda para
> sua computaÃ§Ã£o como desejar (liberdade 1).
> O acesso ao cÃ³digo -fonte Ã© uma condiÃ§Ã£o prÃ©via para isso.
> 2.A liberdade para redistribuir cÃ³pias para que vocÃª possa ajudar os outros (liberdade 2).
> 3.Freedom para distribuir cÃ³pias de suas versÃµes modificadas para outras pessoas (liberdade 3).

* * *

## ğŸ” Inspecione os comandos

```sh
type COMMAND
apropos COMMAND
whatis COMMAND --long
whereis COMMAND
COMMAND --help, --h
man COMMAND
```

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351"></a>

## ğŸ–¥ï¸ TÃ³pico 351: VirtualizaÃ§Ã£o completa

![LPIC3-305-300](images/virtualization-351.png)

* * *

<a name="topic-351.1"></a>

### ğŸ§  351.1 Conceitos e teoria de virtualizaÃ§Ã£o

**Peso:**6

**DescriÃ§Ã£o:**Os candidatos devem conhecer e entender os conceitos gerais, a teoria e a terminologia da virtualizaÃ§Ã£o. Isso inclui terminologia Xen, Qemu e Libvirt.

**Principais Ã¡reas de conhecimento:**

-   ğŸ–¥ï¸ entender a terminologia da virtualizaÃ§Ã£o
-   âš–ï¸ entender os prÃ³s e contras da virtualizaÃ§Ã£o
-   ğŸ› ï¸ Entenda as vÃ¡rias variaÃ§Ãµes de hipervisores e monitores de mÃ¡quinas virtuais
-   ğŸ”„ Entenda os principais aspectos da migraÃ§Ã£o de mÃ¡quinas fÃ­sicas para virtuais
-   ğŸš€ Entenda os principais aspectos da migraÃ§Ã£o de mÃ¡quinas virtuais entre os sistemas host
-   ğŸ“¸ Entenda as caracterÃ­sticas e implicaÃ§Ãµes da virtualizaÃ§Ã£o para uma mÃ¡quina virtual, como instantÃ¢neos, pausas, clonagem e limites de recursos
-   ğŸŒ ConsciÃªncia de ovirt, proxmox, Systemd Machined e VirtualBox
-   ğŸ”— ConsciÃªncia do Vswitch aberto

#### ğŸ“‹ 351.1 Objetos citados

```sh
Hypervisor
Hardware Virtual Machine (HVM)
Paravirtualization (PV)
Emulation and Simulation
CPU flags
/proc/cpuinfo
Migration (P2V, V2V)
```

#### Hypervisors Hipervisores

##### ğŸ¢ Hypervisor tipo 1 (hipervisor nua-metal)

###### ğŸ“„ DefiniÃ§Ã£o do tipo 1

Executa diretamente no hardware fÃ­sico do host, fornecendo uma camada base para gerenciar VMs sem a necessidade de um sistema operacional host.

###### ğŸ“ CaracterÃ­sticas do tipo 1

-   âš¡ Alto desempenho e eficiÃªncia.
-   LatÃªncia mais baixa e sobrecarga.
-   ğŸ¢ FreqÃ¼entemente usado em ambientes corporativos e data centers.

###### ğŸ’¡ Exemplos do tipo 1

-   VMware ESXi: um hipervisor robusto e amplamente usado em configuraÃ§Ãµes corporativas.
-   Microsoft Hyper-V: integrado ao Windows Server, oferecendo fortes recursos de desempenho e gerenciamento.
-   Xen: Um hipervisor de cÃ³digo aberto usado por muitos provedores de serviÃ§os em nuvem.
-   KVM (mÃ¡quina virtual baseada em kernel): integrada ao kernel Linux, fornecendo alto desempenho para sistemas baseados em Linux.

##### ğŸ  Hypervisor tipo 2 (hipervisor hospedado)

###### ğŸ“„ DefiniÃ§Ã£o do tipo 2

Executa em cima de um sistema operacional convencional, contando com o sistema operacional host para gerenciamento de recursos e suporte ao dispositivo.

###### ğŸ“ CaracterÃ­sticas do tipo 2

-   ğŸ› ï¸ mais fÃ¡cil de configurar e usar, especialmente em computadores pessoais.
-   ğŸ”§ Mais flexÃ­vel para desenvolvimento, teste e implantaÃ§Ãµes em menor escala.
-   ğŸ¢ Normalmente menos eficiente que os hipervisores do tipo 1 devido a uma sobrecarga adicional do sistema operacional do host.

###### ğŸ’¡ Exemplos do tipo 2

-   VMware Workstation: um poderoso hipervisor para executar vÃ¡rios sistemas operacionais em uma Ãºnica Ã¡rea de trabalho.
-   Oracle VirtualBox: um hipervisor de cÃ³digo aberto conhecido por sua flexibilidade e facilidade de uso.
-   Paralels Desktop: Projetado para usuÃ¡rios de Mac para executar o Windows e outros sistemas operacionais ao lado de MacOS.
-   Qemu (emulador rÃ¡pido): um emulador de cÃ³digo aberto e virtualizador, geralmente usado em conjunto com o KVM.

##### âš–ï¸ DiferenÃ§as -chave entre os hipervisores do tipo 1 e do tipo 2

-   Ambiente de implantaÃ§Ã£o:
    -   Os hipervisores tipo 1 sÃ£o comumente implantados em data centers e ambientes corporativos devido Ã  sua interaÃ§Ã£o direta com hardware e alto desempenho.
    -   Os hipervisores do tipo 2 sÃ£o mais adequados para tarefas de uso pessoal, desenvolvimento, teste e virtualizaÃ§Ã£o em pequena escala.
-   Desempenho:
    -   Os hipervisores do tipo 1 geralmente oferecem melhor desempenho e menor latÃªncia porque nÃ£o confiam em um sistema operacional host.
    -   Os hipervisores do tipo 2 podem experimentar alguma degradaÃ§Ã£o do desempenho devido Ã  sobrecarga de execuÃ§Ã£o no topo de um sistema operacional host.
-   Gerenciamento e facilidade de uso:
    -   Os hipervisores do tipo 1 requerem configuraÃ§Ã£o e gerenciamento mais complexos, mas fornecem recursos avanÃ§ados e escalabilidade para implantaÃ§Ãµes em larga escala.
    -   Os hipervisores tipo 2 sÃ£o mais fÃ¡ceis de instalar e usar, tornando -os ideais para usuÃ¡rios individuais e projetos menores.

##### ğŸ”„ Tipos de migraÃ§Ã£o

No contexto dos hipervisores, que sÃ£o tecnologias usadas para criar e gerenciar mÃ¡quinas virtuais, os termos migraÃ§Ã£o de P2V e migraÃ§Ã£o V2V sÃ£o comuns em ambientes de virtualizaÃ§Ã£o.
Eles se referem a processos de sistemas de migraÃ§Ã£o entre diferentes tipos de plataformas.

##### ğŸ–¥ï¸â¡ï¸ğŸ–¥ï¸ P2V - MigraÃ§Ã£o fÃ­sica para virtual

A migraÃ§Ã£o de P2V refere-se ao processo de migraÃ§Ã£o de um servidor fÃ­sico para uma mÃ¡quina virtual. Em outras palavras, um sistema operacional e seus aplicativos, executando em hardware fÃ­sico dedicado, sÃ£o "convertidos" e movidos para uma mÃ¡quina virtual que Ã© executada em um hipervisor (como VMware, Hyper-V, KVM, etc.).

-   Exemplo: vocÃª tem um servidor fÃ­sico executando um sistema Windows ou Linux e deseja movÃª -lo para um ambiente virtual, como uma infraestrutura em nuvem ou um servidor de virtualizaÃ§Ã£o interna.
    O processo envolve copiar todo o estado do sistema, incluindo o sistema operacional, drivers e dados, para criar uma mÃ¡quina virtual equivalente que possa funcionar como se estivesse no hardware fÃ­sico.

##### ğŸ–¥ï¸ğŸ”ğŸ–¥ï¸ V2V - Virtual para MigraÃ§Ã£o Virtual

A migraÃ§Ã£o V2V refere-se ao processo de migraÃ§Ã£o de uma mÃ¡quina virtual de um hipervisor para outro. Nesse caso, vocÃª jÃ¡ tem uma mÃ¡quina virtual em execuÃ§Ã£o em um ambiente virtualizado (como o VMware) e deseja movÃª-lo para outro ambiente virtualizado (por exemplo, para Hyper-V ou um novo servidor VMware).

-   Exemplo: vocÃª tem uma mÃ¡quina virtual em execuÃ§Ã£o em um servidor de virtualizaÃ§Ã£o do VMware, mas decide migrÃ¡-lo para uma plataforma Hyper-V. Nesse caso, a migraÃ§Ã£o V2V converte a mÃ¡quina virtual de um formato ou hipervisor para outro, garantindo que ela possa continuar funcionando corretamente.

#### ğŸ§© HVM e paravirtualizaÃ§Ã£o

##### âš™ï¸ VirtualizaÃ§Ã£o assistida por hardware (HVM)

###### ğŸ“„ DefiniÃ§Ã£o HVM

A HVM aproveita as extensÃµes de hardware fornecidas pelas CPUs modernas para virtualizar o hardware, permitindo a criaÃ§Ã£o e o gerenciamento de VMs com o mÃ­nimo de sobrecarga de desempenho.

###### ğŸ“ CaracterÃ­sticas -chave HVM

-   ğŸ–¥ï¸**Suporte de hardware**: Requer suporte Ã  CPU para extensÃµes de virtualizaÃ§Ã£o, como Intel VT-X ou AMD-V.
-   ğŸ› ï¸**VirtualizaÃ§Ã£o completa:**As VMs podem executar sistemas operacionais de hÃ³spedes nÃ£o modificados, pois o hipervisor fornece uma emulaÃ§Ã£o completa do ambiente de hardware.
-   âš¡**Desempenho:**Normalmente, oferece desempenho quase nativo devido Ã  execuÃ§Ã£o direta do cÃ³digo de convidado na CPU.
-   ğŸ”’**Isolamento:**Fornece um forte isolamento entre as VMs, pois cada VM opera como se tivesse seu prÃ³prio hardware dedicado.

###### ğŸ’¡ Exemplo de HVM

VMware Esxi, Microsoft Hyper-V, KVM (mÃ¡quina virtual baseada em kernel).

###### âœ… Vantagens HVM

-   âœ…**Compatibilidade:**Pode executar qualquer sistema operacional sem modificaÃ§Ã£o.
-   âš¡**Desempenho:**Alto desempenho devido ao suporte de hardware.
-   ğŸ”’**SeguranÃ§a:**Recursos aprimorados de isolamento e seguranÃ§a fornecidos pelo hardware.

###### Desvantagens HVM Desvantagens

-   ğŸ› ï¸**DependÃªncia de hardware:**Requer recursos especÃ­ficos de hardware, limitando a compatibilidade com sistemas mais antigos.
-   ğŸ”§**Complexidade:**Pode envolver configuraÃ§Ã£o e gerenciamento mais complexos.

##### ğŸ§© ParavirtualizaÃ§Ã£o

###### ğŸ“„ DefiniÃ§Ã£o de paravirtualizaÃ§Ã£o

A paravirtualizaÃ§Ã£o envolve a modificaÃ§Ã£o do sistema operacional convidado para estar ciente do ambiente virtual, permitindo que ele interaja com mais eficiÃªncia com o hipervisor.

###### ğŸ“ CaracterÃ­sticas -chave da paravirtualizaÃ§Ã£o

-   ğŸ› ï¸**ModificaÃ§Ã£o de convidados:**Requer alteraÃ§Ãµes no sistema operacional convidado para se comunicar diretamente com o hipervisor usando hipercalls.
-   âš¡**Desempenho:**Pode ser mais eficiente que a virtualizaÃ§Ã£o completa tradicional, pois reduz a sobrecarga associada ao hardware emulando.
-   ğŸ”—**Compatibilidade:**Limitado a sistemas operacionais que foram modificados para paravirtutualizaÃ§Ã£o.

###### ğŸ’¡ Exemplos de paravirtualizaÃ§Ã£o

Xen com convidados paravirtualizados, ferramentas VMware em determinadas configuraÃ§Ãµes e algumas configuraÃ§Ãµes de KVM.

###### âœ… Vantagens de paravirtualizaÃ§Ã£o

-   âš¡**EficiÃªncia:**Reduz a sobrecarga de virtualizar hardware, potencialmente oferecendo melhor desempenho para determinadas cargas de trabalho.
-   âœ…**UtilizaÃ§Ã£o de recursos:**Uso mais eficiente dos recursos do sistema devido Ã  comunicaÃ§Ã£o direta entre o sistema operacional convidado e o hipervisor.

###### âŒ Desvantagens de paravirtualizaÃ§Ã£o

-   ğŸ› ï¸**ModificaÃ§Ã£o do sistema operacional convidado:**Requer modificaÃ§Ãµes para o sistema operacional convidado, limitando a compatibilidade aos sistemas operacionais suportados.
-   ğŸ”§**Complexidade:**Requer complexidade adicional no sistema operacional convidado para implementaÃ§Ãµes de hipercall.

##### âš–ï¸ DiferenÃ§as -chave

###### ğŸ–¥ï¸ Requisitos do sistema operacional convidado

-   **HVM:**Pode executar sistemas operacionais de convidados nÃ£o modificados.
-   **ParavirtutualizaÃ§Ã£o:**Requer que os sistemas operacionais de convidados sejam modificados para trabalhar com o hipervisor.

###### âš¡ Desempenho

-   **HVM:**Normalmente, fornece desempenho quase nativo devido Ã  execuÃ§Ã£o assistida por hardware.
-   **ParavirtutualizaÃ§Ã£o:**Pode oferecer desempenho eficiente, reduzindo a sobrecarga da emulaÃ§Ã£o de hardware, mas depende do sistema operacional convidado modificado.

###### ğŸ§° DependÃªncia de hardware

-   **HVM:**Requer recursos especÃ­ficos da CPU (Intel VT-X, AMD-V).
-   **ParavirtutualizaÃ§Ã£o:**NÃ£o requer recursos especÃ­ficos da CPU, mas precisa de um sistema operacional de convidado modificado.

###### ğŸ”’ Isolamento

-   **HVM:**Fornece um isolamento forte usando recursos de hardware.
-   **ParavirtutualizaÃ§Ã£o:**Depende do isolamento baseado em software, que pode nÃ£o ser tÃ£o robusto quanto o isolamento baseado em hardware.

###### ğŸ§© Complexidade

-   **HVM:**Geralmente mais simples de implantar, pois suporta o sistema operacional nÃ£o modificado.
-   **ParavirtutualizaÃ§Ã£o:**Requer configuraÃ§Ã£o e modificaÃ§Ãµes adicionais para o sistema operacional convidado, aumentando a complexidade.

#### ğŸ§  NUMA (acesso nÃ£o uniforme de memÃ³ria)

O NUMA (acesso nÃ£o uniforme de memÃ³ria) Ã© uma arquitetura de memÃ³ria usada em sistemas multiprocessadores para otimizar o acesso Ã  memÃ³ria pelos processadores.
Em um sistema NUMA, a memÃ³ria Ã© distribuÃ­da de maneira desigual entre os processadores, o que significa que cada processador tem um acesso mais rÃ¡pido a uma parte da memÃ³ria (sua "memÃ³ria local") do que Ã  memÃ³ria que estÃ¡ fisicamente mais distante (referida como "memÃ³ria remota") e associada a outros processadores.

##### ğŸ“ Principais caracterÃ­sticas da arquitetura NUMA

1.  **MemÃ³ria local e remota**: Cada processador tem sua prÃ³pria memÃ³ria local, que pode acessar mais rapidamente. No entanto, tambÃ©m pode acessar a memÃ³ria de outros processadores, embora isso leve mais tempo.
2.  **LatÃªncia diferenciada**: A latÃªncia do acesso Ã  memÃ³ria varia dependendo se o processador estÃ¡ acessando sua memÃ³ria local ou a memÃ³ria de outro nÃ³. O acesso Ã  memÃ³ria local Ã© mais rÃ¡pido, enquanto o acesso Ã  memÃ³ria de outro nÃ³ (remoto) Ã© mais lento.
3.  **Escalabilidade**: A arquitetura da NUMA foi projetada para melhorar a escalabilidade em sistemas com muitos processadores. Ã€ medida que mais processadores sÃ£o adicionados, a memÃ³ria tambÃ©m Ã© distribuÃ­da, evitando o gargalo que ocorreria em uma arquitetura uniforme de acesso Ã  memÃ³ria (Uma).

##### âœ… Advantages of NUMA

-   âš¡ Melhor desempenho em sistemas grandes: como cada processador possui memÃ³ria local, ele pode funcionar com mais eficiÃªncia sem competir tanto com outros processadores pelo acesso Ã  memÃ³ria.
-   Scalability: O NUMA permite sistemas com muitos processadores e grandes quantidades de memÃ³ria para escalar com mais eficÃ¡cia em comparaÃ§Ã£o com uma arquitetura Uma.

##### âŒ Desvantagens

-   Complexidade da programaÃ§Ã£o: os programadores precisam estar cientes de quais regiÃµes de memÃ³ria sÃ£o locais ou remotas, otimizando o uso da memÃ³ria local para obter um melhor desempenho.
-   ğŸ¢ Penalidades potenciais de desempenho: se um processador acessar frequentemente a memÃ³ria remota, o desempenho poderÃ¡ sofrer devido Ã  maior latÃªncia.
    Essa arquitetura Ã© comum em sistemas multiprocessadores de alto desempenho, como servidores e supercomputadores, onde a escalabilidade e a otimizaÃ§Ã£o da memÃ³ria sÃ£o crÃ­ticas.

#### ğŸ†“ SoluÃ§Ãµes OpenSource

-   ğŸŒ Ovirt:<https://www.ovirt.org/>
-   ğŸŒ Proxmox:<https://www.proxmox.com/en/proxmox-virtual-environment/overview>
-   ğŸŒ Oracle VirtualBox:<https://www.virtualbox.org/>
-   ğŸŒ Open Vswitch:<https://www.openvswitch.org/>

#### ğŸ—‚ï¸ Tipos de virtualizaÃ§Ã£o

##### VirtualizaÃ§Ã£o de hardware (virtualizaÃ§Ã£o do servidor)

###### ğŸ“„ DefiniÃ§Ã£o HV

Abstraia o hardware fÃ­sico para criar mÃ¡quinas virtuais (VMs) que executam sistemas e aplicativos operacionais separados.

###### ğŸ› ï¸ Casos de uso de hv hv

Data centers, computaÃ§Ã£o em nuvem, consolidaÃ§Ã£o do servidor.

###### ğŸ’¡ Exemplos de HV

VMware Esxi, Microsoft Hyper-V, KVM.

##### ğŸ“¦ VirtualizaÃ§Ã£o do sistema operacional (contÃªinerizaÃ§Ã£o)

###### ğŸ“„ DefiniÃ§Ã£o de contÃªiner

Permite que vÃ¡rias instÃ¢ncias isoladas do espaÃ§o do usuÃ¡rio (contÃªineres) sejam executadas em um Ãºnico kernel do sistema operacional.

###### ğŸ› ï¸ Casos de uso de contÃªinerizaÃ§Ã£o

Ambientes de arquitetura, desenvolvimento e teste de microsserviÃ§os.

###### ğŸ’¡ Exemplos de contÃªiner

Docker, Kubernetes, LXC.

##### ğŸŒ VirtualizaÃ§Ã£o de rede

###### ğŸ“„ DefiniÃ§Ã£o de virtualizaÃ§Ã£o de rede

Combina recursos de rede de hardware e software em uma Ãºnica entidade administrativa baseada em software.

###### ğŸ› ï¸ Casos de uso da virtualizaÃ§Ã£o de rede de rede

Networking definido por software (SDN), VirtualizaÃ§Ã£o da FunÃ§Ã£o de Rede (NFV).

###### ğŸ’¡ Exemplos de virtualizaÃ§Ã£o de rede

VMware NSX, Cisco ACI, OpenStack Neutron.

##### ğŸ’¾ VirtualizaÃ§Ã£o de armazenamento

###### ğŸ“„ DefiniÃ§Ã£o de virtualizaÃ§Ã£o de armazenamento

Pools armazenamento fÃ­sico de vÃ¡rios dispositivos em uma Ãºnica unidade de armazenamento virtual que pode ser gerenciada centralmente.

###### ğŸ› ï¸ Casos de uso da virtualizaÃ§Ã£o de armazenamento

Gerenciamento de dados, otimizaÃ§Ã£o de armazenamento, recuperaÃ§Ã£o de desastres.

###### ğŸ’¡ Exemplos de virtualizaÃ§Ã£o de armazenamento

IBM SAN Volume Controller, VMware vsan, NetApp OTAP.

##### VirttualizaÃ§Ã£o da Ã¡rea de trabalho

###### ğŸ“„ DefiniÃ§Ã£o de virtualizaÃ§Ã£o da Ã¡rea de trabalho

Permite que um sistema operacional de desktop seja executado em uma mÃ¡quina virtual hospedada em um servidor.

###### ğŸ› ï¸ Casos de uso da virtualizaÃ§Ã£o de desktop

Infraestrutura de Desktop Virtual (VDI), SoluÃ§Ãµes de Trabalho Remoto.

###### ğŸ’¡ Exemplos de virtualizaÃ§Ã£o da Ã¡rea de trabalho

Citrix Apps e desktops Citrix, VMware Horizon, Microsoft Remote Desktop Services.

##### ğŸ“± VirtualizaÃ§Ã£o de aplicativos

###### ğŸ“„ DefiniÃ§Ã£o de virtualizaÃ§Ã£o de aplicativos

Separa os aplicativos do hardware subjacente e do sistema operacional, permitindo que eles sejam executados em ambientes isolados.

###### ğŸ› ï¸ Casos de uso da virtualizaÃ§Ã£o de aplicativos

ImplantaÃ§Ã£o simplificada de aplicativos, teste de compatibilidade.

###### ğŸ’¡ Exemplos de virtualizaÃ§Ã£o de aplicativos

VMware ThinApp, Microsoft App-V, Citrix XenApp.

##### ğŸ—ƒï¸ VirtualizaÃ§Ã£o de dados

###### ğŸ“„ DefiniÃ§Ã£o de virtualizaÃ§Ã£o de dados

Integra dados de vÃ¡rias fontes sem consolidÃ¡ -los fisicamente, fornecendo uma visÃ£o unificada para anÃ¡lise e relatÃ³rio.

###### ğŸ› ï¸ Casos de uso da virtualizaÃ§Ã£o de dados

InteligÃªncia de negÃ³cios, integraÃ§Ã£o de dados em tempo real.

###### ğŸ’¡ Exemplos de virtualizaÃ§Ã£o de dados

Denodo, Red Hat JBoss VirtualizaÃ§Ã£o de dados, IBM InfoSphere.

##### ğŸŒŸ BenefÃ­cios da virtualizaÃ§Ã£o

-   âš¡ EficiÃªncia de recursos: melhor utilizaÃ§Ã£o de recursos fÃ­sicos.
-   ğŸ’° Economia de custos: hardware reduzido e custos operacionais.
-   ğŸ“ˆ Escalabilidade: fÃ¡cil de aumentar ou diminuir de acordo com a demanda.
-   ğŸ”§ Flexibilidade: suporta uma variedade de cargas de trabalho e aplicaÃ§Ãµes.
-   ğŸ”„ RecuperaÃ§Ã£o de desastres: processos simplificados de backup e recuperaÃ§Ã£o.
-   ğŸ”’ Isolamento: seguranÃ§a aprimorada atravÃ©s do isolamento de ambientes.

#### EmulaÃ§Ã£o

A emulaÃ§Ã£o envolve a simulaÃ§Ã£o do comportamento de hardware ou software em uma plataforma diferente do originalmente pretendido.

Esse processo permite que o software projetado para um sistema seja executado em outro sistema que possa ter arquitetura ou ambiente operacional diferente.

Embora a emulaÃ§Ã£o forneÃ§a versatilidade, permitindo a execuÃ§Ã£o de sistemas ou aplicativos operacionais de convidados nÃ£o modificados, ela geralmente vem com sobrecarga de desempenho.

Essa sobrecarga surge porque o sistema emulado precisa interpretar e traduzir instruÃ§Ãµes destinadas ao sistema original em instruÃ§Ãµes compatÃ­veis com o sistema host. Como resultado, a emulaÃ§Ã£o pode ser mais lenta que a execuÃ§Ã£o nativa, tornando-o menos eficiente para tarefas com uso intensivo de recursos.

Apesar dessa desvantagem, a emulaÃ§Ã£o permanece valiosa para a execuÃ§Ã£o de software herdado, testando aplicativos em diferentes plataformas e facilitando o desenvolvimento de plataformas cruzadas.

#### Systemd-Mathined

O serviÃ§o usinado pela SystemD Ã© dedicado ao gerenciamento de mÃ¡quinas e contÃªineres virtuais no ecossistema Systemd.
 Ele fornece funcionalidades essenciais para controlar, monitorar e manter instÃ¢ncias virtuais, oferecendo integraÃ§Ã£o e eficiÃªncia robustas nos ambientes Linux.

<p align="right">(<a href="#topic-351.1">back to sub Topic 351.1</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351.2"></a>

### ğŸ§ 351.2 AlternaÃ§Ã£o

![xen-architecture](images/xen-achitecture.png)

![xen-architecture](images/xen-achitecture2.png)

**Peso:**3

**DescriÃ§Ã£o:**Os candidatos devem poder instalar, configurar, manter, migrar e solucionar problemas de instalaÃ§Ãµes XEN. O foco estÃ¡ no Xen versÃ£o 4.x.

**Principais Ã¡reas de conhecimento:**

-   Entenda a arquitetura de Xen, incluindo networking e armazenamento
-   ConfiguraÃ§Ã£o bÃ¡sica dos nÃ³s e domÃ­nios Xen
-   Gerenciamento bÃ¡sico de nÃ³s e domÃ­nios Xen
-   SoluÃ§Ã£o de problemas bÃ¡sicos de instalaÃ§Ãµes Xen
-   Avarines fora da pÃ­lula
-   ConsciÃªncia de Xenstore
-   ConsciÃªncia dos parÃ¢metros de inicializaÃ§Ã£o do Xen
-   ConsciÃªncia do utilitÃ¡rio XM

#### ğŸ§ AlternaÃ§Ã£o

![panda](images/xen-panda.png)

O XEN Ã© um hipervisor de cÃ³digo aberto tipo 1 (sem metal), que permite que vÃ¡rios sistemas operacionais sejam executados simultaneamente no mesmo hardware fÃ­sico.xen fornece uma camada entre o hardware fÃ­sico e as mÃ¡quinas virtuais (VMs), permitindo compartilhamento de recursos eficientes e isolamento.

-   **Arquitetura:**O Xen opera com um sistema de duas camadas em que o DomÃ­nio 0 (DOM0) Ã© o domÃ­nio privilegiado com acesso direto ao hardware e gerencia o hipervisor. Outras mÃ¡quinas virtuais, chamadas de domÃ­nio U (DOMU), executam sistemas operacionais convidados e sÃ£o gerenciados pelo DOM0.
-   **Tipos de virtualizaÃ§Ã£o:**O XEN suporta paravirtualizaÃ§Ã£o (PV), que requer o sistema operacional convidado modificado e a virtualizaÃ§Ã£o assistida por hardware (HVM), que usa extensÃµes de hardware (por exemplo, Intel VT-X ou AMD-V) para executar sistemas operacionais de convidados nÃ£o modificados.
    O XEN Ã© amplamente utilizado em ambientes em nuvem, principalmente pela Amazon Web Services (AWS) e outros provedores de nuvem em larga escala.

#### ğŸ¢ Xensource

A Xensource foi a empresa fundada pelos desenvolvedores originais do Xen Hypervisor da Universidade de Cambridge para comercializar a Xen. A empresa forneceu soluÃ§Ãµes corporativas com base no XEN e ofereceu ferramentas e suporte adicionais para aprimorar os recursos da XEN para uso corporativo.

-   **AquisiÃ§Ã£o pela Citrix**: Em 2007, a Xensource foi adquirida pela Citrix Systems, Inc. A Citrix usou a tecnologia Xen como base para o seu produto Citrix Xenserver, que se tornou uma popular plataforma de virtualizaÃ§Ã£o de grau corporativo baseado em Xen.
-   **TransiÃ§Ã£o**: ApÃ³s a aquisiÃ§Ã£o, o projeto Xen continuou como um projeto de cÃ³digo aberto, enquanto a Citrix se concentrou em ofertas comerciais como Xenserver, alavancando a tecnologia Xensource.

#### ğŸŒ Projeto Xen

O projeto XEN refere-se Ã  comunidade de cÃ³digo aberto e Ã  iniciativa responsÃ¡vel pelo desenvolvimento e manutenÃ§Ã£o do hipervisor Xen apÃ³s sua comercializaÃ§Ã£o. O projeto Xen opera sob a FundaÃ§Ã£o Linux, com foco na construÃ§Ã£o, melhoria e apoio a Xen como um esforÃ§o colaborativo e orientado pela comunidade.

-   **Metas:**O projeto XEN visa avanÃ§ar o hipervisor, melhorando seu desempenho, seguranÃ§a e conjunto de recursos para uma ampla gama de casos de uso, incluindo computaÃ§Ã£o em nuvem, virtualizaÃ§Ã£o focada na seguranÃ§a (por exemplo, Qubes OS) e sistemas incorporados.
-   **Colaboradores:**O projeto inclui colaboradores de vÃ¡rias organizaÃ§Ãµes, incluindo os principais provedores de nuvem, fornecedores de hardware e desenvolvedores independentes.
-   **PÃ­lula e hedools:**O projeto XEN tambÃ©m inclui ferramentas como XAPI (XenAPI), que Ã© usado para gerenciar instalaÃ§Ãµes do Xen Hypervisor e vÃ¡rios outros utilitÃ¡rios para gerenciamento e otimizaÃ§Ã£o do sistema.

#### ğŸ—„ï¸ Xenstore

A Xen Store Ã© um componente crÃ­tico do hipervisor Xen.
Essencialmente, o Xen Store Ã© um banco de dados de valor-chave distribuÃ­do usado para comunicaÃ§Ã£o e compartilhamento de informaÃ§Ãµes entre o hypervisor Xen e as mÃ¡quinas virtuais (tambÃ©m conhecidas como domÃ­nios) que ele gerencia.

Aqui estÃ£o alguns aspectos importantes da Xen Store:

-   **ComunicaÃ§Ã£o entre domÃ­nios:**O Xen Store permite a comunicaÃ§Ã£o entre domÃ­nios, como o DOM0 (o domÃ­nio privilegiado que controla os recursos de hardware) e o DOMUS (domÃ­nios do usuÃ¡rio, que sÃ£o as VMs). Isso Ã© feito atravÃ©s de entradas de valor-chave, onde cada domÃ­nio pode ler ou escrever informaÃ§Ãµes.
-   **Gerenciamento de configuraÃ§Ã£o:**Ã‰ usado para armazenar e acessar informaÃ§Ãµes de configuraÃ§Ã£o, como dispositivos virtuais, redes e parÃ¢metros de inicializaÃ§Ã£o. Isso facilita o gerenciamento dinÃ¢mico e a configuraÃ§Ã£o das VMs.
-   **Eventos e notificaÃ§Ãµes:**A Xen Store tambÃ©m suporta notificaÃ§Ãµes de eventos. Quando uma chave ou valor especÃ­fica na loja Xen Ã© modificada, os domÃ­nios interessados podem ser notificados para reagir a essas alteraÃ§Ãµes. Isso Ã© Ãºtil para monitorar e gerenciar recursos.
-   API simples: a Xen Store fornece uma API simples para ler e escrever dados, facilitando os desenvolvedores para integrar seus aplicativos ao sistema de virtualizaÃ§Ã£o Xen.

#### ğŸ”— Xapi

XAPI, ou Xenapi, Ã© a interface de programaÃ§Ã£o de aplicativos (API) usada para gerenciar o hipervisor Xen e suas mÃ¡quinas virtuais (VMs).
O XAPI Ã© um componente essencial do Xenserver (agora conhecido como Citrix Hypervisor) e fornece uma maneira padronizada de interagir com o hipervisor Xen para executar operaÃ§Ãµes como criar, configurar, monitorar e controlar VMs.

Aqui estÃ£o alguns aspectos importantes de Xapi:

-   **Gerenciamento de VM:**O XAPI permite que os administradores criem, excluam, excluam, iniciem e parem e parem e parem programaticamente as mÃ¡quinas virtuais.
-   **AutomaÃ§Ã£o:**Com o XAPI, Ã© possÃ­vel automatizar o gerenciamento de recursos virtuais, incluindo redes, armazenamento e computaÃ§Ã£o, o que Ã© crucial para grandes ambientes em nuvem.
-   **IntegraÃ§Ã£o:**O XAPI pode ser integrado a outras ferramentas e scripts para fornecer administraÃ§Ã£o mais eficiente e personalizada do ambiente XEN.
-   **Controle de acesso:**O XAPI tambÃ©m fornece mecanismos de controle de acesso para garantir que apenas usuÃ¡rios autorizados possam executar operaÃ§Ãµes especÃ­ficas no ambiente virtual.

O XAPI Ã© a interface que permite o controle e a automaÃ§Ã£o do hipervisor Xen, facilitando o gerenciamento de ambientes virtualizados.

#### Resumo Xen Resumo

-   **INCROPPING:**A tecnologia principal do hipervisor que permite que as mÃ¡quinas virtuais sejam executadas em hardware fÃ­sico.
-   **Xensource:**A empresa que comercializou Xen, mais tarde adquirida pela Citrix, levando ao desenvolvimento do Citrix Xenserver.
-   **Projeto Xen:**A iniciativa e a comunidade de cÃ³digo aberto que continuam a desenvolver e manter o hipervisor Xen sob a FundaÃ§Ã£o Linux.
-   **Xenstore:**A Xen Store atua como uma intermediÃ¡ria de comunicaÃ§Ã£o e configuraÃ§Ã£o entre o Hypervisor Xen e as VMs, simplificando a operaÃ§Ã£o e o gerenciamento de ambientes virtualizados.
-   **PÃ­lula**Ã© a interface que permite o controle e a automaÃ§Ã£o do hipervisor Xen, facilitando o gerenciamento de ambientes virtualizados.

#### ğŸ–¥ï¸ Domain0 (DOM0)

Domain0, OR DOM0, Ã© o domÃ­nio de controle em uma arquitetura Xen. Ele gerencia outros domÃ­nios (DOMUS) e tem acesso direto ao hardware.
O DOM0 executa drivers de dispositivo, permitindo que o Domus, que nÃ£o possua acesso direto ao hardware, se comunique com dispositivos. Normalmente, Ã© uma instÃ¢ncia completa de um sistema operacional, como o Linux, e Ã© essencial para a operaÃ§Ã£o de hipervisor do Xen.

#### ğŸ’» DomÃ­nio (casa)

Domus sÃ£o domÃ­nios nÃ£o privilegiados que executam mÃ¡quinas virtuais.
Eles sÃ£o gerenciados pelo DOM0 e nÃ£o tÃªm acesso direto ao hardware. O DOMUS pode ser configurado para executar diferentes sistemas operacionais e Ã© usado para vÃ¡rios fins, como servidores de aplicativos e ambientes de desenvolvimento. Eles dependem do DOM0 para interaÃ§Ã£o de hardware.

#### ğŸ§© PV-DOMU (domÃ­nio paravirtualizado)

O PV-Domus usa uma tÃ©cnica chamada paravirtutualizaÃ§Ã£o. Neste modelo, o sistema operacional DOMU Ã© modificado para estar ciente de que ele Ã© executado em um ambiente virtualizado, permitindo que ele se comunique diretamente com o hipervisor para o desempenho otimizado.
Isso resulta em menor sobrecarga e melhor eficiÃªncia em comparaÃ§Ã£o com a virtualizaÃ§Ã£o total.

#### âš™ï¸ HVM-DOMU (domÃ­nio da mÃ¡quina virtual de hardware)

O HVM-Domus sÃ£o mÃ¡quinas virtuais que utilizam virtualizaÃ§Ã£o completa, permitindo que os sistemas operacionais nÃ£o modificados sejam executados. O Xen Hypervisor fornece emulaÃ§Ã£o de hardware para esses Domus, permitindo que eles executem qualquer sistema operacional que suporta a arquitetura de hardware subjacente.
Embora isso ofereÃ§a maior flexibilidade, pode resultar em uma sobrecarga mais alta em comparaÃ§Ã£o com o PV-Domus.

#### ğŸŒ Rede Xen

Dispositivos de rede paravirtualizados![pv-networking](images/xen-networking2.png)

Ponte![pv-networking](images/xen-networking1.png)

#### ğŸ“‹ 351.2 Objetos citados

```sh
Domain0 (Dom0), DomainU (DomU)
PV-DomU, HVM-DomU
/etc/xen/
xl
xl.cfg 
xl.conf # Xen global configurations
xentop
oxenstored # Xenstore configurations
```

#### ğŸ“ 351.2 Notas

```sh

# Xen Settings
/etc/xen/
/etc/xen/xl.conf - Main general configuration file for Xen
/etc/xen/oxenstored.conf - Xenstore configurations

# VM Configurations
/etc/xen/xlexample.pvlinux
/etc/xen/xlexample.hvm

# Service Configurations
/etc/default/xen
/etc/default/xendomains

# xen-tools configurations
/etc/xen-tools/
/usr/share/xen-tools/

# docs
xl(1)
xl.conf(5)
xlcpupool.cfg(5)
xl-disk-configuration(5)
xl-network-configuration(5)
xen-tscmode(7)

# initialized domains auto
/etc/default/xendomains
   XENDOMAINS_AUTO=/etc/xen/auto

/etc/xen/auto/


# set domain for up after xen reboot
## create folder auto
cd /etc/xen && mkdir -p auto && cd auto

# create simbolic link
ln -s /etc/xen/lpic3-pv-guest /etc/xen/auto/lpic3-pv-guest
```

#### ğŸ’» 351.2 Comandos importantes

##### ğŸ—ï¸ Xen-Create-Image

```sh
# create a pv image
xen-create-image \
  --hostname=lpic3-pv-guest \
  --memory=1gb \
  --vcpus=2 \
  --lvm=vg_xen \
  --bridge=xenbr0 \
  --dhcp \
  --pygrub \
  --password=vagrant \
  --dist=bookworm
```

##### ğŸ“„ Xen-List-Imagens

```sh
# list image
xen-list-image
```

##### âŒ Xen-Delete-Image

```sh
# delete a pv image
xen-delete-image lpic3-pv-guest --lvm=vg_xen
```

##### ğŸ—„ï¸ XENSTORE-LS

```sh
# list xenstore infos
xenstore-ls
```

##### âš™ï¸ XL

```sh
# view xen information
xl infos

# list Domains
xl list
xl list lpic3-hvm-guest
xl list lpic3-hvm-guest -l

# uptime Domains
xl uptime

# pause Domain
xl pause 2
xl pause lpic3-hvm-guest

# save state Domains
xl -v save lpic3-hvm-guest ~root/image-lpic3-hvm-guest.save

# restore Domain
xl restore /root/image-lpic3-hvm-guest.save

# get Domain name
xl domname 2

# view dmesg information
xl dmesg

# monitoring domain
xl top
xentop
xen top

# Limit mem Dom0
xl mem-set 0 2048

# Limit cpu (not permanent after boot)
xl vcpu-set 0 2

# create DomainU - virtual machine
xl create /etc/xen/lpic3-pv-guest.cfg

# create DomainU virtual machine and connect to guest
xl create -c /etc/xen/lpic3-pv-guest.cfg

##----------------------------------------------
# create DomainU virtual machine HVM

## create logical volume
lvcreate -l +20%FREE -n lpic3-hvm-guest-disk  vg_xen

## create a ssh tunel for vnc
ssh -l vagrant -L 5900:localhost:5900  192.168.0.130

## configure /etc/xen/lpic3-hvm-guest.cfg
## set boot for cdrom: boot = "d"

## create domain hvm
xl create /etc/xen/lpic3-hvm-guest.cfg

## open vcn conection in your vnc client with localhost
## for view install details

## after installation finished, destroy domain: xl destroy <id_or_name>

## set /etc/xen/lpic3-hvm-guest.cfg: boot for hard disc: boot = "c"

## create domain hvm
xl create /etc/xen/lpic3-hvm-guest.cfg

## access domain hvm
xl console <id_or_name>
##----------------------------------------------

# connect in domain guest
xl console <id>|<name> (press enter)
xl console 1
xl console lpic3-pv-guest

#How do I exit domU "xl console" session
#Press ctrl+] or if you're using Putty press ctrl+5.

# Poweroff domain
xl shutdown lpic3-pv-guest

# destroy domain
xl destroy lpic3-pv-guest

# reboot domain
xl reboot lpic3-pv-guest

# list block devices
xl block-list 1
xl block-list lpic3-pv-guest

# detach block devices
xl block-detach lpic3-hvm-guest hdc
xl block-detach 2 xvdc

# attach block devices

## hard disk devices
xl block-attach lpic3-hvm-guest-ubuntu 'phy:/dev/vg_xen/lpic3-hvm-guest-disk2,xvde,w'

## cdrom
xl block-attach lpic3-hvm-guest 'file:/home/vagrant/isos/ubuntu/seed.iso,xvdc:cdrom,r'
xl block-attach 2 'file:/home/vagrant/isos/ubuntu/seed.iso,xvdc:cdrom,r'

# insert and eject cdrom devices
xl cd-insert lpic3-hvm-guest-ubuntu xvdb  /home/vagrant/isos/ubuntu/ubuntu-24.04.1-live-server-amd64.iso
xl cd-eject lpic3-hvm-guest-ubuntu xvdb
```

#### 251.2 Notas

##### VIF

Em Xen, "VIF" significa interface virtual e Ã© usado para configurar a rede para mÃ¡quinas virtuais (domÃ­nios).

Ao especificar as diretrizes "VIF" nos arquivos de configuraÃ§Ã£o do domÃ­nio, os administradores podem definir interfaces de rede, atribuir endereÃ§os IP, configurar VLANs e configurar outros parÃ¢metros de rede para mÃ¡quinas virtuais em execuÃ§Ã£o em hosts XEN. Por exemplo: VIF =[=Bridge  Xenbr0], neste caso, conecta a interface de rede da VM Ã  ponte Xen chamada "XenBr0".

<p align="right">(<a href="#topic-351.2">back to sub Topic 351.2</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351.3"></a>

### ğŸ–¥ï¸ 351.3 Qemu

![xen-kvm-qemu](/images/xen-kvm-qemu.png)

**Peso:**4

**DescriÃ§Ã£o:**Os candidatos devem poder instalar, configurar, manter, migrar e solucionar problemas de instalaÃ§Ãµes QEMU.

**Principais Ã¡reas de conhecimento:**

-   Entenda a arquitetura de Qemu, incluindo KVM, networking e armazenamento
-   Inicie as instÃ¢ncias QEMU da linha de comando
-   Gerencie instantÃ¢neos usando o monitor qemu
-   Instale o agente convidado QEMU e os drivers de dispositivo virtio
-   Solucionar problemas de instalaÃ§Ãµes QEMU, incluindo redes e armazenamento
-   ConsciÃªncia de importantes parÃ¢metros de configuraÃ§Ã£o do QEMU

#### ğŸ“‹ 351.3 Objetos citados

```sh
Kernel modules: kvm, kvm-intel and kvm-amd
/dev/kvm
QEMU monitor
qemu
qemu-system-x86_64
ip
brctl
tunctl
```

#### ğŸ› ï¸ 351.3 Comandos importantes

##### ğŸ“ 351.3 outros comandos

##### ğŸ§ª Verifique o mÃ³dulo KVM

```sh
# check if kvm is enabled
egrep -o '(vmx|svm)' /proc/cpuinfo
lscpu |grep Virtualization
lsmod|grep kvm
ls -l /dev/kvm
hostnamectl
systemd-detect-virt
```

```sh
# check if kvm is enabled
egrep -o '(vmx|svm)' /proc/cpuinfo
lscpu |grep Virtualization
lsmod|grep kvm
ls -l /dev/kvm

# check kernel infos
uname -a

# check root device
findmnt /

# mount a qcow2 image
## Example 1:
mkdir -p /mnt/qemu
guestmount -a os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 -i /mnt/qemu/

## Example 2:
sudo guestfish --rw -a os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2
run
list-filesystems

# run commands in qcow2 images
## Example 1:
virt-customize -a  os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2  --run-command 'echo hello >/root/hello.txt'
## Example 2:
sudo virt-customize -a os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  --run-command 'echo -e "auto ens3\niface ens3 inet dhcp" > /etc/network/interfaces.d/ens3.cfg'

# generate mac 
printf 'DE:AD:BE:EF:%02X:%02X\n' $((RANDOM%256)) $((RANDOM%256))
```

##### ğŸŒ IP

```sh
# list links
ip link show

# create bridge
ip link add br0 type bridge
```

##### ğŸŒ‰ Brctl

```sh
# list links
ip link show

# create bridge
ip link add br0 type bridge
```

##### ğŸ’¾ qemu-img

```sh
# create image
qemu-img create -f qcow2 vm-disk-debian-12.qcow2 20G

# convert vmdk to qcow2 image
qemu-img convert \
  -f vmdk \
  -O qcow2 os-images/Debian_12.0.0_VMM/Debian_12.0.0_VMM_LinuxVMImages.COM.vmdk os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -p \
  -m16

# check image
qemu-img info os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2
```

##### ğŸ–¥ï¸ Qemu-System-X86_64

```sh
# create vm with ISO
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm -hda vm-disk-debian-12.qcow2 \
  -cdrom /home/vagrant/isos/debian/debian-12.8.0-amd64-DVD-1.iso  \
  -boot d \
  -m 2048 \
  -smp cpus=2 \
  -k pt-br

# create vm with ISO using vnc in no gui servers \ ssh connections

## create ssh tunel in host
 ssh -l vagrant -L 5902:localhost:5902  192.168.0.131

## create vm 
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -k pt-br \
  -vnc :2 \
  -device qemu-xhci \
  -device usb-tablet \
  -device ide-cd,bus=ide.1,drive=cdrom,bootindex=1 \
  -drive id=cdrom,media=cdrom,if=none,file=/home/vagrant/isos/debian/debian-12.8.0-amd64-DVD-1.iso \
  -hda vm-disk-debian-12.qcow2 \
  -boot order=d \
  -vga std \
  -display none \
  -monitor stdio

# create vm with OS Image - qcow2

## create vm
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -k pt-br \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2

## create vm with custom kernel params
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -kernel /vmlinuz \
  -initrd /initrd.img \
  -append "root=/dev/mapper/debian--vg-root ro fastboot console=ttyS0" \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -k pt-br \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2

## create vm with and attach disk
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -hdb vmdisk-debian12.qcow2 \
  -drive file=vmdisk-extra-debian12.qcow2,index=2,media=disk,if=ide \
  -netdev bridge,id=net0,br=qemubr0 \
  -device virtio-net-pci,netdev=net0
  
## create vm network netdev user
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -netdev user,id=mynet0,net=192.168.0.150/24,dhcpstart=192.168.0.155,hostfwd=tcp::2222-:22 \
  -device virtio-net-pci,netdev=mynet0

## create vm network netdev tap (Private Network)
ip link add br0 type bridge ; ifconfig br0 up
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -netdev tap,id=br0 \
  -device e1000,netdev=br0,mac=DE:AD:BE:EF:1A:24

## create vm with public bridge
#create a public bridge : https://www.linux-kvm.org/page/Networking

qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -k pt-br \
  -vnc :2 \
  -device qemu-xhci \
  -device usb-tablet \
  -vga std \
  -display none \
  -netdev bridge,id=net0,br=qemubr0 \
  -device virtio-net-pci,netdev=net0

## get a ipv4 ip - open ssh in vm and:
dhcpclient ens4
```

#### Monitor qemu Monitor

Para iniciar o monitor Qemu no uso de linha de comando**-Monitor stdio**param in**Qemu-System-X86_64**

```sh
qemu-system-x86_64 -monitor stdio
```

Saia Qemu-Monitor:

```sh
ctrl+alt+2
```

```sh
# Managment
info status # vm info
info cpus # cpu information
info network # network informations
stop # pause vm
cont # start vm in status pause
system_powerdown # poweroff vm
system_reset # restart monitor


# Blocks
info block # block info
boot_set d # force boot iso
change ide1-cd0  /home/vagrant/isos/debian/debian-12.8.0-amd64-DVD-1.iso  # attach cdrom
eject ide1-cd0 # detach cdrom

# Snapshots
info snapshots # list snapshots
savevm snapshot-01  # create snapshot
loadvm snapshot-01 # restore snapshot
delvm snapshot-01
```

#### ğŸ¤– Agente convidado

Para ativar, use:

```sh
qemu-system-x86_x64
 -chardev socket,path=/tmp/qga.sock,server=on,wait=off,id=qga0 \
 -device virtio-serial \
 -device virtserialport,chardev=qga0,name=org.qemu.guest_agent.0
```

<p align="right">(<a href="#topic-351.3">back to sub Topic 351.3</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351.4"></a>

### ğŸ¢ 351.4 Libvirt MÃ¡quina virtual Gerenciamento de mÃ¡quinas

![libvirt](images/libvirt.png)

![libvirt-network](images/libvirt-default-network.jpg)

**Peso:**9

**DescriÃ§Ã£o:**Os candidatos devem ser capazes de gerenciar hosts de virtualizaÃ§Ã£o e mÃ¡quinas virtuais ('LibVirt Domains') usando o LibVirt e as ferramentas relacionadas.

**Principais Ã¡reas de conhecimento:**

-   Entenda a arquitetura do Libvirt
-   Gerenciar conexÃµes e nÃ³s da LibVirt
-   Crie e gerencie domÃ­nios qemu e xen, incluindo instantÃ¢neos
-   Gerenciar e analisar o consumo de recursos de domÃ­nios
-   Crie e gerencie pools e volumes de armazenamento
-   Crie e gerencie redes virtuais
-   Migrar domÃ­nios entre nÃ³s
-   Entenda como o LibVirt interage com Xen e Qemu
-   Entenda como a LibVirt interage com serviÃ§os de rede, como Dnsmasq e RadVD
-   Entenda os arquivos de configuraÃ§Ã£o do LibVirt XML
-   ConsciÃªncia de VirtLogd e Virtlockd

#### ğŸ“‹ 351.4 Objetos citados

```sh
libvirtd
/etc/libvirt/
/var/lib/libvirt
/var/log/libvirt
virsh (including relevant subcommands) 
```

#### ğŸ› ï¸ 351.4 Comandos importantes

##### ğŸ–¥ï¸ Virsh

```sh
# using env variable for set virsh uri (local or remotly)
export LIBVIRT_DEFAULT_URI=qemu:///system
export LIBVIRT_DEFAULT_URI=xen+ssh://vagrant@192.168.0.130
export LIBVIRT_DEFAULT_URI='xen+ssh://vagrant@192.168.0.130?keyfile=/home/vagrant/.ssh/skynet-key-ecdsa'

# COMMONS

# get helps
virsh help
virsh help pool-create

# view version
virsh version

# view system info
sudo virsh sysinfo

# view node info
virsh nodeinfo

# hostname
virsh hostname

# check vcn allocated port
virsh vncdisplay <domain_id>
virsh vncdisplay <domain_name>
virsh vncdisplay rocky9-server01 

# HYPERVISIONER

# view libvirt hypervisioner connection
virsh uri

# list valid hypervisioners
virt-host-validate
virt-host-validate qemu

# test connetion uri(vm test)
virsh -c test:///default list

# connect remotly
virsh -c xen+ssh://vagrant@192.168.0.130
virsh -c xen+ssh://vagrant@192.168.0.130 list
virsh -c qemu+ssh://vagrant@192.168.0.130/system list

# connect remotly without enter password
virsh -c 'xen+ssh://vagrant@192.168.0.130?keyfile=/home/vagrant/.ssh/skynet-key-ecdsa'

# STORAGE

# list storage pools
virsh pool-list --details

# list all storage pool
virsh pool-list --all --details

# get a pool configuration
virsh pool-dumpxml default

# get pool info
virsh pool-info default

# create a storage pool
virsh pool-define-as --name default --type dir --target /var/lib/libvirt/images

# create a storage pool with dumpxml
virsh pool-create --overwrite --file configs/kvm/libvirt/pool.xml

# start storage pool
virsh pool-start default

# set storage pool for autostart
virsh pool-autostart default

# stop storage pool
virsh pool-destroy linux

# delete xml storage pool file
virsh pool-undefine linux

# edit storage pool
virsh pool-edit linux

# list volumes
virsh vol-list linux

# get volume infos
virsh vol-info Debian_12.0.0.qcow2 os-images
virsh vol-info --pool os-images Debian_12.0.0.qcow2 

# get volume xml
virsh vol-dumpxml rocky9-disk1 default

# create volume
virsh vol-create-as default --format qcow2 disk1 10G

# delete volume
virsh vol-delete  disk1 default

# DOMAINS \ INSTANCES \ VIRTUAL MACHINES

# list domain\instance\vm
virsh list
virsh list --all

# create domain\instance\vm
virsh create configs/kvm/libvirt/rocky9-server03.xml

# view domain\instance\vm info
virsh dominfo rocky9-server01

# view domain\instance\vm xml
virsh dumpxml rocky9-server01

# edit domain\instance\vm xml
virsh edit rocky9-server01

# stop domain\instance\vm
virsh shutdown rocky9-server01 # gracefully
virsh destroy 1
virsh destroy rocky9-server01

# suspend domain\instance\vm
virsh suspend rocky9-server01

# resume domain\instance\vm
virsh resume rocky9-server01

# start domain\instance\vm
virsh start rocky9-server01

# remove domain\instance\vm
virsh undefine rocky9-server01

# remove domain\instance\vm and storage volumes
virsh undefine rocky9-server01 --remove-all-storage

# save domain\instance\vm
virsh save rocky9-server01 rocky9-server01.qcow2

# restore domain\instance\vm
virsh restore rocky9-server01.qcow2

# list snapshots
virsh snapshot-list rocky9-server01

# create snapshot
virsh snapshot-create rocky9-server01

# restore snapshot
virsh snapshot-revert rocky9-server01 1748983520

# view snapshot xml
virsh snapshot-info rocky9-server01 1748983520

# dumpxml snapshot
virsh snapshot-dumpxml rocky9-server01 1748983520

# xml snapshot path
/var/lib/libvirt/qemu/snapshot/rocky9-server01/

# view snapshot info
virsh snapshot-info rocky9-server01 1748983671

# edit snapshot
virsh snapshot-edit rocky9-server01 1748983520

# delete snapshot
virsh snapshot-delete rocky9-server01 1748983520

# DEVICES

# list block devices
virsh domblklist rocky9-server01 --details

# add cdrom media 
virsh change-media rocky9-server01 sda /home/vagrant/isos/rocky/Rocky-9.5-x86_64-minimal.iso
virsh attach-disk rocky9-server01 /home/vagrant/isos/rocky/Rocky-9.5-x86_64-minimal.iso sda --type cdrom --mode readonly

# remove cdrom media
virsh change-media rocky9-server01 sda --eject

# add new disk
virsh attach-disk rocky9-server01  /var/lib/libvirt/images/rocky9-disk2  vdb --persistent

# remove disk
virsh detach-disk rocky9-server01 vdb --persistent

# RESOURCES (CPU and Memory)

# get cpu infos
virsh vcpuinfo rocky9-server01 --pretty
virsh dominfo rocky9-server01 | grep 'CPU'

# get vcpu count
virsh vcpucount rocky9-server01

# set vcpus maximum config
virsh setvcpus rocky9-server01 --count 4 --maximum --config
virsh shutdown rocky9-server01
virsh start rocky9-server01

# set vcpu current config
virsh setvcpus rocky9-server01 --count 4 --config

# set vcpu current live
virsh setvcpus rocky9-server01 --count 3 --current
virsh setvcpus rocky9-server01 --count 3 --live

# configure vcpu afinity config
virsh vcpupin rocky9-server01 0 7 --config
virsh vcpupin rocky9-server01 1 5-6 --config

# configure vcpu afinity current
virsh vcpupin rocky9-server01 0 7
virsh vcpupin rocky9-server01 1 5-6

# set maximum memory config
virsh setmaxmem rocky9-server01 3000000 --config
virsh shutdown rocky9-server01
virsh start rocky9-server01

# set current memory config
virsh setmem rocky9-server01 2500000 --current

# NETWORK

# get netwwork bridges
brctl show

# get iptables rules for libvirt
sudo iptables -L -n -t  nat

# list network
virsh net-list --all

# set default network
virsh net-define /etc/libvirt/qemu/networks/default.xml

# get network infos
virsh net-info default

# get xml network
virsh net-dumpxml default

# xml file
cat /etc/libvirt/qemu/networks/default.xml

# dhcp config
sudo cat /etc/libvirt/qemu/networks/default.xml | grep -A 10 dhcp
sudo cat /var/lib/libvirt/dnsmasq/default.conf

# get domain ipp address
virsh net-dhcp-leases default
virsh net-dhcp-leases default --mac 52\:54\:00\:89\:19\:86

# edit network
virsh net-edit default

# get domain network detais
virsh domiflist debian-server01

# path for network filter files
/etc/libvirt/nwfilter/

# list network filters
virsh nwfilter-list

# create network filter - block icmp traffic
virsh nwfilter-define block-icmp.xml
# virsh edit Debian-Server
    #  <interface type='network'>
    #        ...
    #        <filterref filter='block-icmp'/>
    #        ...
    # </interface>
# virsh destroy debian-server01
# virsh start debian-server01

# delete network filter
virsh nwfilter-undefine block-icmp

# get xml network filter
virsh nwfilter-dumpxml block-icmp
```

###### ğŸ—ï¸ Virt-Install

```sh
# list os variants
virt-install --os-variant list
osinfo-query os

# create domain\instance\vm with iso file
virsh vol-create-as default --format qcow2 rocky9-disk1 20G
virt-install --name rocky9-server01 \
--vcpus 2 \
--cpu host \
--memory 2048 \
--disk vol=default/rocky9-disk1 \
--cdrom /home/vagrant/isos/rocky/Rocky-9.5-x86_64-minimal.iso \
--os-variant=rocky9 \
--graphics vnc,listen=0.0.0.0,port=5905

# create debian domain\instance\vm with qcow2 file
virt-install --name debian-server01 \
--vcpus 2 \
--ram 2048 \
--disk vol=os-images/Debian_12.0.0.qcow2 \
--import \
--osinfo detect=on \
--graphics vnc,listen=0.0.0.0,port=5906 \
--network network=default \
--noautoconsole

# create rocky9 domain\instance\vm with qcow2 file
virt-install --name rocky9-server02 \
--vcpus 2 \
--ram 2048 \
--disk path=os-images/RockyLinux_9.4_VMG/RockyLinux_9.4.qcow2,format=qcow2,bus=virtio \
--import \
--osinfo detect=on \
--graphics vnc,listen=0.0.0.0,port=5907 \
--network bridge=qemubr0,model=virtio \
--noautoconsole

# open domain\instance\vm gui console
virt-viewer debian-server01

# check metadata domain\instance\vm file (if uri is qemu:////system)
less /etc/libvirt/qemu/debian-server01.xml
```

<p align="right">(<a href="#topic-351.4">back to sub Topic 351.4</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351.5"></a>

### ğŸ’¾ 351.5 Gerenciamento de imagem em disco da mÃ¡quina virtual

![disk-managment](images/virtual-machine-disk.png)

**Peso:**3

**DescriÃ§Ã£o:**Os candidatos devem poder gerenciar imagens de disco de mÃ¡quinas virtuais. Isso inclui a conversÃ£o de imagens de disco entre vÃ¡rios formatos e hipervisores e acesso a dados armazenados em uma imagem.

**Principais Ã¡reas de conhecimento:**

-   Entenda os recursos de vÃ¡rios formatos de imagem de disco virtual, como imagens cruas, QCOW2 e VMDK
-   Gerenciar imagens de disco da mÃ¡quina virtual usando Qemu-IMG
-   Monte PartiÃ§Ãµes e Arquivos de Acesso contidos em imagens de disco da mÃ¡quina virtual usando LibGuestfish
-   Copie o conteÃºdo do disco fÃ­sico para uma imagem de disco da mÃ¡quina virtual
-   Migrar o conteÃºdo do disco entre vÃ¡rios formatos de imagem de disco da mÃ¡quina virtual
-   ConsciÃªncia do formato de virtualizaÃ§Ã£o aberta (OVF)

#### ğŸ“‹ 351.5 Objetos citados

```sh
qemu-img
guestfish (including relevant subcommands)
guestmount
guestumount
virt-cat
virt-copy-in
virt-copy-out
virt-diff
virt-inspector
virt-filesystems
virt-rescue
virt-df
virt-sparsify
virt-p2v
virt-p2v-make-disk
virt-v2v
```

#### ğŸ› ï¸ 351.5 Comandos importantes

##### ğŸ’¾ 351.5.1 Qemu-img

```sh
# Display detailed information about a disk image
qemu-img info UbuntuServer_24.04.qcow2

# Create a new 22G raw disk image (default format is raw)
qemu-img create new-disk 22G

# Create a new 22G disk image in qcow2 format
qemu-img create -f qcow2 new-disk2 22G

# Convert a VDI image to raw format using 5 threads and show progress
qemu-img convert -f vdi -O raw Ubuntu-Server.vdk new-Ubuntu.raw -m5 -p

# Convert vmdk to qcow2 image
qemu-img convert \
-f vmdk \
-O qcow2 os-images/UbuntuServer_24.04_VM/UbuntuServer_24.04_VM_LinuxVMImages.COM.vmdk \
os-images/UbuntuServer_24.04_VM/UbuntuServer_24.04.qcow2 \
-p \
-m16

# Resize a raw image to 30G
qemu-img resize -f raw new-disk 30G

# Resize a qcow2 image to 15G(actual size 30Gdisk 30G)
qemu-img resize -f raw --shrink new-disk 15G

# Snapshots

# List all snapshots in the image
qemu-img snapshot -l new-disk2.qcow2

# Create a snapshot named SNAP1
qemu-img snapshot -c SNAP1 disk

# Apply a snapshot by ID or name
qemu-img snapshot -a 123456789 disk

# Delete the snapshot named SNAP1
qemu-img snapshot -d SNAP1 disk
```

##### ğŸŸ Peixe -convidado

```sh
# set enviroment variables for guestfish
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg

# Launch guestfish with a disk image
guestfish -a UbuntuServer_24.04.qcow2
#run
#list-partitions

# Run the commands in a script file
guestfish -a UbuntuServer_24.04.qcow2 -m /dev/sda -i < script.ssh

# Interactively run commands
guestfish --rw -a UbuntuServer_24.04.qcow2 <<'EOF'
run
list-filesystems
EOF

# Copy a file from the guest image to the host
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg
sudo guestfish --rw -a UbuntuServer_24.04.qcow2 -i <<'EOF'
copy-out /etc/hostname /tmp/
EOF

# Copy a file from the host into the guest image
echo "new-hostname" > /tmp/hostname
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg
sudo guestfish --rw -a UbuntuServer_24.04.qcow2 -i <<'EOF'
copy-in /tmp/hostname /etc/
EOF

# View contents of a file in the guest image
guestfish --ro -a UbuntuServer_24.04.qcow2 -i <<'EOF'
cat /etc/hostname
EOF

# List files in the guest image
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg
guestfish --rw -a UbuntuServer_24.04.qcow2 -i <<'EOF'
ls /home/ubuntu
EOF

# Edit a file in the guest image
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg
guestfish --rw -a UbuntuServer_24.04.qcow2 -i <<'EOF'
edit /etc/hosts
EOF
```

###### ğŸ—‚ï¸ Guestrount

```sh
# Mount a disk image to a directory
guestmount -a UbuntuServer_24.04.qcow2 -m /dev/ubuntu-vg/ubuntu-lv /mnt/ubuntu
# domain
guestmount -d rocky9-server02 -m /dev/ubuntu-vg/ubuntu-lv /mnt/ubuntu 

# Mount a specific partition from a disk image
guestmount -a UbuntuServer_24.04.qcow2 -m /dev/sda2 /mnt/ubuntu
# domain
guestmount -d debian-server01 --ro -m  /dev/debian-vg/root /mnt/debian
```

###### ğŸ—‘ï¸ GuestUmount

```sh
# Umount a disk image to a directory
sudo guestunmount /mnt/ubuntu
```

##### ğŸ“Š Virt-df

```sh
# Show free and used space on virtual machine filesystems
virt-df UbuntuServer_24.04.qcow2 -h
virt-df -d rocky9-server02 -h
```

##### ğŸ—ƒï¸ Fil-Filesystems

```sh
# List filesystems, partitions, and logical volumes in a VM disk image (disk image)
virt-filesystems -a UbuntuServer_24.04.qcow2 --all --long -h

# List filesystems, partitions, and logical volumes in a VM disk image (domain)
virt-filesystems -d debian-server01 --all --long -h
```

##### ğŸ” Inspetor de Virt

```sh
# Inspect and report on the operating system in a VM disk image
virt-inspector -a UbuntuServer_24.04.qcow2 #(disk)
virt-inspector -d debian-server01 #(domain) 
```

##### ğŸ± Virt-Cat

```sh
# Display the contents of a file inside a VM disk image
virt-cat -a UbuntuServer_24.04.qcow2 /etc/hosts
virt-cat -d debian-server01 /etc/hosts #(domain)
```

##### ğŸ”€ Virt-Diff

```sh
# Show differences between two VM disk images
virt-diff -a UbuntuServer_24.04.qcow2 -A Rocky-Linux.qcow2
```

##### ğŸ§¹ Virt-Spparsify

```sh
# Make a VM disk image smaller by removing unused space
virt-sparsify UbuntuServer_24.04.qcow2 UbuntuServer_24.04-sparse.qcow2
```

##### ğŸ“ Virt-ressectize

```sh
# Resize a VM disk image or its partitions
virt-filesystems -a UbuntuServer_24.04.qcow2 --all --long -h #(check size of partitions)
qemu-img create -f qcow2 UbuntuServer_24.04-expanded.qcow2 100G #(create new disk image with 100G)
virt-resize --expand /dev/ubuntu-vg/ubuntu-lv \
UbuntuServer_24.04.qcow2 UbuntuServer_24.04-expanded.qcow2

```

##### ğŸ“¥ Virt-copy-in

```sh
# Copy files from the host into a VM disk image

virt-copy-in -a UbuntuServer_24.04.qcow2 ~vagrant/test-virt-copy-in.txt /home/ubuntu
```

##### ğŸ“¤ Virt-copy-out

```sh
# Copy files from a VM disk image to the host
virt-copy-out -a UbuntuServer_24.04.qcow2 /home/ubuntu/.bashrc /tmp
```

##### ğŸ“‹ Virt-ls

```sh
# List files and directories inside a VM disk image
virt-ls -a UbuntuServer_24.04.qcow2 /home/ubuntu
```

##### ğŸš‘ Virt-rescue

```sh
# Launch a rescue shell on a VM disk image for recovery
virt-rescue -a UbuntuServer_24.04.qcow2
```

##### ğŸ§° Virt-Sysprep

```sh
# Prepare a VM disk image for cloning by removing system-specific data
virt-sysprep -a UbuntuServer_24.04.qcow2
```

##### ğŸ”„ Virt-V2V

```sh
# Convert a VM from a foreign hypervisor to run on KVM
virt-v2v -i disk input-disk.img -o local -os /var/tmp
```

##### ğŸ”„ Virt-P2V

```sh
# Convert a physical machine to use KVM
```

##### ğŸ’½ Virt-P2V-Make-Disk

```sh
# Create a bootable disk image for physical to virtual conversion
sudo virt-p2v-make-disk -o output.img
```

#### ğŸ“ 351.5 Notas

##### ğŸ“¦ OVF: Formato de virtualizaÃ§Ã£o aberto

OVF: um formato aberto que define um padrÃ£o para embalagem e distribuiÃ§Ã£o de mÃ¡quinas virtuais em diferentes ambientes.

O pacote gerado possui a extensÃ£o .ova e contÃ©m os seguintes arquivos:

-   .ovf: arquivo xml com metadados definindo o ambiente da mÃ¡quina virtual
-   Arquivos de imagem: .vmdk, .vhd, .vhdx, .qcow2, .raw
-   Arquivos adicionais: metadados, instantÃ¢neos, configuraÃ§Ã£o, hash

<p align="right">(<a href="#topic-351.5">back to sub Topic 351.5</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-352"></a>

## ğŸ“¦ TÃ³pico 352: VirtualizaÃ§Ã£o de contÃªineres

* * *

<a name="topic-352.1"></a>

### ğŸ§  352.1 conceitos de virtualizaÃ§Ã£o de contÃªineres

![virtualization-container](images/virtualization-container.png)

```mermaid
timeline
    title Time Line Containers Evolution
    1979 : chroot
    2000 : FreeBSD Jails
    2002 : Linux Namespaces
    2005 : Solaris Containers
    2007 : cgroups
    2008 : LXC
    2013 : Docker
    2015 : Kubernetes
```

* * *

**Peso:**7

**DescriÃ§Ã£o:**Os candidatos devem entender o conceito de virtualizaÃ§Ã£o de contÃªineres. Isso inclui a compreensÃ£o dos componentes do Linux usados para implementar a virtualizaÃ§Ã£o de contÃªineres, bem como usar as ferramentas padrÃ£o do Linux para solucionar esses componentes.

**Principais Ã¡reas de conhecimento:**

-   Entenda os conceitos de sistema e contÃªiner de aplicativos
-   Entender e analisar namespaces de kernel
-   Entender e analisar grupos de controle
-   Entender e analisar recursos
-   Entenda o papel do Seccomp, Selinux e Apparmor for Container Virtualization
-   Entenda como o LXC e o Docker alavancam namespaces, cgroups, recursos, Seccomp e Mac
-   Entenda o princÃ­pio de Runc
-   Entenda o princÃ­pio de Cri-O e Containerd
-   ConsciÃªncia das especificaÃ§Ãµes de tempo de execuÃ§Ã£o da OCI e imagem
-   ConsciÃªncia da interface de tempo de execuÃ§Ã£o do contÃªiner Kubernetes (CRI)
-   ConsciÃªncia de Podman, Buildah e Scopeo
-   ConsciÃªncia de outras abordagens de virtualizaÃ§Ã£o de contÃªineres no Linux e em outros sistemas operacionais gratuitos, como RKT, OpenVZ, Systemd-Nspawn ou BSD prisÃµes

* * *

#### ğŸ“‹ 352.1 Objetos citados

```sh
nsenter
unshare
ip (including relevant subcommands)
capsh
/sys/fs/cgroups
/proc/[0-9]+/ns
/proc/[0-9]+/status
```

* * *

#### ğŸ§  Entendendo os recipientes

![container](images/containers1.png)

Os contÃªineres sÃ£o uma tecnologia de virtualizaÃ§Ã£o leve que empacota aplicativos, juntamente com as dependÃªncias necessÃ¡rias - cÃ³digo, bibliotecas, variÃ¡veis de ambiente e arquivos de configuraÃ§Ã£o - em unidades isoladas, portÃ¡teis e reproduzÃ­veis.

> Em termos simples: um contÃªiner Ã© uma caixa independente que executa seu aplicativo da mesma maneira, em qualquer lugar.

##### ğŸ’¡ O que Ã© um contÃªiner?

Ao contrÃ¡rio das mÃ¡quinas virtuais (VMs), os contÃªineres nÃ£o virtualizam o hardware. Em vez disso, eles virtualizam o sistema operacional. Os contÃªineres compartilham o mesmo kernel Linux com o host, mas cada um opera em um espaÃ§o de usuÃ¡rio totalmente isolado.

ğŸ“Œ Recipientes versus mÃ¡quinas virtuais:

| Recurso                 | ContÃªineres                     | MÃ¡quinas virtuais                           |
| ----------------------- | ------------------------------- | ------------------------------------------- |
| OS Kernel               | Compartilhado com o host        | Cada VM tem seu prÃ³prio sistema operacional |
| Hora de inicializaÃ§Ã£o   | RÃ¡pido (segundos ou menos)      | Lento (minutos)                             |
| Tamanho da imagem       | Leve (MBS)                      | Pesado (GBS)                                |
| EficiÃªncia de recursos  | Alto                            | Mais baixo                                  |
| Mecanismo de isolamento | Recursos de kernel (namespaces) | Hipervisor                                  |

##### ğŸ”‘ CaracterÃ­sticas -chave dos contÃªineres

ğŸ”¹**Leve**: Compartilhe o kernel do OS host, reduzindo a sobrecarga e permitindo uma inicializaÃ§Ã£o rÃ¡pida.

ğŸ”¹**PortÃ¡til**: Execute de forma consistente em diferentes ambientes (dev, estadiamento, Prod, Cloud, On-Prem).

ğŸ”¹**Isolado**: Use namespaces para isolamento de processo, rede e sistema de arquivos.

ğŸ”¹**Eficiente**: Habilite maior densidade e melhor utilizaÃ§Ã£o de recursos do que as VMs tradicionais.

ğŸ”¹**EscalÃ¡vel**: Ajuste perfeito para microsserviÃ§os e arquitetura nativa em nuvem.

##### ğŸ§± Tipos de recipientes

1.  ContÃªineres do sistema

    -   Projetado para executar o sistema operacional inteiro, assemelhar -se a mÃ¡quinas virtuais.
    -   Suporte a vÃ¡rios processos e serviÃ§os do sistema (init, syslog).
    -   Ideal para aplicaÃ§Ãµes legadas ou monolÃ­ticas.
    -   Exemplo: lxc, libvirt-lxc.
2.  ContÃªineres de aplicaÃ§Ã£o

    -   Projetado para executar um Ãºnico processo.
    -   Sem estado, efÃªmero e horizontalmente escalÃ¡vel.
    -   Utilizado amplamente em ambientes modernos de DevOps e Kubernetes.
    -   Exemplo: Docker, Containerd, Cri-O.

##### ğŸš€ Tempos de contÃªineres populares

| Tempo de execuÃ§Ã£o | DescriÃ§Ã£o                                                                                 |
| ----------------- | ----------------------------------------------------------------------------------------- |
| **Docker**        | A CLI/daemon mais amplamente adotada para construir e executar recipientes.               |
| **contÃªiner**     | Docker e Kubernetes com tempo de execuÃ§Ã£o leves.                                          |
| **CRI-O**         | Tempo de execuÃ§Ã£o nativo de Kubernetes para contÃªineres OCI.                              |
| **LXC**           | ContÃªineres tradicionais do sistema Linux, mais prÃ³ximos do sistema operacional completo. |
| **Rkt**           | Tempo de execuÃ§Ã£o focado na seguranÃ§a (depreciado).                                       |

##### ğŸ” InternaÃ§Ãµes de contÃªineres e elementos de seguranÃ§a

| Componente            | Papel                                                                  |
| --------------------- | ---------------------------------------------------------------------- |
| **Namespaces**        | Isolar processos, usuÃ¡rios, montagens, redes.                          |
| **CGROUPS**           | Controle e limite o uso de recursos (CPU, memÃ³ria, IO).                |
| **Recursos**          | Controle de privilÃ©gios de granulaÃ§Ã£o fina dentro de recipientes.      |
| **Seccomp**           | Restringem os syscalls permitidos para reduzir a superfÃ­cie de ataque. |
| **APARMOR / SELinux** | ExecuÃ§Ã£o obrigatÃ³ria de controle de acesso no nÃ­vel do kernel.         |

* * *

#### ğŸ§  Entendendo o Chroot - Alterar o diretÃ³rio raiz no Unix/Linux

![chroot](images/chroot.png)

##### O que Ã© chroot?

O Chroot (abreviaÃ§Ã£o de ROOTE de mudanÃ§a) Ã© uma chamada e comando do sistema em sistemas operacionais do tipo UNIX que altera o diretÃ³rio raiz aparente (/) para o processo de execuÃ§Ã£o atual e seus filhos. Isso cria um ambiente isolado, comumente referido como uma prisÃ£o de chroot.

##### ğŸ§± Casos de propÃ³sito e uso

-   ğŸ”’ Isolar solicitaÃ§Ãµes de seguranÃ§a (prisÃ£o).
-   ğŸ§ª Crie ambientes de teste sem afetar o restante do sistema.
-   Recovery RecuperaÃ§Ã£o do sistema (por exemplo, inicializaÃ§Ã£o no LiveCD e Chroot no sistema instalado).
-   ğŸ“¦ Construindo pacotes de software em um ambiente controlado.

##### ğŸ“ Estrutura mÃ­nima necessÃ¡ria

O ambiente de chroot deve ter seus prÃ³prios arquivos e estrutura essenciais:

```sh
/mnt/myenv/
â”œâ”€â”€ bin/
â”‚   â””â”€â”€ bash
â”œâ”€â”€ etc/
â”œâ”€â”€ lib/
â”œâ”€â”€ lib64/
â”œâ”€â”€ usr/
â”œâ”€â”€ dev/
â”œâ”€â”€ proc/
â””â”€â”€ tmp/
```

Use LDD para identificar as bibliotecas necessÃ¡rias:

```sh
ldd /bin/bash
```

##### ğŸš¨ LimitaÃ§Ãµes e consideraÃ§Ãµes de seguranÃ§a

-   Chroot nÃ£o Ã© um limite de seguranÃ§a como recipientes ou VMs.
-   Um usuÃ¡rio privilegiado (root) dentro da prisÃ£o pode potencialmente sair.
-   Nenhum isolamento de namespaces de processo, dispositivos ou recursos no nÃ­vel do kernel.

Para um isolamento mais forte, considere alternativas como:

-   ContÃªineres Linux (LXC, Docker)
-   MÃ¡quinas Virtuais (KVM, Qemu)
-   Namespaces de kernel e cgroups

##### ğŸ§ª Teste o chroot com Debootstrap

```sh
# download debain files
sudo debootstrap stable ~vagrant/debian http://deb.debian.org/debian
sudo chroot ~vagrant/debian bash
```

##### : ğŸ§ª CHROOT LAB

Use este script para laboratÃ³rio:[chroot.sh](scripts/container/chroot.sh)

[![asciicast](https://asciinema.org/a/PWkjazgTXll9678Qy6LLOaKdN.svg)](https://asciinema.org/a/PWkjazgTXll9678Qy6LLOaKdN)

* * *

#### ğŸ§  Entendendo namespaces Linux

![linux-namespaces](images/linux-namespaces2.png)

Os namespaces sÃ£o um recurso principal do kernel Linux que permite o isolamento no nÃ­vel do processo. Eles criam "visualizaÃ§Ãµes" separadas dos recursos globais do sistema - como IDs de processo, redes, sistemas de arquivos e usuÃ¡rios - para que cada grupo de processos acredite que estÃ¡ em execuÃ§Ã£o em seu prÃ³prio sistema.

> Em termos simples: os namespaces enganam um processo a pensar que ele Ã© dono da mÃ¡quina, mesmo que esteja apenas compartilhando -a.

Esta Ã© a base para o isolamento de contÃªineres.

##### ğŸ” O que os namespaces isolam?

Cada tipo de espaÃ§o para nome isola um recurso especÃ­fico do sistema. Juntos, eles compÃµem a caixa de areia em que um contÃªiner opera:

| EspaÃ§o para nome | Isolados ...                              | Exemplo do mundo real                                           |
| ---------------- | ----------------------------------------- | --------------------------------------------------------------- |
| **PID**          | IDs de processo                           | Processos dentro de um recipiente, veja um espaÃ§o PID diferente |
| **Montar**       | Pontos de montagem do sistema de arquivos | Cada contÃªiner vÃª seu prÃ³prio sistema de arquivos raiz          |
| **Rede**         | Pilha de rede                             | ContÃªineres tÃªm IPs isolados, interfaces e rotas                |
| **Uts**          | Nome de host e nome de domÃ­nio            | Cada contÃªiner define seu prÃ³prio nome de host                  |
| **IPC**          | MemÃ³ria compartilhada e semÃ¡foros         | Impede a comunicaÃ§Ã£o entre processos entre contÃªineres          |
| **UsuÃ¡rio**      | IDs de usuÃ¡rio e grupo                    | Ativa a raiz falsa (UID 0) dentro do recipiente                 |
| **CGROUP (V2)**  | AssociaÃ§Ã£o do grupo de controle           | LaÃ§os em controles de recursos como CPU e limites de memÃ³ria    |

##### ğŸ§ª Analogia visual

![linux-namespaces](images/linux-namespaces.png)

Imagine um prÃ©dio de escritÃ³rios compartilhado:

-   Todos os inquilinos compartilham a mesma base (Linux Kernel).
-   Cada empresa possui seu prÃ³prio escritÃ³rio (espaÃ§o para nome): bloqueios diferentes, mÃ³veis, linhas telefÃ´nicas e nome da empresa.
-   Para cada inquilino, parece seu prÃ³prio prÃ©dio.

Ã‰ exatamente assim que os contÃªineres experimentam o sistema - isolados, mas eficientes.

##### ğŸ”§ Como os contÃªineres usam namespaces

Quando vocÃª executa um contÃªiner (por exemplo, com Docker ou Podman), o tempo de execuÃ§Ã£o cria um novo conjunto de espaÃ§os para nome:

```bash
docker run -it --rm alpine sh
```

Este comando fornece o processo:

-   Um novo espaÃ§o para nome de PID â†’ Ã© o processo 1 dentro do contÃªiner.
-   Um novo espaÃ§o para nome de rede â†’ sua prÃ³pria Ethernet virtual.
-   Um espaÃ§o para nome de montagem â†’ Um sistema de arquivos raiz especÃ­fico do contÃªiner.
-   Outros espaÃ§os para nome, dependendo da configuraÃ§Ã£o (usuÃ¡rio, IPC, etc.)

O resultado: um ambiente de tempo de execuÃ§Ã£o leve e isolado que se comporta como um sistema separado.

##### âš™ï¸ Recursos de kernel complementares

Os namespaces oculam recursos de contÃªineres. Mas para controlar o quanto eles podem usar e o que podem fazer, precisamos de mecanismos adicionais:

###### ğŸ”© CGROUPS (Grupos de controle)

Os cgroups permitem que o kernel limite, priorize e monitore o uso de recursos entre os grupos de processos.

| Recurso      | Use exemplos de casos                       |
| ------------ | ------------------------------------------- |
| CPU          | Limitar o tempo da CPU por contÃªiner        |
| MemÃ³ria      | Cap Ram Uso                                 |
| E/S de disco | OperaÃ§Ãµes de leitura/gravaÃ§Ã£o do acelerador |
| Rede (V2)    | RestriÃ§Ãµes de largura de banda              |

ğŸ›¡ï¸ Impede o problema "vizinho barulhento", impedindo que um contÃªiner consumindo todos os recursos do sistema.

###### ğŸ§± Capacidades

O Linux tradicional usa um modelo de privilÃ©gio binÃ¡rio: raiz (UID 0) pode fazer tudo, todo mundo Ã© limitado.

| Capacidade             | Permite ...                                                   |
| ---------------------- | ------------------------------------------------------------- |
| `CAP_NET_BIND_SERVICE` | LigaÃ§Ã£o a portas privilegiadas (por exemplo, 80, 443)         |
| `CAP_SYS_ADMIN`        | Uma poderosa captura para tarefas de administraÃ§Ã£o do sistema |
| `CAP_KILL`             | Enviando sinais para processos arbitrÃ¡rios                    |

Ao soltar recursos desnecessÃ¡rios, os contÃªineres podem executar apenas o que precisam - reduzindo o risco.

##### ğŸ” Mecanismos de seguranÃ§a

Usado em conjunto com namespaces e cgroups para bloquear o que um processo contÃªiner pode fazer:

| Recurso     | DescriÃ§Ã£o                                                                   |
| ----------- | --------------------------------------------------------------------------- |
| **Seccomp** | Lista de permissÃµes ou bloqueios de chamadas do sistema Linux (syscalls)    |
| **APARMOR** | Aplicar perfis de seguranÃ§a por aplicaÃ§Ã£o                                   |
| **Selinux** | Aplicar o controle de acesso obrigatÃ³rio com polÃ­ticas de sistema apertadas |

##### ğŸ§  Resumo para iniciantes

> âœ… Namespaces Isolle o que um contÃªiner pode ver
> âœ… CGROUPS Controle o que pode usar
> âœ… Capacidades e mÃ³dulos de seguranÃ§a definem o que pode fazer

Juntos, esses recursos do kernel formam a espinha dorsal tÃ©cnica do isolamento de contÃªineres-permitindo implantaÃ§Ã£o de aplicaÃ§Ã£o de alta densidade, seguranÃ§a e eficiÃªncia sem VMs completas.

##### ğŸ§ª Namespaces de laboratÃ³rio

Use este script para laboratÃ³rio:[namespace.sh](scripts/container/namespace.sh)

[![asciicast](https://asciinema.org/a/8H6iczCMO24VgjWqwCcXEKWBG.svg)](https://asciinema.org/a/8H6iczCMO24VgjWqwCcXEKWBG)

* * *

#### ğŸ§© Entendendo os cgroups (grupos de controle)

![cgroups](images/cgroups1.png)

##### ğŸ“Œ DefiniÃ§Ã£o

Os grupos de controle (CGROUPS) sÃ£o um recurso Linux Kernel introduzido em 2007 que permite limitar, explicar e isolar o uso de recursos (CPU, memÃ³ria, E/S de disco, etc.) de grupos de processos.

Os cgroups sÃ£o fortemente usados por tempos de execuÃ§Ã£o de contÃªineres de baixo nÃ­vel, como Runc e Crun, e alavancados por motores de contÃªineres como Docker, Podman e LXC para aplicar os limites dos recursos e fornecer isolamento entre os contÃªineres.

Os namespaces isolam o controle de cgroups.

Os namespaces criam ambientes separados para processos (como PID, rede ou montagens), enquanto o CGROUPS limitam e monitoram o uso de recursos (CPU, memÃ³ria, E/S) para esses processos.

âš™ï¸ Capacidades -chave

| Recurso                   | DescriÃ§Ã£o                                                        |
| ------------------------- | ---------------------------------------------------------------- |
| **LimitaÃ§Ã£o de recursos** | Impor limites para quanto de um recurso um grupo pode usar       |
| **PriorizaÃ§Ã£o**           | Alocar mais prioridade da CPU/IO para alguns grupos sobre outros |
| **Contabilidade**         | Rastrear o uso de recursos por grupo                             |
| **Controlar**             | Suspender, retomar ou matar processos a granel                   |
| **Isolamento**            | Impedir a fome de recursos entre os grupos                       |

##### ğŸ“¦ Subsistemas (controladores)

Os cgroups operam atravÃ©s dos controladores, cada um responsÃ¡vel pelo gerenciamento de um tipo de recurso:

| Subsistema | DescriÃ§Ã£o                                 |
| ---------- | ----------------------------------------- |
| `cpu`      | Controla a programaÃ§Ã£o da CPU             |
| `cpuacct`  | Gera relatÃ³rios de uso da CPU             |
| `memory`   | Limita e contas o uso da memÃ³ria          |
| `blkio`    | Limita a E/S do dispositivo de bloco      |
| `devices`  | Controla o acesso a dispositivos          |
| `freezer`  | Suspende/retoma a execuÃ§Ã£o de tarefas     |
| `net_cls`  | Pacotes de tags para modelagem de trÃ¡fego |
| `ns`       | Gerencia o acesso ao namespace (raro)     |

##### Layout Layout do sistema de arquivos

Os cgroups sÃ£o expostos atravÃ©s do sistema de arquivos virtual em/sys/fs/cgroup.

Dependendo da versÃ£o:

-   **CGROUPS V1**: Hierarquias separadas para cada controlador (por exemplo, memÃ³ria, CPU, etc.)
-   **CGROUPS V2**: Hierarquia unificada sob um Ãºnico ponto de montagem

Montado em:

```sh
/sys/fs/cgroup/
```

Hierarquia tÃ­pica de CGROUPS v1:

```sh
/sys/fs/cgroup/
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ mygroup/
â”‚   â”‚   â”œâ”€â”€ tasks
â”‚   â”‚   â”œâ”€â”€ memory.limit_in_bytes
â”œâ”€â”€ cpu/
â”‚   â””â”€â”€ mygroup/
â””â”€â”€ ...
```

No CGROUPS V2, todos os recursos sÃ£o gerenciados sob uma hierarquia unificada:

```sh
/sys/fs/cgroup/
â”œâ”€â”€ cgroup.procs
â”œâ”€â”€ cgroup.controllers
â”œâ”€â”€ memory.max
â”œâ”€â”€ cpu.max
â””â”€â”€ ...
```

##### ğŸ§ª Uso comum (exemplos V1 e V2)

v1 - Crie e atribua limite de memÃ³ria:

```sh
# Mount memory controller (if needed)
mount -t cgroup -o memory none /sys/fs/cgroup/memory

# Create group
mkdir /sys/fs/cgroup/memory/mygroup

# Set memory limit (100 MB)
echo 104857600 | tee /sys/fs/cgroup/memory/mygroup/memory.limit_in_bytes

# Assign a process (e.g., current shell)
echo $$ | tee /sys/fs/cgroup/memory/mygroup/tasks
```

V2 - Hierarquia unificada:

```sh
# Create subgroup
mkdir /sys/fs/cgroup/mygroup

# Enable controllers
echo +memory +cpu > /sys/fs/cgroup/cgroup.subtree_control

# Move shell into group
echo $$ > /sys/fs/cgroup/mygroup/cgroup.procs

# Set limits
echo 104857600 > /sys/fs/cgroup/mygroup/memory.max
echo "50000 100000" > /sys/fs/cgroup/mygroup/cpu.max  # 50ms quota per 100ms period
```

ğŸ§­ Process e inspeÃ§Ã£o de grupo

| Comando                 | DescriÃ§Ã£o                                 |
| ----------------------- | ----------------------------------------- |
| `cat /proc/self/cgroup` | Mostra a associaÃ§Ã£o atual do CGROUP       |
| `cat /proc/PID/cgroup`  | cgroup de outro processo                  |
| `cat /proc/PID/status`  | InformaÃ§Ãµes de memÃ³ria e cgroup           |
| `ps -o pid,cmd,cgroup`  | Mostre mapeamento de processo para cgrupo |

##### ğŸ“¦ Uso em contÃªineres

Motores de contÃªiner como Docker, Podman e Containerd Delegate Resource Control para CGroups (via Runc ou Crun), permitindo:

-   CPU por conteÃºdo e limites de memÃ³ria
-   Controle de grÃ£o fino sobre o BLKIO e dispositivos
-   Contabilidade de recursos em tempo real

Exemplo do Docker:

```sh
docker run --memory=256m --cpus=1 busybox
```

Nos bastidores, isso cria regras do CGROUP para limites de memÃ³ria e CPU para o processo de contÃªiner.

##### ğŸ§  Resumo dos conceitos

| Conceito          | ExplicaÃ§Ã£o                                                             |
| ----------------- | ---------------------------------------------------------------------- |
| **Controladores** | MÃ³dulos como`cpu`,`memory`,`blkio`, etc. Aplique limites e regras      |
| **Tarefas**       | PIDs (processos) atribuÃ­dos ao grupo de controle                       |
| **Hierarquia**    | CGROUPS estÃ£o estruturados em uma Ã¡rvore pai-filho                     |
| **DelegaÃ§Ã£o**     | Os serviÃ§os Systemd e do usuÃ¡rio podem gerenciar subÃ¡rvores de cgroups |

##### ğŸ§ª CGROUPS LAB

Use este script para laboratÃ³rio:[cgroups.sh](scripts/container/cgroups.sh)

[![asciicast](https://asciinema.org/a/WbudWJpHKPzBWMh8CGRxCIpZf.svg)](https://asciinema.org/a/WbudWJpHKPzBWMh8CGRxCIpZf)

* * *

#### ğŸ›¡ï¸ Recursos de compreensÃ£o

â“ Quais sÃ£o os recursos do Linux?

Tradicionalmente no Linux, o usuÃ¡rio raiz tem acesso irrestrito ao sistema. Os recursos do Linux foram introduzidos para dividir esses privilÃ©gios todo-poderosos em permissÃµes menores e discretas, permitindo que os processos realizem operaÃ§Ãµes privilegiadas especÃ­ficas sem exigir acesso total Ã  raiz.

Isso aprimora a seguranÃ§a do sistema, aplicando o princÃ­pio do menor privilÃ©gio.

| ğŸ” Capacidade          | ğŸ“‹ DescriÃ§Ã£o                                                        |
| ---------------------- | ------------------------------------------------------------------- |
| `CAP_CHOWN`            | Alterar o proprietÃ¡rio do arquivo, independentemente das permissÃµes |
| `CAP_NET_BIND_SERVICE` | Ligue para as portas abaixo de 1024 (por exemplo, 80, 443)          |
| `CAP_SYS_TIME`         | Defina o relÃ³gio do sistema                                         |
| `CAP_SYS_ADMIN`        | âš ï¸ Muito poderoso - inclui Mount, BPF e muito mais                  |
| `CAP_NET_RAW`          | Use soquetes crus (por exemplo, ping, traceroute)                   |
| `CAP_SYS_PTRACE`       | Rastrear outros processos (depuraÃ§Ã£o)                               |
| `CAP_KILL`             | Envie sinais para qualquer processo                                 |
| `CAP_DAC_OVERRIDE`     | Modificar arquivos e diretÃ³rios sem permissÃ£o                       |
| `CAP_SETUID`           | Alterar ID de usuÃ¡rio (UID) do processo                             |
| `CAP_NET_ADMIN`        | Gerenciar interfaces de rede, roteamento, etc.                      |

ğŸ” Alguns tipos de recursos do Linux

| Tipo de capacidade       | DescriÃ§Ã£o                                                                      |
| ------------------------ | ------------------------------------------------------------------------------ |
| **CapInh (Inherited)**   | Recursos herdados do processo pai.                                             |
| **CAPPRM (permitido)**   | Recursos que o processo pode ter.                                              |
| **Capeff (eficaz)**      | Recursos que o processo estÃ¡ usando atualmente.                                |
| **Capbnd (delimitador)** | Restringe o conjunto mÃ¡ximo de recursos eficazes que um processo pode obter.   |
| **Capamb (ambiente)**    | Permite que um processo defina explicitamente seus prÃ³prios recursos eficazes. |

ğŸ“¦ Capacidades em recipientes e vagens
Os contÃªineres normalmente nÃ£o sÃ£o executados como raiz completa, mas recebem um conjunto limitado de recursos por padrÃ£o, dependendo do tempo de execuÃ§Ã£o.

Os recursos podem ser adicionados ou descartados em Kubernetes usando o SecurityContext.

ğŸ“„ Kubernetes Exemplo:

```yaml
securityContext:
  capabilities:
    drop: ["ALL"]
    add: ["NET_BIND_SERVICE"]
```

ğŸ” Isso garante que o contÃªiner inicie com privilÃ©gios zero e receba apenas o que Ã© necessÃ¡rio.

##### ğŸ§ª Recursos de laboratÃ³rio

Use este script para laboratÃ³rio:[capabilities.sh](scripts/container/capabilities.sh)

[![asciicast](https://asciinema.org/a/kCiUGvY0YGA5Mdzbj1NSdfLAx.svg)](https://asciinema.org/a/kCiUGvY0YGA5Mdzbj1NSdfLAx)

#### ğŸ›¡ï¸ Seccomp (modo de computaÃ§Ã£o segura)

**O que Ã©?**

-   Um recurso do kernel do Linux para restringir quais syscalls (sistema chama) um processo pode usar.
-   Comumente usado em recipientes (como o docker), navegadores, caixas de areia, etc.

**Como funciona?**

-   Um processo permite um perfil/filtro Seccomp.
-   O kernel bloqueia, registra ou mata o processo se tentar os syscalls proibidos.
-   Os filtros sÃ£o escritos no formato BPF (Berkeley Packet Filter).

**Comandos rÃ¡pidos**

```sh
# Check support
docker info | grep Seccomp

# Disable for a container:
docker run --security-opt seccomp=unconfined ...

# Inspect running process:
grep Seccomp /proc/$$/status
```

**Ferramentas**

```sh
# for analyzing
seccomp-tools 

# Profiles
/etc/docker/seccomp.json
```

#### ğŸ¦ºAPARMOR

**O que Ã©?**

-   Um sistema de controle de acesso obrigatÃ³rio (MAC) para restringir o que programas especÃ­ficos podem acessar.
-   Os perfis sÃ£o baseados em texto, orientados para o caminho, fÃ¡ceis de ler e editar.

**Como funciona?**

-   Cada binÃ¡rio pode ter um perfil que define seus arquivos, rede e recursos permitidos - mesmo como root!
-   FÃ¡cil de alternar entre reclamar, aplicar e desativar os modos desativados.

**Comandos rÃ¡pidos:**

```sh
#Status
aa-status

# Put a program in enforce mode
sudo aa-enforce /etc/apparmor.d/usr.bin.foo

# Profiles
location: /etc/apparmor.d/
```

**Ferramentas:**

AA-GENPROF, AA-LOGPROF para gerar/atualizar perfis

Logs

```sh
/var/log/syslog (search for apparmor)
```

#### ğŸ”’SeLinux (Linux aprimorado de seguranÃ§a)

**O que Ã©?**

-   Um sistema MAC muito poderoso para controlar o acesso a tudo: arquivos, processos, usuÃ¡rios, portas, redes e muito mais.
-   Usa rÃ³tulos (contextos) e polÃ­ticas detalhadas.

**Como funciona?**

-   Tudo (processo, arquivo, porta, etc.) recebe um contexto de seguranÃ§a.
-   O kernel verifica todas as aÃ§Ãµes contra regras polÃ­ticas.

**Comandos rÃ¡pidos:**

```sh
#Status
sestatus

#Set to enforcing/permissive:
setenforce 1  # Enforcing
setenforce 0  # Permissive

#List security contexts:
ls -Z  # Files
ps -eZ # Processes
```

**Ferramentas:**

-   Audit2allow, Semanage, CHCON (para gerenciar polÃ­ticas/etiquetas)
-   Logs: /var/log/audit/audit.log
-   PolÃ­ticas:/etc/Selinux/

#### ğŸ“‹ Tabela de resumo para sistemas de seguranÃ§a comuns

| Sistema | Foco                    | Complexidade | LocalizaÃ§Ã£o da polÃ­tica                | Uso tÃ­pico              |
| ------- | ----------------------- | ------------ | -------------------------------------- | ----------------------- |
| Seccomp | Syscalls do kernel      | MÃ©dio        | Por processo (via cÃ³digo/configuraÃ§Ã£o) | Docker, caixas de areia |
| APARMOR | Acesso por programa     | FÃ¡cil        | /etc/apparmor.d/                       | Ubuntu, Snap, Suse      |
| Selinux | Mac do sistema completo | AvanÃ§ado     | /etc/selinux/ + rÃ³tulos                | Rhel, Fedora, Centos    |

#### ğŸ—‚ï¸ Isolamento de contÃªineres Linux e comparaÃ§Ã£o de seguranÃ§a

| Tecnologia        | PropÃ³sito / o que faz                                                                                                   | Principais diferenÃ§as                                                                                         | Exemplo em contÃªineres                                                                              |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| **Chroot ğŸ **     | Altera o diretÃ³rio raiz aparente para um processo. Isolate o sistema de arquivos.                                       | Isolamento simples do sistema de arquivos; faz**nÃ£o**restringir recursos, privilÃ©gios ou chamadas do sistema. | Docker usa`chroot`Internamente para criar imagens mÃ­nimas, mas nÃ£o para um forte isolamento.        |
| **CGROUPS ğŸ“Š**    | Controla e limita o uso de recursos (CPU, memÃ³ria, E/S de disco, etc.) por grupo de processos.                          | Recurso do kernel; Controle de recursos de grÃ£o fino, nÃ£o isolamento.                                         | Docker e Kubernetes usam cgroups para limitar a CPU/MEM por contÃªiner/pod.                          |
| **Namespaces ğŸŒ** | Isolar recursos do sistema: PID, MOUNT, UTS, Rede, UsuÃ¡rio, IPC, Time.                                                  | Recurso do kernel; fornece diferentes tipos de isolamento.                                                    | Cada contÃªiner Ã© executado em seu prÃ³prio conjunto de espaÃ§os para nome (PID, rede, montagem etc.). |
| **Recursos ğŸ›¡ï¸**  | Dividir privilÃ©gios de raiz em unidades de grÃ£o fino (por exemplo, rede_admin, sys_admin).                              | Mais granular do que tudo o ou nada raiz/nÃ£o raiz; pode abandonar ou conceder privilÃ©gios especÃ­ficos.        | Os contÃªineres do Docker geralmente sÃ£o executados com recursos reduzidos (solteiros perigosos).    |
| **Seccomp ğŸ§±**    | Filtrar/restringir quais syscalls um processo pode fazer (lista de permissÃµes/lista negra).                             | Muito focado: blocos de syscalls do kernel; nÃ£o pode bloquear todas as aÃ§Ãµes.                                 | O perfil padrÃ£o do Docker bloqueia syscalls perigosos (por exemplo,,`ptrace`,`mount`).              |
| **APARMOR ğŸ§**    | Estrutura de controle de acesso obrigatÃ³rio (MAC): restringe o acesso de arquivo/rede dos programas por meio de perfis. | Baseado em perfil, mais fÃ¡cil de gerenciar do que o Selinux; menos granulaÃ§Ã£o fina em alguns casos.           | Os contÃªineres baseados em Ubuntu geralmente usam o Apmor para perfis de processo de contÃªiner.     |
| **Selinux ğŸ”’**    | Estrutura MAC mais complexa, baseada em etiquetas, muito fina. Pode limitar usuÃ¡rios, processos e arquivos.             | Mais poderoso e complexo que o Aparmor; ForÃ§ado em Fedora/Rhel/Centos.                                        | No OpenShift/Kubernetes com RHEL, os rÃ³tulos do Selinux sÃ£o usados para manter as vagens separadas. |

Resumo

-   CHROOT: Isolamento bÃ¡sico, sem garantias de recurso/seguranÃ§a.
-   CGROUPS: Controle de recursos, nÃ£o isolamento.
-   Namespaces: isolar "vistas" dos recursos do kernel.
-   Recursos: privilÃ©gios de processo de ajuste fino.
-   Seccomp: Restre a superfÃ­cie da chamada do sistema.
-   APARMOR/SELinux: limite o que os processos podem tocar, mesmo como root (Mac).

#### ğŸ§© OCI, Runc, Containerd, CRI, CRI-O-O que eles sÃ£o no ecossistema de contÃªineres

##### VisÃ£o geral e papÃ©is

-   **OCI (iniciativa de contÃªiner aberto) ğŸ›ï¸**

    Uma fundaÃ§Ã£o criando padrÃµes abertos para**imagens de contÃªiner**e**Runtimes**.

    _Define como as imagens sÃ£o formatadas, armazenadas e como os contÃªineres sÃ£o iniciados/parados (especificaÃ§Ãµes de tempo de execuÃ§Ã£o)._
-   **âš™ï¸ Runc**

    Uma ferramenta CLI universal, de baixo nÃ­vel e leve que pode executar contÃªineres de acordo com a especificaÃ§Ã£o de tempo de execuÃ§Ã£o da OCI.

    _"O motor" que transforma uma configuraÃ§Ã£o de imagem + em um contÃªiner Linux em execuÃ§Ã£o real._
-   **Containerd ğŸ‹ï¸**

    Um daemon de tempo de execuÃ§Ã£o do contÃªiner principal para gerenciar o ciclo de vida completo do contÃªiner:**puxando imagens, gerenciando armazenamento, executando recipientes**(chama Runc), plugins de rede, etc.

    _Usado por Docker, Kubernetes, Nerdctl e outras ferramentas como seu principal back -end de tempo de execuÃ§Ã£o do contÃªiner._
-   **CRI (interface de tempo de execuÃ§Ã£o do contÃªiner) ğŸ”Œ**

    Uma API GRPC especÃ­fica para Kubernetes para conectar o Kubernetes com os tempos de execuÃ§Ã£o do contÃªiner.

    _NÃ£o usado fora de Kubernetes, mas permite que os K8s conversem com contÃªineres, Cri-O, etc._
-   **CRI-O ğŸ¥¤**

    Um tempo de execuÃ§Ã£o leve e focado em Kubernetes que**apenas**Executa contÃªineres OCI, usando o Runc sob o capÃ´.

    _Principalmente usado em Kubernetes, mas demonstra como criar um tempo de execuÃ§Ã£o mÃ­nimo de contÃªiner focado nos padrÃµes abertos._

##### Tables Tabelas de comparaÃ§Ã£o: OCI, Runc, Containerd, CRI, CRI-O

| Componente    | Emoji | O que Ã©?                                     | Quem o usa?                             | Exemplo de uso                                                               |
| ------------- | ----- | -------------------------------------------- | --------------------------------------- | ---------------------------------------------------------------------------- |
| **OCI**       | ğŸ›ï¸   | PadrÃµes/especificaÃ§Ãµes                       | Docker, Podman, CRI-O, containerd, runc | Garante que imagens/recipientes sejam compatÃ­veis entre as ferramentas       |
| **Runc**      | âš™ï¸    | Time de execuÃ§Ã£o de contÃªineres (CLI)        | containerd, CRI-O, Docker, Podman       | Executando diretamente um recipiente de um pacote (por exemplo`runc run`)    |
| **contÃªiner** | ğŸ‹ï¸   | Daemon de tempo de execuÃ§Ã£o do contÃªiner     | Docker, Kubernetes, Nerdctl             | Handles pulling images, managing storage/network, starts containers via runc |
| **Cri**       | ğŸ”Œ    | Interface de tempo de execuÃ§Ã£o do K8S (API)  | Somente Kubernetes                      | Deixe Kubelet falar com o contÃªiner/Cri-O                                    |
| **CRI-O**     | ğŸ¥¤    | Tempo de execuÃ§Ã£o do contÃªiner leve para K8s | Kubernetes, OpenShift                   | Usado como motor de contÃªiner K8S                                            |

* * *

##### Exemplos Exemplos prÃ¡ticos (mundo geral de contÃªineres)

-   **Imagens de construÃ§Ã£o:**

    Qualquer ferramenta (Docker, Podman, Builtah) pode produzir imagens seguindo o**OCI Image Spec**EntÃ£o eles sÃ£o compatÃ­veis em todos os lugares.
-   **Recipientes em execuÃ§Ã£o:**

    Podman e Docker finalmente usam**Runc**(via contÃªiner ou diretamente) para criar contÃªineres.
-   **Gerenciando muitos contÃªineres:**

    **contÃªiner**pode ser usado por conta prÃ³pria (via`ctr`ou`nerdctl`) ou como um back -end para Docker e Kubernetes.
-   **Tempos de execuÃ§Ã£o plug-and-play:**

    Obrigado a**OCI**, vocÃª pode trocar o Runc por outro tempo de execuÃ§Ã£o compatÃ­vel com OCI (como os contÃªineres KATA para VMS, Gvisor for Sandboxing) sem alterar a maneira como vocÃª cria ou gerencia imagens.

* * *

##### ğŸš¢ Pilha tÃ­pica

```plaintext
[User CLI / Orchestration]
           |
   [containerd / CRI-O]
           |
        [runc]
           |
[Linux Kernel: namespaces, cgroups, etc]
```

-   **Docker**: UsuÃ¡rio 151 â†’ ContÃªiner â†’ Runc
-   **Subman**: UsuÃ¡rio 151 â†’ Runc
-   **Kubernetes**: Kubelet (CRI) â†’ ContÃªiner ou Cri-O â†’ Runc

* * *

##### ğŸ§  Resumo

-   **OCI**= Linguagem comum para imagens/tempo de execuÃ§Ã£o (padrÃµes/especificaÃ§Ãµes)
-   **Runc**= Ferramenta real que cria e gerencia processos de contÃªiner
-   **contÃªiner**= Daemon completo que gerencia imagens, recipientes, ciclo de vida
-   **Cri**= Somente para Kubernetes, para tornar os tempos de execuÃ§Ã£o que
-   **CRI-O**= Tempo de execuÃ§Ã£o leve focado em Kubernetes, construÃ­dos com os padrÃµes da OCI e Runc

##### ğŸ§© Diagrama: ecossistema de contÃªineres

```mermaid
graph TD
    subgraph OCI_Standards
        OCI1["OCI Image Spec"]
        OCI2["OCI Runtime Spec"]
    end

    subgraph Orchestration_CLI
        Docker["Docker CLI"]
        Podman["Podman CLI"]
        Kubelet["Kubelet"]
        Nerdctl["nerdctl CLI"]
    end

    subgraph Container_Runtimes
        containerd["containerd"]
        crio["CRI-O"]
    end

    runc["runc"]

    Kernel["Linux Kernel(namespaces, cgroups, seccomp, etc)"]

    %% Connections
    Docker --> containerd
    Podman --> runc
    Nerdctl --> containerd
    Kubelet --> CRI[CRI API]
    CRI --> containerd
    CRI --> crio
    containerd --> runc
    crio --> runc
    runc --> Kernel

    OCI1 -.-> containerd
    OCI1 -.-> crio
    OCI2 -.-> runc
```

##### ğŸ§ª Lab Runc

Para o Runc Lab, vocÃª pode usar este script:[runc.sh](scripts/container/runc.sh)

[![asciicast](https://asciinema.org/a/UDVnhKSxPFRXDcwg0HYFkZdlX.svg)](https://asciinema.org/a/UDVnhKSxPFRXDcwg0HYFkZdlX)

##### ğŸ§ª Container de laboratÃ³rio

Para o contÃªiner, vocÃª pode usar este script:[containerd.sh](scripts/container/container.sh)

[![asciicast](https://asciinema.org/a/fCJsiwcL2ePneQX1aafITtoGM.svg)](https://asciinema.org/a/fCJsiwcL2ePneQX1aafITtoGM)

* * *

#### ğŸš€ Podman, Buildah, Skopeo, OpenVZ, Crun e Kata Containers - Fast Track

* * *

##### ğŸ³**Subman**

-   **O que Ã©?**Um gerente de contÃªineres compatÃ­vel com a CLI do Docker, mas**Sem daemon**e pode correr**sem raiz**.
-   **Usar:**Crie, execute, pare e inspecione recipientes e vagens.
-   **Destaques:**Nenhum Daemon Central, mais seguro para o MultiUser, integra-se ao Systemd.
-   [Mais informaÃ§Ãµes](<>)

* * *

##### ğŸ“¦**Buildah**

-   **O que Ã©?**Ferramenta para**Construir e manipular imagens de contÃªineres**(OCI/Docker) sem daemon.
-   **Usar:**Construindo imagens em pipelines CI/CD ou scripts.
-   **Destaques:**Suporte leve e sem raiz, usado por podman sob o capÃ´.
-   [Mais informaÃ§Ãµes](https://www.redhat.com/en/topics/containers/what-is-buildah)

* * *

##### ğŸ”­**Escopo**

-   **O que Ã©?**Utilidade para**Inspecionar, copiar e mover imagens de contÃªiner**entre registros**sem puxar ou correr**eles.
-   **Usar:**Mova imagens, verifique as assinaturas e os metadados.
-   **Destaques:**Sem daemon, ideal para automaÃ§Ã£o e seguranÃ§a.
-   [Mais informaÃ§Ãµes](<>)

* * *

##### ğŸ¢**OpenVZ**

-   **O que Ã©?****VirtualizaÃ§Ã£o baseada em contÃªiner**SoluÃ§Ã£o para Linux (ferramentas modernas de contÃªiner modernas).
-   **Usar:**VPs leves (servidores privados virtuais) compartilhando o mesmo kernel.
-   **Destaques:**Muito eficiente, mas menos isolado que a VM (aÃ§Ãµes do kernel).
-   [Mais informaÃ§Ãµes](https://en.wikipedia.org/wiki/OpenVZ)

* * *

##### âš¡**Crun**

-   **O que Ã©?**Tempo de execuÃ§Ã£o Ultra-Fast e Minimal OCI para contÃªineres, escrito em C (nÃ£o vÃ¡).
-   **Usar:**Executa recipientes com sobrecarga mÃ­nima.
-   **Destaques:**Mais rÃ¡pido e mais leve que o Runc, padrÃ£o para o Podman em alguns sistemas.
-   [Mais informaÃ§Ãµes](https://www.redhat.com/sysadmin/introduction-crun)

* * *

##### ğŸ›¡ï¸**ContÃªiner de palavras**

-   **O que Ã©?**Projeto de cÃ³digo aberto Combinando recipientes e VMs: Cada contÃªiner Ã© executado em um micro-VM leve.
-   **Usar:**Isolamento forte para cargas de trabalho sensÃ­veis ou ambientes multi-inquilinos.
-   **Destaques:**SeguranÃ§a de grau de VM, desempenho prÃ³ximo do contorno.
-   [Mais informaÃ§Ãµes](https://katacontainers.io/)

* * *

##### ğŸ“Š**Tabela de comparaÃ§Ã£o**

| Projeto                   | Categoria               | Isolamento            | Daemon? | Uso principal                         | Sem raiz | Notas                                   |
| ------------------------- | ----------------------- | --------------------- | ------- | ------------------------------------- | -------- | --------------------------------------- |
| **Subman**                | OrquestraÃ§Ã£o            | Recipiente            | No      | Gerenciar contÃªineres                 | Sim      | CLI do tipo Docker                      |
| **Buildah**               | Construir               | N / D                 | No      | Construir imagens                     | Sim      | Para CI/CD, sem execuÃ§Ã£o de contÃªineres |
| **Escopo**                | TransferÃªncia de imagem | N / D                 | No      | Mova/verifique as imagens             | Sim      | Nenhuma execuÃ§Ã£o de contÃªiner           |
| **OpenVZ**                | VirtualizaÃ§Ã£o           | ContÃªiner/vps         | Sim     | VPS leves                             | No       | Kernel compartilhou, Tech Legacy        |
| **Crun**                  | OCI Runtime             | Recipiente            | No      | Tempo de execuÃ§Ã£o rÃ¡pido do contÃªiner | Sim      | Mais rÃ¡pido que o Runc                  |
| **ContÃªiner de palavras** | Runtime/VM              | Microvm por contÃªiner | No      | Isolamento forte                      | Sim      | SeguranÃ§a no nÃ­vel da VM                |

* * *

##### â˜‘ï¸**RecapitulaÃ§Ã£o rÃ¡pida**

-   **Podman:**Alternativa moderna e sem daemon sem daemon.
-   **Buildah:**Crie imagens, nÃ£o executa recipientes.
-   **Skeape:**Move/inspeciona imagens, nunca as executa.
-   **OpenVZ:**VPs baseados em contÃªineres legados.
-   **Cruel:**Tempo de execuÃ§Ã£o super rÃ¡pido e leve da OCI.
-   **Dizer:**Recipientes com isolamento no nÃ­vel da VM.

#### ğŸ› ï¸ 352.1 comandos importantes

##### ğŸ”— NÃ£o se bem

```sh
# create a new namespaces and run a command in it
unshare --mount --uts --ipc --user --pid --net  --map-root-user --mount-proc --fork chroot ~vagrant/debian bash
# mount /proc for test
#mount -t proc proc /proc
#ps -aux
#ip addr show
#umount /proc
```

##### ğŸ” lsns

```sh
# show all namespaces
lsns

# show only pid namespace
lsns -s <pid>
lsns -p 3669

ls -l /proc/<pid>/ns
ls -l /proc/3669/ns

ps -o pid,pidns,netns,ipcns,utsns,userns,args -p <PID>
ps -o pid,pidns,netns,ipcns,utsns,userns,args -p 3669
```

##### ğŸšª NSENTER

```sh
# execute a command in namespace
sudo nsenter -t <PID> -n  ip link show
sudo nsenter -t 3669 -n ip link show
```

##### ğŸŒ 252.1 IP

```sh
# create a new network namespace
sudo ip netns add lxc1

# list network list
ip netns list

# exec command in network namespace
sudo ip netns exec lxc1 ip addr show
```

##### ğŸ“Š Stat

```sh
# get cgroup version
stat -fc %T /sys/fs/cgroup
```

##### ğŸ› ï¸ Systemctl e Systemd

```sh
# get cgroups of system
systemctl status
systemd-cgls
```

##### ğŸ—ï¸ CGcreate

```sh
cgcreate -g memory,cpu:lsf
```

##### ğŸ·ï¸ CGclassify

```sh
cgclassify -g memory,cpu:lsf <PID>
```

##### ğŸ›¡ï¸ PSCAP - Recursos de processo de lista

```sh
# List capabilities of all process
pscap
```

##### ğŸ›¡ï¸ getcap/usr/bin/tcpdump

```sh
getcap /usr/bin/tcpdump
```

##### ğŸ›¡ï¸ setcap cap_net_raw = ep/usr/bin/tcpdump

```sh
# add capabilities to tcpdump
sudo setcap cap_net_raw=ep /usr/bin/tcpdump

# remove capabilities from tcpdump
sudo setcap -r /usr/bin/tcpdump
sudo setcap '' /usr/bin/tcpdump
```

##### Recursos de verificaÃ§Ã£o dos recursos de verificaÃ§Ã£o por processo

```sh
grep Cap /proc/<PID>/status
```

##### ğŸ›¡ï¸ Capsh - invÃ³lucro de shell de capacidade

```sh
# use grep Cap /proc/<PID>/statusfor get hexadecimal value(Example CApEff=0000000000002000)
capsh --decode=0000000000002000
```

##### ğŸ¦º Apparmor - Aprimoramento do kernel para limitar os programas a um conjunto limitado de recursos

```sh
# check AppArmor status
sudo aa-status

#  unload all AppArmor profiles
aa-teardown

# loads AppArmor profiles into the kernel
aaparmor_parser
```

##### ğŸ”’ Selinux - Linux aprimorado em seguranÃ§a

```sh
# check SELinux status
sudo sestatus

# check SELinux mode
sudo getenforce 

# set SELinux to enforcing mode
sudo setenforce 1
```

##### âš™ï¸ Runc

```sh
#create a spec file for runc
runc spec

# run a container using runc
sudo runc run mycontainer
```

* * *

<p align="right">(<a href="#topic-352.1">back to sub topic 352.1</a>)</p>
<p align="right">(<a href="#topic-352">back to topic 352</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-352.2"></a>

### ğŸ“¦ 352.2 LXC

**Peso:**6

**DescriÃ§Ã£o:**Os candidatos devem poder usar contÃªineres do sistema usando LXC e LXD. A versÃ£o do LXC coberta Ã© 3,0 ou superior.

**Principais Ã¡reas de conhecimento:**

-   Entenda a arquitetura do LXC e LXD
-   Gerencie recipientes LXC com base nas imagens existentes usando LXD, incluindo redes e armazenamento
-   Configurar propriedades de contÃªiner LXC
-   Limite o uso de recursos de contÃªiner LXC
-   Use perfis LXD
-   Entenda imagens LXC
-   ConsciÃªncia das ferramentas tradicionais do LXC

#### ğŸ“‹ 352.2 Objetos citados

```sh
lxd
lxc (including relevant subcommands)
/etc/lxc/
/etc/default/lxc
/var/log/lxc/
/usr/share/lxc/templates
```

#### ğŸ§© LXC & LXD - O conjunto de contÃªineres do sistema Linux

* * *

##### ğŸ“¦ LXC (contÃªineres Linux)

-   **O que Ã©?**

    O_essencial_UsuÃ¡riosPace ToolSet para gerenciar contÃªineres de aplicativos e sistemas no Linux. Pense no LXC como**"Chroot em esterÃ³ides"**- Ele fornece isolamento leve do processo usando recursos do kernel (espaÃ§os para nome, cgroups, Aparmor, Seccomp, etc.).
-   **Usar:**

    -   Execute distribuiÃ§Ãµes completas do Linux como contÃªineres (nÃ£o apenas aplicativos Ãºnicos).
    -   Ãštil para testes, aplicativos herdados ou servidores simulados.
-   **Destaques:**

    -   Focado na CLI:`lxc-create`,`lxc-start`,`lxc-attach`, etc.
    -   Controle de grÃ£o fino sobre os recursos de contÃªiner.
    -   Nenhum Daemon-executa processos por conteÃºdo.
-   **Melhor para:**

    Especialistas do Linux que desejam controle total e sensaÃ§Ã£o de "metal nu" para recipientes.

##### ğŸ§ª LAB LXC

Para o laboratÃ³rio LXC, vocÃª pode usar este script:[lxc.sh](scripts/container/lxc.sh)

[![asciicast](https://asciinema.org/a/CpjDAXRnaKH5kExg9eWSBJGHI.svg)](https://asciinema.org/a/CpjDAXRnaKH5kExg9eWSBJGHI)

* * *

##### ğŸŒ LXD

-   **O que Ã©?**

    **Lxd**Ã© a_prÃ³xima geraÃ§Ã£o_Container e VM Manager,**ConstruÃ­do em cima do LXC**. Ele oferece uma experiÃªncia poderosa, mas fÃ¡cil de usar, para gerenciar contÃªineres e mÃ¡quinas virtuais via API REST, CLI ou atÃ© uma interface da web.
-   **Usar:**

    -   Gerenciar contÃªineres do sistema e mÃ¡quinas virtuais em escala.
    -   ContÃªiner em rede â€œcomo serviÃ§oâ€ com orquestraÃ§Ã£o fÃ¡cil.
-   **Destaques:**

    -   **Rest API**: Gerencie contÃªineres/VMs sobre a rede.
    -   **Imagens:**ImplantaÃ§Ã£o instantÃ¢nea de muitas distros do Linux.
    -   **InstantÃ¢neos, piscinas de armazenamento, clustering, migraÃ§Ã£o ao vivo.**
    -   Suporta a execuÃ§Ã£o de contÃªineres sem privilÃ©gios por padrÃ£o.
    -   CLI:`lxc launch`,`lxc exec`,`lxc snapshot`, etc._(Sim, o mesmo prefixo que LXC, mas back -end diferente!)_
-   **Melhor para:**

    DevOps, sysadmins, configuraÃ§Ãµes nativas da nuvem, ambientes de laboratÃ³rio.

##### ğŸ“**Armazenamento LXD: Tabela de recursos (por back -end)**

| Recurso                  | VocÃª | ZFS               | Brfs              | LVM/LVMTHIN       | ceph/cephfs          |
| ------------------------ | ---- | ----------------- | ----------------- | ----------------- | -------------------- |
| **InstantÃ¢neos**         | âŒ    | âœ…                 | âœ…                 | âœ…                 | âœ…                    |
| **Provisionamento fino** | âŒ    | âœ…                 | âœ…                 | âœ… (LvMthin)       | âœ…                    |
| **Redimensionamento**    | âŒ    | âœ…                 | âœ…                 | âœ…                 | âœ…                    |
| **Cotas**                | âŒ    | âœ…                 | âœ…                 | âœ… (LvMthin)       | âœ…                    |
| **MigraÃ§Ã£o ao vivo**     | âŒ    | âœ…                 | âœ…                 | âœ…                 | âœ…                    |
| **DesduplicaÃ§Ã£o**        | âŒ    | âœ…                 | âŒ                 | âŒ                 | âœ… (ceph)             |
| **CompressÃ£o**           | âŒ    | âœ…                 | âœ…                 | âŒ                 | âœ… (ceph)             |
| **Criptografia**         | âŒ    | âœ…                 | âŒ                 | âœ… (luxo)          | âœ…                    |
| **Cluster/remoto**       | âŒ    | âŒ                 | âŒ                 | âŒ                 | âœ…                    |
| **Melhor caso de uso**   | Dev  | LaboratÃ³rios/prod | LaboratÃ³rios/prod | LaboratÃ³rios/prod | Clusters, Enterprise |

##### ğŸ”**Resumo do armazenamento rÃ¡pido LXD**

-   **Pools de armazenamento:**Abstrair o back -end - piscinas multiplicadas, diferentes motoristas por piscina.
-   **Drivers disponÃ­veis:**Dir, zfs, btrfs, lvm, lvmthin, ceph, cephfs (mais via plugins).
-   **Volumes personalizados:**Criar, montar, desmontar para contÃªineres/VMs.
-   **InstantÃ¢neos e clones:**Nativo, rÃ¡pido, suporta migraÃ§Ã£o de backup/restauraÃ§Ã£o, cÃ³pia-na-gravaÃ§Ã£o.
-   **Cotas e redimensionar:**Easy Live Management para piscinas, contÃªineres ou volumes.
-   **MigraÃ§Ã£o ao vivo:**Mova contÃªineres/VMs entre hosts sem tempo de inatividade.
-   **SeguranÃ§a:**Criptografia embutida (ZFS, LVM, CEPH), ACLs, backup/restauraÃ§Ã£o, etc.
-   **Enterprise-Proy:**InstalaÃ§Ãµes em cluster e alta disponibilidade em cluster.

* * *

##### ğŸ“Š LXC vs Tabela de comparaÃ§Ã£o LXD

| Recurso           | ğŸ·ï¸ LXC                                       | ğŸŒ LXD                                                       |
| ----------------- | --------------------------------------------- | ------------------------------------------------------------ |
| **Tipo**          | Gerente de contÃªiner de espaÃ§o de baixo nÃ­vel | Gerente de alto nÃ­vel (contÃªineres + VMs)                    |
| **Interface**     | Apenas CLI                                    | API REST, CLI, interface da usuÃ¡rio da web                   |
| **Daemon?**       | No (runs as processes)                        | Sim (daemon/serviÃ§o central)                                 |
| **OrquestraÃ§Ã£o**  | Manual, Scriptable                            | Clustering embutido e API                                    |
| **Imagens**       | Baseado em modelo                             | RepositÃ³rio de imagem completa, muitos sistemas operacionais |
| **InstantÃ¢neos**  | Manual                                        | Nativo, integrado                                            |
| **Suporte da VM** | No                                            | Sim (qemu/kvm)                                               |
| **Uso de uso**    | Controle de grÃ£o fino, â€œBare-metalâ€           | Multi-host escalÃ¡vel, fÃ¡cil de usar                          |
| **SeguranÃ§a**     | Pode ser sem privilÃ©gios, mas DIY             | NÃ£o privilegiado, mais isolamento                            |
| **Melhor para**   | PrÃ³s Linux, script avanÃ§ado                   | DevOps, nuvem, equipes, autoatendimento                      |

* * *

##### â˜‘ï¸ RecapitulaÃ§Ã£o rÃ¡pida

-   **LXC**= Os blocos de construÃ§Ã£o de baixo nÃ­vel. Poder e flexibilidade para_Puristas de contÃªineres_.
-   **Lxd**= Moderno, orientado a API e plataforma escalÃ¡vel em cima do LXC para_fÃ¡cil_Gerenciamento de contÃªineres e VM (nÃ³ Ãºnico ou clusters).

##### ğŸ—ƒï¸ LXC vs LXD - Suporte de armazenamento (resumo)

| Recurso                             | **LXC**                                          | **Lxd**                                                                                         |
| ----------------------------------- | ------------------------------------------------ | ----------------------------------------------------------------------------------------------- |
| **Back -ends de armazenamento**     | Sistema de arquivos local (somente padrÃ£o)       | **VocÃª**(FileSystem),**ZFS**,**Brfs**,**lvm**,**Ceph**,**ceffs**,**LvMthin**                    |
| **Pools de armazenamento**          | âŒ (apenas caminhos locais, sem piscinas nativas) | âœ… VÃ¡rios pools de armazenamento, cada um com diferentes drivers                                 |
| **InstantÃ¢neos**                    | Manual/FS dependente                             | âœ… InstantÃ¢neos nativos, rÃ¡pidos, automÃ¡ticos, programados e consistentes                        |
| **Provisionamento fino**            | âŒ (nÃ£o suportado nativamente)                    | âœ… Suportado em ZFS, BTRFS, LVM Thin, CEPH                                                       |
| **Cotas**                           | âŒ                                                | âœ… Suporte por contÃªiner/volume (em ZFS, BTRFS, CEPH, LVMTHIN)                                   |
| **MigraÃ§Ã£o ao vivo**                | Limitado                                         | âœ… MigraÃ§Ã£o de armazenamento ao vivo entre hosts, copiar-se-write                                |
| **Criptografia**                    | âŒ                                                | âœ… (ZFS, LVM, CEPH)                                                                              |
| **Volumes personalizados**          | âŒ                                                | âœ… Criar, anexar/destacar os volumes de armazenamento personalizado para contÃªineres/VMs         |
| **Armazenamento remoto**            | âŒ                                                | âœ… Ceph, Cephfs, NFS, Suporte de SMB                                                             |
| **Recursos do sistema de arquivos** | Host dependente                                  | ZFS: Dedup, Compress, instantÃ¢neos, enviar/receber, cache, cotas. LVM: fino, instantÃ¢neos, etc. |
| **Redimensionamento**               | Manual (via host)                                | âœ… Volumes e piscinas podem ser redimensionados ao vivo                                          |
| **Drivers de armazenamento**        | Somente bÃ¡sico/local                             | Plugins extensÃ­veis, vÃ¡rios drivers prontos para empresas                                       |

##### ğŸ“Š Tabela de comparaÃ§Ã£o de armazenamento final

|                                | **LXC**         | **Lxd**                                                |
| ------------------------------ | --------------- | ------------------------------------------------------ |
| **Back -end de armazenamento** | Somente local   | Dir, zfs, btrfs, lvm, lvmthin, ceph, cephfs            |
| **Pools de armazenamento**     | âŒ               | âœ… MÃºltiplo, independente e quente                      |
| **InstantÃ¢neos**               | Limitado/manual | âœ… RÃ¡pido, automÃ¡tico, consistente                      |
| **Provisionamento fino**       | âŒ               | âœ… (ZFS, BTRFS, LVMTHAIN, CEPPH)                        |
| **Cotas**                      | âŒ               | âœ…                                                      |
| **Redimensionamento**          | Manual          | âœ…                                                      |
| **Armazenamento remoto**       | âŒ               | âœ… (Ceph, NFS, SMB)                                     |
| **Volumes personalizados**     | âŒ               | âœ…                                                      |
| **Cluster pronto**             | âŒ               | âœ…                                                      |
| **Empresa**                    | No              | Sim - ha, backup, migraÃ§Ã£o, seguranÃ§a, produÃ§Ã£o pronta |

##### ğŸŒ LXC vs LXD - Suporte de rede (resumo)

| Recurso                  | **LXC**                           | **Lxd**                                                                    |
| ------------------------ | --------------------------------- | -------------------------------------------------------------------------- |
| **Tipos de rede**        | Bridge, Veth, MacVlan, Phys, Vlan | ponte, ovn, macvlan, sriiov, fÃ­sico, vlan, fÃ£, tÃºneis                      |
| **Redes gerenciadas**    | âŒ manual (configuraÃ§Ã£o do host)   | âœ… Gerenciado nativamente via API/CLI, fÃ¡cil de criar e editar              |
| **API de rede**          | âŒ Comandos da CLI SOMENTE         | âœ… API REST, CLI, integraÃ§Ã£o com ferramentas externas                       |
| **Suporte da ponte**     | âœ… manual                          | âœ… AutomÃ¡tico e avanÃ§ado (L2, Open VSwitch, Bridge nativo)                  |
| **Nat & dhcp**           | âŒ manual (iptables/dnsmasq)       | âœ… NAT integrado, DHCP, DNS, por rede configurÃ¡vel                          |
| **Dns**                  | âŒ manual                          | âœ… DNS integrados, domÃ­nios personalizados, integraÃ§Ã£o resolvida do SystemD |
| **IPVSH**                | âœ… (manual, limitado)              | âœ… Suporte completo, Auto, DHCPV6, Nat6, roteamento                         |
| **VLAN**                 | âœ… (manual, anfitriÃ£o)             | âœ… VLANs nativas, configuraÃ§Ã£o fÃ¡cil                                        |
| **SR-IOV**               | âŒ                                 | âœ… Suporte nativo                                                           |
| **ACLs de rede**         | âŒ                                 | âœ… ACLs, atacantes, zonas, pares, regras de firewall                        |
| **Clustering**           | âŒ                                 | âœ… Redes replicadas e gerenciadas em clusters                               |
| **Anexar/destacar**      | Manual (anfitriÃ£o)                | âœ… CLI/API, HOTPLUG, fÃ¡cil para contÃªineres/VMs                             |
| **SeguranÃ§a**            | Manual (anfitriÃ£o)                | âœ… Isolamento, firewall, LCA, integraÃ§Ã£o de firewalld, regras por rede      |
| **Rotas personalizadas** | Manual                            | âœ… Suporte de rotas personalizadas, vÃ¡rios gateways                         |
| **Perfis de rede**       | âŒ                                 | âœ… Perfis de rede reutilizÃ¡veis                                             |
| **Monitoramento**        | Manual                            | âœ… Status, ipam, logs, informaÃ§Ãµes detalhadas via CLI/API                   |
| **Empresa**              | No                                | Sim-Multi-Tenant, ACL, Clustering, integraÃ§Ã£o em nuvem                     |

##### ğŸ“Š Tabela de comparaÃ§Ã£o de rede final

|                   | **LXC**           | **Lxd**                                               |
| ----------------- | ----------------- | ----------------------------------------------------- |
| **Tipos de rede** | Ponte, Veth, Vlan | ponte, ovn, macvlan, sriiov, fÃ­sico, vlan, fÃ£, tÃºneis |
| **Gerenciou**     | âŒ                 | âœ…                                                     |
| **Nat/DHCP/DNS**  | Manual            | âœ… Integrado                                           |
| **VLAN**          | Manual            | âœ…                                                     |
| **SR-IOV**        | âŒ                 | âœ…                                                     |
| **API**           | âŒ                 | âœ…                                                     |
| **Clustering**    | âŒ                 | âœ…                                                     |
| **SeguranÃ§a/ACL** | Manual            | âœ…                                                     |
| **Perfis**        | âŒ                 | âœ…                                                     |
| **Empresa**       | No                | Sim                                                   |

##### ğŸ§ª LAB LXD

Para LXD Lab, vocÃª pode usar este script:[lxd.sh](scripts/container/lxd.sh)

#### ğŸ› ï¸ 352.2 comandos importantes

##### ğŸ“¦ LXC

```sh

# lxc configuration
/etc/default/lxc
/etc/default/lxc-net
/etc/lxc/default.conf
/usr/share/lxc/

# lxc container configuration
/var/lib/lxc/

# check lxc version
lxc-create --version

# list containers
sudo lxc-ls --fancy
sudo lxc-ls -f

# create a priveleged container
sudo lxc-create -n busybox -t busybox

# create a priveleged container with template
sudo lxc-create -n debian01 -t download
sudo lxc-create --name server2 --template download -- --dist alpine --release 3.19 --arch amd64

# get container info
sudo lxc-info -n debian01

# get container PID
sudo lxc-info -n debian01 -pH

# get container config
sudo lxc-checkconfig -n debian01

# start container
sudo lxc-start -n debian01

# stop container
sudo lxc-stop -n debian01

# connect to container
sudo lxc-attach -n debian01

# excute a command in container
sudo lxc-attach -n debian01 --  echo "Hello from"
sudo lxc-attach -n debian01 -- bash -c ls

# delete container
sudo lxc-destroy -n debian01

# delete container and snapshot
sudo lxc-destroy -n -s debian01

# rootfs of a container
sudo ls -l /var/lib/lxc/server1/rootfs

# modify rootfs of a container
sudo touch  /var/lib/lxc/server1/rootfs/tmp/test_toofs_file
sudo lxc-attach server1
ls /tmp

# get lxc namespaces
sudo lsns -p <LXC_CONTAINER_PID>
sudo lsns -p $(sudo lxc-info server2 -pH)
sudo lsns -p $(sudo lxc-info -n server1 | awk '/PID:/ { print $2 }')

# unprivileged container namespaces
lsns -p $(lxc-info -n ubuntu | awk '/PID:/ { print $2 }')

# get container resource 
sudo lxc-top

# create a container snapshot
sudo lxc-stop -k -n debian01
sudo lxc-snapshot -n debian01

# list snapshots
sudo lxc-snapshot -n debian01 -L

# restore snapshot
sudo lxc-stop -n debian01
sudo lxc-snapshot -n debian01 -r snap0

# delete snapshot
sudo lxc-snapshot -n debian01 -d snap0

# create a new container with snapshot
sudo lxc-snapshot -n debian01 -r snap0 -N debian02

# create container checkpoint (privileged container)
sudo lxc-checkpoint -n debian01 -s -D /home/vagrant/.config/lxc/checkpoints/debian01-checkpoint01.file 

# define memory container limits with cgroups
sudo lxc-cgroup -n debian01 memory.max 262144000 #(250 MB Ã— 1.048.576 bytes = 262144000 bytes)

# define CPU cores of container  with cgroups
sudo lxc-cgroup -n debian01 cpuset.cpus 0-2

# get container cgroup limits
sudo cgget -g :lxc.payload.debian01 -a |grep memory.max
sudo cgget -g :lxc.payload.debian01 -a |grep cpuset

# set container cgroup vcpus range in file
sudo vim /var/lib/lxc/debian01/config
# add the following lines
lxc.cgroup2.cpuset.cpus = "5-6"

######## create unprivileged container #######

## create directory for unprivileged container
mkdir -p /home/vagrant/.config/lxc

## copy default config
cp /etc/lxc/default.conf /home/vagrant/.config/lxc/

## get subordinate user and group IDs
cat /etc/subuid

## configure subordinate user and group IDs
vim /home/vagrant/.config/lxc/default.conf

## add the following lines
lxc.idmap = u 0 100000 65536
lxc.idmap = g 0 100000 65536

## configure lxc-usernet
sudo vim /etc/lxc/lxc-usernet

## add the following line
vagrant veth lxcbr0 10

## create unprivileged container
lxc-create -n unprivileged -t download -- -d ubuntu -r jammy -a amd64

## set permissions for unprivileged container
sudo setfacl -m u:100000:--x /home/vagrant
sudo setfacl -m u:100000:--x /home/vagrant/.config
sudo setfacl -m u:100000:--x /home/vagrant/.local
sudo setfacl -m u:100000:--x /home/vagrant/.local/share

## start unprivileged container
lxc-start -n unprivileged --logpriority=DEBUG --logfile=lxc.log

## check container status
lxc-ls -f

## unprivileged container files
ls .local/share/lxc/unprivileged/
```

##### ğŸŒ LXD

```sh
# lxd configuration files
/var/lib/lxd
/var/log/lxd

# initialize lxd
sudo lxd init
sudo lxd init --auto
sudo cat lxd-init.yaml | lxd init --preseed

# check lxd version
sudo lxd --version

# check lxd status
systemctl status lxd

#### LXD STORAGE MANAGEMENT ####

# lxd list storage
lxc storage list

# show lxd storage pools
lxc storage show default

# lxd storage info
lxc storage info default

# craete a new storage pool dir
lxc storage create lpic3-dir dir 

# create a new storage pool lvm
lxc storage create lpic3-lvm lvm source=/dev/sdb1

# create a new storage pool btrfs
lxc storage create lpic3-btrfs btrfs
lxc storage create lpic3-btrfs btrfs size=10GB
lxc storage create lpic3-btrfs btrfs source=/dev/sdb2

# create a new storage pool zfs
lxc storage create lpic3-zfs zfs source=/dev/sdb3

# delete storage pool
lxc storage delete lpic3-btrfs

# edit storage pool
lxc storage edit lpic3-btrfs

# get storage pool properties
lxc storage  get lpic3-btrfs size

# set storage pool properties
lxc storage set lpic3-btrfs size 20GB

# list storage volumes
lxc storage volume list lpic3-btrfs

# create a new storage volume
lxc storage volume create lpic3-btrfs vol-lpic3-btrfs

# delete storage volume
lxc storage volume delete lpic3-btrfs vol-lpic3-btrfs

### managment lxd storage buckets ####

# create lxd bucket
lxc storage bucket create lpic3-btrfs bucket-lpic3-btrfs
lxc storage bucket create lpic3-zfs bucket-lpic3-zfs

# list lxd buckets
lxc storage bucket list lpic3-btrfs

# set lxd bucket properties
lxc storage bucket set lpic3-btrfs bucket-lpic3-btrfs size 10GB

# edit lxd bucket 
lxc storage bucket edit lpic3-btrfs bucket-lpic3-btrfs

# delete lxd bucket
lxc storage bucket delete lpic3-btrfs bucket-lpic3-btrfs

# show ldx storage bucket
lxc storage bucket show lpic3-btrfs bucket-lpic3-btrfs

# create storage bucket keys
lxc storage bucket key create lpic3-btrfs bucket-lpic3-btrfs key-bucket-lpic3-btrfs

# edit storage bucket keys
lxc storage bucket key edit lpic3-btrfs bucket-lpic3-btrfs key-bucket-lpic3-btrfs

# list storage bucket keys
lxc storage bucket key list lpic3-btrfs bucket-lpic3-btrfs

# show storage bucket keys
lxc storage bucket key show lpic3-btrfs bucket-lpic3-btrfs key-bucket-lpic3-btrfs

# delete storage bucket keys
lxc storage bucket key delete lpic3-btrfs bucket-lpic3-btrfs key-bucket-lpic3-btrfs

### LXD IMAGE MANAGEMENT ###

# list lxd repositories
lxc remote list

# add lxd remote repository
lxc remote add lpic3-images https://images.lxd.canonical.com --protocol=simplestreams

# remove lxd remote repository
lxc remote remove lpic3-images 

# list lxd images
lxc image list

# list lxd images from remote repository
lxc image list images:
lxc image list images: os=Ubuntu
lxc image list images: os=Ubuntu release=jammy
lxc image list images: os=Ubuntu release=jammy architecture=amd64
lxc image list images: architecture=amd64 type=containe
lxc image list images: d kal

# download lxd image to local
lxc image copy images:centos/9-Stream local: --alias centos-9

# export lxd remote image
lxc image export aed8a3749942  ./lxd-images/centos-9

# export lxd remote image
lxc image export images:f8fadb0d1b28 ./lxd-images/alma-9

# remove lxd image
lxc image delete centos-9

# mount lxd rootfs
mkdir -p /mnt/lxd-rootfs/centos-9
sudo mount lxd-images/centos-9/aed8a374994230243aaa82e979ac7d23f379e511556d35af051b1638662d47ae.squashfs  /mnt/lxd-rootfs/centos-9/
ls /mnt/lxd-rootfs/centos-9/

### LXD INSTANCES MANAGEMENT ###

# create a new container from image
lxc launch images:ubuntu/jammy ubuntu-lxd
lxc launch images:debian/12 debian12lxc
lxc launch images:fedora/41 fedora41
lxc launch images:opensuse/15.6 opensuse15

# create a new container from image with storage pool
lxc launch images:alpine/3.19 alpine --storage lpic3-lvm
lxc launch images:kali kali --storage lpic3-zfs

# create a new container from image local
lxc launch 757b2a721e9d kali-local-image

# create new vm
lxc launch --vm  images:debian/13 debian13 --storage lpic3-zfs
lxc launch --vm  images:e44d713a71b6 rocky9 --storage lpic3-btrfs

# list container\instances
lxc list

# stop container\instance
lxc stop alpine

# start container\instance
lxc start alpine

# delete container\instance
lxc delete alpine --force

# show container\instance
lxc info alpine

# show container\instance config
lxc config show alpine

# edit container\instance config
lxc config edit alpine

# view container\instance config
lxc config get alpine boot.autostart

# set container\instance config
lxc config set alpine boot.autostart=false

# set limit for container\instance
lxc config set alpine limits.cpu 2
lxc config set alpine limits.memory 10%

# unset limit for container\instance
lxc config unset alpine limits.cpu  
lxc config unset alpine limits.memory

# execute command in container\instance
lxc --exec alpine -- /bin/bash
lxc exec alpine -- uname -a || dhclient
lxc exec alpine -- sh -c "echo 'Hello from Alpine'"

# lxd copy file to container\instance
lxc file push /etc/hosts alpine/etc/hosts

# lxd edit file in container\instance
lxc file edit alpine/etc/hosts

# download file from container\instance
lxc file pull alpine/etc/hosts /tmp/alpine-hosts

### LXD NETWORK MANAGEMENT ###

# list networks
lxc network list

# show network details
lxc network show lxdbr0

# create a new network
lxc network create lxdbr1

# delete a network
lxc network delete lxdbr0

# show network details
lxc network show lxdbr0

# set ipv4.dhcp.ranges
lxc network set lxdbr0 ipv4.dhcp.ranges=10.119.220.100-10.119.220.200

# attach a network to a container
lxc network attach lxdbr0 alpine

# detach a network from a container
lxc network detach lxdbr0 alpine

### LXD SNAPSHOT MANAGEMENT ###

# snapshot files
/var/lib/lxd/snapshots/
/var/snap/lxd/common/lxd/snapshots

# create a snapshot
lxc snapshot debian12

# create a snapshot
lxc snapshot debian12 nome-snapshot

# restore a snapshot
lxc restore debian12 nome-snapshot

# delete a snapshot
lxc delete debian12/snap0

# show snapshot info
lxc info debian12

# copy a snapshot
lxc copy debian12/snap0 debian12-2

### LXD PROFILES MANAGEMENT ###

# list profiles
lxc profile list

# show profile details
lxc profile show default

# copy profile
lxc profile copy default production

# edit profile
lxc profile edit production

#set environment variables
lxc profile set production environment.EDITOR vim

# unset memory limit
lxc profile unset production limits.memory

# set boot autostart
lxc profile set production boot.autostart true

# add profile to container
lxc profile add debian12 production

# remove profile from container
lxc profile remove debian12 production

# launch container with profile
lxc launch 1u1u1u1u1u1 rockylinux9-2 -p producition

```

<p align="right">(<a href="#topic-352.2">back to sub topic 352.2</a>)</p>
<p align="right">(<a href="#topic-352">back to topic 352</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-352.3"></a>

### ğŸ³ 352.3 Docker

**Peso:**9

**DescriÃ§Ã£o:**O candidato deve ser capaz de gerenciar nÃ³s do Docker e recipientes do Docker. Isso inclui entender a arquitetura do Docker, alÃ©m de entender como o Docker interage com o sistema Linux do nÃ³.

**Principais Ã¡reas de conhecimento:**

-   Entenda a arquitetura e componentes do Docker
-   Gerencie os contÃªineres do Docker usando imagens de um registro do Docker
-   Entenda e gerencie imagens e volumes para recipientes do Docker
-   Entenda e gerencie o log para recipientes do docker
-   Entenda e gerencie a rede para o Docker
-   Use DockerFiles para criar imagens de contÃªiner
-   Execute um registro do Docker usando a imagem do Docker do Registro

#### ğŸ“‹ 352.3 Objetos citados

```sh
dockerd
/etc/docker/daemon.json
/var/lib/docker/
docker
Dockerfile
```

#### ğŸ› ï¸ 352.3 Comandos importantes

##### ğŸ³ Docker

```sh
# Examples of docker
```

<p align="right">(<a href="#topic-352.3">back to sub topic 352.3</a>)</p>
<p align="right">(<a href="#topic-352">back to topic 352</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-352.4"></a>

### ğŸ—‚ï¸ 352.4 Plataformas de orquestraÃ§Ã£o de contÃªineres

**Peso:**3

**DescriÃ§Ã£o:**Os candidatos devem entender a importÃ¢ncia da orquestraÃ§Ã£o de contÃªineres e os principais conceitos Docker Swarm e Kubernetes fornecem para implementar a orquestraÃ§Ã£o de contÃªineres.

**Principais Ã¡reas de conhecimento:**

-   Entenda a relevÃ¢ncia da orquestraÃ§Ã£o de contÃªineres
-   Entenda os principais conceitos de composiÃ§Ã£o do Docker e Swarm Docker
-   Entenda os principais conceitos de Kubernetes e comando
-   ConsciÃªncia do OpenShift, Rancher e Mesosfera DC/OS

<p align="right">(<a href="#topic-352.4">back to sub topic 352.4</a>)</p>
<p align="right">(<a href="#topic-352">back to topic 352</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-353"></a>

## â˜ï¸ TÃ³pico 353: VM Deployment and Provisioning

* * *

<a name="topic-353.1"></a>

### â˜ï¸ 353.1 Ferramentas de gerenciamento em nuvem

**Peso:**2

**DescriÃ§Ã£o:**Os candidatos devem entender as ofertas comuns em nuvens pÃºblicas e ter o conhecimento bÃ¡sico das ferramentas de gerenciamento de nuvem comumente disponÃ­veis.

**Principais Ã¡reas de conhecimento:**

-   Entender ofertas comuns em nuvens pÃºblicas
-   Conhecimento bÃ¡sico de recurso do OpenStack
-   Feature Basic Feature Knowledge of Terraform
-   ConsciÃªncia do CloudStack, Eucalyptus e Opennebula

#### ğŸ“‹ 353.1 Objetos citados

```sh
IaaS, PaaS, SaaS
OpenStack
Terraform
```

#### ğŸ› ï¸ 353.1 Comandos importantes

##### ğŸ“ Foo

```sh
# examples
```

<p align="right">(<a href="#topic-353.1">back to sub topic 353.1</a>)</p>
<p align="right">(<a href="#topic-353">back to topic 353</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-353.2"></a>

### ğŸ“¦ 353.2 Packer

**Peso:**2

**DescriÃ§Ã£o:**Os candidatos devem poder usar o Packer para criar imagens do sistema. Isso inclui a execuÃ§Ã£o do Packer em vÃ¡rios ambientes de nuvem pÃºblica e privada, alÃ©m de criar imagens de contÃªineres para LXC/LXD.

**Principais Ã¡reas de conhecimento:**

-   Entenda a funcionalidade e os recursos do Packer
-   Crie e mantenha arquivos de modelo
-   Crie imagens a partir de arquivos de modelo usando diferentes construtores

#### ğŸ“‹ 353.2 Objetos citados

```sh
packer
```

#### ğŸ› ï¸ 353.2 Comandos importantes

##### ğŸ“¦ Packer

```sh
# examples
```

<p align="right">(<a href="#topic-353.2">back to sub topic 353.2</a>)</p>
<p align="right">(<a href="#topic 353">back to topic 353</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-353.3"></a>

### â˜ï¸ 353.3 Cloud-Init

**Peso:**3

**DescriÃ§Ã£o:**Os candidatos devem usar a entrada da nuvem para configurar mÃ¡quinas virtuais criadas a partir de imagens padronizadas. Isso inclui o ajuste das mÃ¡quinas virtuais para corresponder aos seus recursos de hardware disponÃ­veis, especificamente, espaÃ§o em disco e volumes.
AlÃ©m disso, os candidatos devem poder configurar instÃ¢ncias para permitir logins SSH seguros e instalar um conjunto especÃ­fico de pacotes de software.
AlÃ©m disso, os candidatos devem ser capazes de criar novas imagens do sistema com suporte Ã  initÃªncia da nuvem.

**Principais Ã¡reas de conhecimento:**

-   Compreendendo os recursos e conceitos de entrada de nuvem, incluindo dados de usuÃ¡rio, inicializaÃ§Ã£o e configuraÃ§Ã£o
-   Use Cloud-Init para criar, redimensionar e montar sistemas de arquivos, configurar contas de usuÃ¡rio, incluindo credenciais de login, como teclas SSH e instalar pacotes de software do repositÃ³rio da distribuiÃ§Ã£o
-   Integre a nuvem-ingressos nas imagens do sistema
-   Use Config Drive DataSource para testar

#### ğŸ“‹ 353.3 Objetos citados

```sh
cloud-init
user-data
/var/lib/cloud/
```

#### ğŸ› ï¸ 353.3 Comandos importantes

##### ğŸ“ Foo

```sh
# examples
```

<p align="right">(<a href="#topic-353.3">back to sub topic 353.3</a>)</p>
<p align="right">(<a href="#topic 353">back to topic 353</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-353.4"></a>

### ğŸ“¦ 353.4 Vagrant

**Peso:**3

**DescriÃ§Ã£o:**O candidato deve poder usar o Vagrant para gerenciar mÃ¡quinas virtuais, incluindo o fornecimento da mÃ¡quina virtual.

**Principais Ã¡reas de conhecimento:**

-   Entenda a arquitetura e os conceitos vagantes, incluindo armazenamento e rede
-   Recuperar e usar caixas do Atlas
-   Crie e execute o VagrantFiles
-   Acesse mÃ¡quinas virtuais vagantes
-   Compartilhe e sincronize a pasta entre uma mÃ¡quina virtual vagante e o sistema host
-   Entenda o provisionamento vagante, ou seja, provisionistas de arquivos e shell
-   Entenda a configuraÃ§Ã£o de vÃ¡rias mÃ¡quinas

#### ğŸ“‹ 353.4 Objetos citados

```sh
vagrant
Vagrantfile
```

#### ğŸ› ï¸ 353.4 Comandos importantes

##### ğŸ“¦ Vagrant

```sh
# examples
```

<p align="right">(<a href="#topic-353.4">back to sub topic 353.4</a>)</p>
<p align="right">(<a href="#topic 353">back to topic 353</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o o que tornam a comunidade de cÃ³digo aberto um lugar tÃ£o incrÃ­vel para
Aprenda, inspire e crie. Quaisquer contribuiÃ§Ãµes que vocÃª faz sÃ£o**muito apreciado**.

Se vocÃª tiver uma sugestÃ£o que melhoraria isso, bifÃ³r -se o repositÃ³rio e
Crie uma solicitaÃ§Ã£o de traÃ§Ã£o. VocÃª tambÃ©m pode simplesmente abrir um problema com a tag "aprimoramento".
NÃ£o se esqueÃ§a de dar uma estrela ao projeto! Obrigado novamente!

1.  Bifurcar o projeto
2.  Crie sua filial de recursos (`git checkout -b feature/AmazingFeature`)
3.  Compreenda suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4.  Empurre para o ramo (`git push origin feature/AmazingFeature`)
5.  Abra um pedido de traÃ§Ã£o

* * *

## ğŸ“„ LicenÃ§a

-   Este projeto estÃ¡ licenciado sob a licenÃ§a do MIT \* Veja o arquivo License.md para obter detalhes

* * *

## ğŸ“¬ Contato

Marcos Silvestrini -[marcos.silvestrini@gmail.com](mailto:marcos.silvestrini@gmail.com)[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/mrsilvestrini.svg?style=social&label=Follow%20%40mrsilvestrini)](https://twitter.com/mrsilvestrini)

Link do projeto:<https://github.com/marcossilvestrini/learning-lpic-3-305-300>

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

## ğŸ™ Agradecimentos

-   [Richard Stallman's](http://www.stallman.org/)
-   [GNU](<>)
    -   [GNU/FAQ Linux por Richard Stallman](https://www.gnu.org/gnu/gnu-linux-faq.html)
    -   [GNU](https://www.gnu.org/)
    -   [Sistema operacional GNU](https://www.gnu.org/gnu/thegnuproject.html)
    -   [Compilador GCC](https://gcc.gnu.org/wiki/History)
    -   [GNU tar](https://www.gnu.org/software/tar/)
    -   [GNU faz](https://www.gnu.org/software/make/)
    -   [GNU Emacs](https://en.wikipedia.org/wiki/Emacs)
    -   [Pacotes GNU](https://www.gnu.org/software/)
    -   [ColeÃ§Ã£o GNU/Linux](https://directory.fsf.org/wiki/Collection:GNU/Linux)
    -   [GNU GRUB Bootloader](https://www.gnu.org/software/grub/)
    -   [GNU Hurd](https://www.gnu.org/software/hurd/hurd/what_is_the_gnu_hurd.html)
-   [Kernel](<>)
    -   [Kernel](https://www.kernel.org/)
    -   [PÃ¡ginas do Kernel Linux](https://www.kernel.org/doc/man-pages/)
    -   [Compile seu kernel](https://wiki.linuxquestions.org/wiki/How_to_build_and_install_your_own_Linux_kernel)
-   [Base padrÃ£o Linux](<>)
    -   [Base padrÃ£o Linux](https://en.wikipedia.org/wiki/Linux_Standard_Base)
    -   [PadrÃ£o de hierarquia do sistema de arquivos](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard)
    -   [Estrutura de hierarquia de arquivos](https://refspecs.linuxfoundation.org/FHS_3.0/fhs-3.0.pdf)
-   [Software livre](<>)
    -   [FSF](https://www.fsf.org)
    -   [DiretÃ³rio de software livre](https://directory.fsf.org/wiki/Free_Software_Directory:Free_software_replacements)
-   [LicenÃ§a](<>)
    -   [Software livre](https://www.gnu.org/philosophy/free-sw.html)
    -   [Copyleft](https://www.gnu.org/licenses/copyleft.en.html)
    -   [Gpl](https://www.gnu.org/licenses/quick-guide-gplv3.html)
    -   [GNU LicenÃ§a pÃºblica geral menor](https://www.gnu.org/licenses/lgpl-3.0.html)
    -   [BSD](https://opensource.org/licenses/BSD-3-Clause)
    -   [Iniciativa de cÃ³digo aberto](https://opensource.org/)
    -   [Creative Commons](https://creativecommons.org/)
    -   [LicenÃ§a LTS](https://en.wikipedia.org/wiki/Long-term_support)
-   [Distos](<>)
    -   [Diretrizes de software livre do Debian](https://www.debian.org/social_contract#guidelines)
    -   [Lista de distribuiÃ§Ã£o Linux](https://en.wikipedia.org/wiki/List_of_Linux_distributions)
    -   [Distrowatch](https://distrowatch.com/)
    -   [ComparaÃ§Ã£o DistribuiÃ§Ãµes Linux](https://en.wikipedia.org/wiki/Comparison_of_Linux_distributions)
-   [Ambientes de mesa](<>)
    -   [X11 org](https://www.x.org/wiki/)
    -   [Wayland](https://wayland.freedesktop.org/)
    -   [GNU Gnome](https://www.gnu.org/press/gnome-1.0.html)
    -   [GNOMO](https://www.gnome.org/)
    -   [Xfce](https://xfce.org/)
    -   [Onde plasma](https://kde.org/plasma-desktop/)
    -   [Harmonia](https://en.wikipedia.org/wiki/Harmony_(toolkit))
-   [Protocolos](<>)
    -   [Http](<>)
        -   [W3Techs](https://w3techs.com/)
        -   [Apache](https://www.apache.org/)
        -   [Diretivas Apache](https://httpd.apache.org/docs/2.4/mod/directives.html)
        -   [CÃ³digos de status HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)
        -   [Cifras fortes para Apache, Nginx e LightTPD](https://cipherlist.eu/)
        -   [Tutoriais da SSL](https://www.golinuxcloud.com/blog/)
        -   [SSL Config Mozilla](https://ssl-config.mozilla.org/)
    -   [XRDP](https://bytexd.com/xrdp-centos/)
    -   [Ntp](https://www.ntppool.org/en/)
-   [Dns](<>)
    -   [Vincular](https://www.isc.org/bind/)
    -   [Vincular o log](https://www.zytrax.com/books/dns/ch7/logging.html)
    -   [Lista de tipos de registro DNS](https://en.wikipedia.org/wiki/List_of_DNS_record_types)
    -   [Lista de tipos de registro DNS](https://en.wikipedia.org/wiki/List_of_DNS_record_types)
-   [Gerente de pacotes](<>)
    -   [Baixar pacotes](https://pkgs.org/)
    -   [Instale pacotes](https://installati.one/)
    -   [Guia de instalaÃ§Ã£o de pacotes](https://installati.one/)
-   [Script de shell](<>)
    -   [Bourne novamente Shell](https://www.gnu.org/software/bash/manual/)
    -   [Shebang](https://bash.cyberciti.biz/guide/Shebang)
    -   [VariÃ¡veis de ambiente](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)
    -   [GNU Globbing](https://man7.org/linux/man-pages/man7/glob.7.html)
    -   [Globbing](https://linuxhint.com/bash_globbing_tutorial/)
    -   [Citando](https://www.gnu.org/software/bash/manual/html_node/Quoting.html)
    -   [ExpressÃµes regulares](https://www.gnu.org/software/grep/manual/html_node/Regular-Expressions.html)
    -   [Comando nÃ£o encontrado](https://command-not-found.com/)
    -   [Bash gerador de prompt](https://bash-prompt-generator.org/)
    -   [Explique a casca](https://explainshell.com/)
    -   [Tutorial Vim](https://www.openvim.com/)
    -   [Tutorial de script linux shell](https://bash.cyberciti.biz/guide/Main_Page)
    -   [Comandos exemplos](https://www.geeksforgeeks.org/)
-   [Outras ferramentas](<>)
    -   [Bugzila](https://bugzilla.kernel.org/)
    -   [CrachÃ¡s do github](https://github.com/alexandresanlim/Badges4-README.md-Profile)
-   [DefiniÃ§Ãµes de virtualizaÃ§Ã£o](<>)
    -   [ChapÃ©u vermelho](https://www.redhat.com/pt-br/topics/virtualization/what-is-virtualization/)
    -   [AWS](https://aws.amazon.com/pt/what-is/virtualization/)
    -   [IBM](https://www.ibm.com/topics/virtualization)
    -   [OpenSource.com](https://opensource.com/resources/virtualization)
-   [Alternar](<>)
    -   [Xenserver](https://www.xenserver.com/)
    -   [Wiki XenProject](https://wiki.xenproject.org/wiki/Main_Page)
    -   [Interfaces de rede](https://wiki.xenproject.org/wiki/Xen_Networking#Virtual_Network_Interfaces)
    -   [Ferramentas Xen](https://xen-tools.org/software/)
    -   [LPI Blog: Xen VirtualizaÃ§Ã£o e computaÃ§Ã£o em nuvem #01: IntroduÃ§Ã£o](https://www.lpi.org/pt-br/blog/2020/10/01/xen-virtualization-and-cloud-computing-01-introduction/)
    -   [LPI Blog: Xen VirtualizaÃ§Ã£o e computaÃ§Ã£o em nuvem #02: Como Xen faz o trabalho](https://www.lpi.org/blog/2020/10/08/xen-virtualization-and-cloud-computing-02-how-xen-does-job/)
    -   [LPI Blog: Xen VirtualizaÃ§Ã£o e computaÃ§Ã£o em nuvem #04: ContÃªineres, OpenStack e outras plataformas relacionadas](https://www.lpi.org/pt-br/blog/2020/10/22/xen-virtualization-and-cloud-computing-04-containers-openstack-and-other-related/)
    -   [VirtualizaÃ§Ã£o Xen e ComputaÃ§Ã£o em Cloud #05: O Projeto Xen, Unikernels e o Futuro](https://www.lpi.org/pt-br/blog/2020/10/29/xen-virtualization-and-cloud-computing-05-xen-project-unikernels-and-future/)
    -   [Guia para iniciantes do projeto Xen](https://wiki.xenproject.org/wiki/Xen_Project_Beginners_Guide#Installing_the_Xen_Project_Software)
    -   [Livro maluco](https://wiki.xenproject.org/wiki/Book/HelloXenProject/0-Contents)
-   [Unicernel](https://www.lpi.org/blog/2020/10/29/xen-virtualization-and-cloud-computing-05-xen-project-unikernels-and-future/)
    -   [ForÃ§a Ãºnica](https://github.com/unikraft/unikraft)
    -   [Mirageos](https://mirage.io/docs/hello-world)
    -   [Ruim](https://galois.com/project/halvm/)
    -   [Exclusivo](https://github.com/solo-io/unik/blob/master/docs/providers/virtualbox.md)
-   [KVM](<>)
    -   [Oficial Doc](https://linux-kvm.org/page/Main_Page)
    -   [KVM (mÃ¡quinas virtuais do kernel por redhat)](https://www.redhat.com/pt-br/topics/virtualization/what-is-KVM)
    -   [Ferramentas de gerenciamento da KVM](https://www.linux-kvm.org/page/Management_Tools)
    -   [Rede KVM](https://www.linux-kvm.org/page/Networking)
-   [Qemu](<>)
    -   [Oficial Doc](https://www.qemu.org/)
    -   [Baixe imagens osboxes](https://www.osboxes.org/)
    -   [FaÃ§a o download de imagens linuximages](https://www.linuxvmimages.com/)
    -   [Urbano](https://en.wikibooks.org/wiki/QEMU/Devices/Virtio)
    -   [Agente convidado](https://wiki.qemu.org/Features/GuestAgent)
-   [Libvirt](<>)
    -   [Oficial Doc](https://libvirt.org/)
    -   [AtivaÃ§Ã£o do soquete do sistema](https://libvirt.org/manpages/libvirtd.html#system-socket-activation)
    -   [ConexÃµes](https://libvirt.org/uri.html)
    -   [Armazenar](https://libvirt.org/storage.html)
    -   [Rede](https://wiki.libvirt.org/Networking.html)
    -   [VirtualNetwork](https://wiki.libvirt.org/VirtualNetworking.html)
    -   [Virtogd](https://libvirt.org/manpages/virtlogd.html)
    -   [Virtlockd](https://libvirt.org/manpages/virtlockd.html)
    -   [virt-manager](https://virt-manager.org/)
-   [Gerenciamento de disco](<>)
    -   [Imagens de disco](https://qemu-project.gitlab.io/qemu/system/images.html)
    -   [cÃ³pia-em-escrever](https://sempreupdate.com.br/linux/tutoriais/sistema-de-arquivos-copy-on-write-saiba-o-que-e-e-quais-as-vantagens-e-desvantagens/)
    -   [RAM X QCOW2](https://docs.redhat.com/en/documentation/red_hat_virtualization/4.3/html/technical_reference/qcow2)
    -   [Libguestfs](https://libguestfs.org/)
-   [ContÃªineres](<>)
    -   [ContÃªineres da AWS DOC](https://aws.amazon.com/pt/containers/)
    -   [ContÃªineres do DOC GCP](https://cloud.google.com/learn/what-are-containers?hl=pt-br)
    -   [IBM Doc Container](https://www.ibm.com/br-pt/topics/containers)
    -   [Red Hat Docs Containers](https://www.redhat.com/en/topics/containers/whats-a-linux-container)
    -   [Namespaces](https://manpages.ubuntu.com/manpages/noble/man7/namespaces.7.html)
    -   [Os namespaces mais importantes](https://www.redhat.com/en/blog/7-linux-namespaces)
    -   [Classes CGROUPS](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/resource_management_guide/ch01)
    -   [Homem CGROUPS](https://manpages.ubuntu.com/manpages/noble/man7/cgroups.7.html)
    -   [Recursos Doc](https://linux-audit.com/kernel/capabilities/linux-capabilities-101/)
    -   [Capacidades do homem](https://manpages.ubuntu.com/manpages/noble/man7/capabilities.7.html)
    -   [Perfis Seccomp no Docker](https://docs.docker.com/engine/security/seccomp/)
    -   [Perfis de Aparmor no Docker](https://docs.docker.com/engine/security/apparmor/)
    -   [Selinux](https://pt.wikipedia.org/wiki/SELinux)
    -   [ComparaÃ§Ã£o do Appmor Selinux](https://www.redhat.com/en/blog/apparmor-selinux-isolation)
    -   [Runc](https://www.docker.com/blog/runc/)
    -   [Runc Github](https://github.com/opencontainers/runc)
    -   [OCI](https://opencontainers.org/about/overview/)
    -   [Cri](https://kubernetes.io/docs/concepts/architecture/cri/)
    -   [CRI-O](https://cri-o.io/)
    -   [contÃªiner](https://containerd.io/)
    -   [Subman](https://www.redhat.com/pt-br/topics/containers/what-is-podman)
    -   [Escopo](https://www.redhat.com/pt-br/topics/containers/what-is-skopeo)
    -   [Buildah](https://www.redhat.com/en/topics/containers/what-is-buildah)
    -   [OpenVZ](https://openvz.org/)
    -   [Crun](https://www.redhat.com/en/blog/introduction-crun)
    -   [dizer](https://katacontainers.io/)
-   [LXC - ContÃªineres Linux](<>)
    -   [LXC](https://linuxcontainers.org/lxc/introduction/)
    -   [Imagens de contÃªiner Linux](https://images.linuxcontainers.org/)
-   [Lxd]
    -   [Lxd canÃ´nico](https://canonical.com/lxd)
    -   [LXD Github canÃ´nico](https://github.com/canonical/lxd)
    -   [DocumentaÃ§Ã£o LXD](https://linuxcontainers.org/lxd/docs/master/)
    -   [InstalaÃ§Ã£o LXD](https://documentation.ubuntu.com/lxd/stable-4.0/instances/)
    -   [Imagens LDX](https://images.lxd.canonical.com/)
    -   [LXD Storage](https://documentation.ubuntu.com/lxd/stable-4.0/storage/)
    -   [Piscinas de armazenamento LXD, volumes e baldes](https://documentation.ubuntu.com/lxd/stable-5.21/explanation/storage/#exp-storage)
    -   [Tipos de rede LXD](https://documentation.ubuntu.com/lxd/latest/explanation/networks/)
    -   [ParÃ¢metros de rede LXD](https://documentation.ubuntu.com/lxd/stable-4.0/networks/)
    -   [ConfiguraÃ§Ã£o da rede LXD](https://documentation.ubuntu.com/lxd/latest/howto/network_create/)
    -   [Perfis LXD](https://documentation.ubuntu.com/lxd/to/latest/profiles/)
    -   [InstÃ¢ncias LXD](https://documentation.ubuntu.com/lxd/en/stable-4.0/instances/)
-   [OpenStack Docs](<>)
    -   [Redhat](https://www.redhat.com/pt-br/topics/openstack)
-   [Aberto vswitch](<>)
    -   [OVS DOC 4LINUX](https://blog.4linux.com.br/open-vswitch-o-que-e-o-que-come-onde-vive)
-   [Exame LPIC-3 305-300](<>)
    -   [Lpic-3 305-300 Objetivos](https://www.lpi.org/our-certifications/exam-305-objectives/)
    -   [LPIC-3 305-300 Wiki](https://wiki.lpi.org/wiki/LPIC-305_Objectives_V3.0)
    -   [Lpic-3 305-300 Material de aprendizado](https://cursos.linuxsemfronteiras.com.br/courses/preparatorio-para-certificacao-lpic-3-305/)
    -   [LPIC-3 305-300 Exame simulado pela ITEXAMS](https://www.itexams.com/info/305-300)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<!-- MARKDOWN LINKS & IMAGES -->

<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->

[contributors-shield]: https://img.shields.io/github/contributors/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[contributors-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/graphs/contributors

[forks-shield]: https://img.shields.io/github/forks/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[forks-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/network/members

[stars-shield]: https://img.shields.io/github/stars/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[stars-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/stargazers

[issues-shield]: https://img.shields.io/github/issues/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[issues-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/issues

[license-shield]: https://img.shields.io/github/license/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[license-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/blob/main/LICENSE

[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555

[linkedin-url]: https://linkedin.com/in/marcossilvestrini

[def]: https://httpd.apache.org/docs/2.4/mod/directives.html
