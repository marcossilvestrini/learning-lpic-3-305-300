<h1><a name="readme-top"></a></h1>

[![Create Release](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/release.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/release.yml)[![Translate README](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/translate.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/translate.yml)[![Generate HTML and PDF](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/generate-html.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/generate-html.yml)[![Deploy Webpage](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/deploy-webpage.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/deploy-webpage.yml)[![Generate GitBook Docs](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/generate-docs.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/generate-docs.yml)[![PSScriptAnalyzer](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/powershell.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/powershell.yml)[![Slack Notification](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/slack.yml/badge.svg)](https://github.com/marcossilvestrini/learning-lpic-3-305-300/actions/workflows/slack.yml)

* * *

[![MIT License][license-shield]][license-url][![Forks][forks-shield]][forks-url][![Stargazers][stars-shield]][stars-url][![Contributors][contributors-shield]][contributors-url][![Issues][issues-shield]][issues-url][![LinkedIn][linkedin-shield]][linkedin-url]

* * *

# Apprendre le LPIC-3 305-300

![LPIC3-305-300](images/lpic3-305-300.jpg)

<p align="center">
<strong>Explore the docs ¬ª</strong></a>
    <br />
    <a href="https://marcossilvestrini.github.io/learning-lpic-3-305-300/">Web Site</a>
    -
    <a href="https://github.com/marcossilvestrini/learning-lpic-3-305-300">Code Page</a>
    -
    <a href="https://skynet-8.gitbook.io/learning-lpic-3-305-300">Gitbook</a>
    -
    <a href="https://github.com/marcossilvestrini/learning-lpic-3-305-300/issues">Report Bug</a>
    -
    <a href="https://github.com/marcossilvestrini/learning-lpic-3-305-300/issues">Request Feature</a>
</p>

* * *

## R√©sum√©

<details>
  <summary><b>TABLE OF CONTENT</b></summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#installation">installation</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#roadmap">Roadmap</a></li>
    <li><a href="#freedoms">Four Essential Freedoms</a></li>
    <li>
      <a href="#topic-351">Topic 351: Full Virtualization</a>
      <ul>
        <li><a href="#topic-351.1">351.1 Virtualization Concepts and Theory </a></li>
        <li><a href="#topic-351.2">351.2 Xen</a></li>
        <li><a href="#topic-351.3">351.3 QEMU</a></li>
        <li><a href="#topic-351.4">351.4 Libvirt Virtual Machine</a></li>
        <li><a href="#topic-351.5">351.5 Virtual Machine Disk Image Management</a></li>
      </ul>
    </li>
    <li>
      <a href="#topic-352">Topic 352: Container Virtualization</a>
      <ul>
        <li><a href="#topic-352.1">352.1 Container Virtualization Concepts</a></li>
        <li><a href="#topic-352.2">352.2 LXC</a></li>
        <li><a href="#topic-352.3">352.3 Docker</a></li>
        <li><a href="#topic-352.4">352.4 Container Orchestration Platforms</a></li>
      </ul>
    </li>
    <li>
      <a href="#topic-353">Topic 353: VM Deployment and Provisioning</a>
      <ul>
        <li><a href="#topic-353.1">353.1 Cloud Management Tools</a></li>
        <li><a href="#topic-353.2">353.2 Packer</a></li>
        <li><a href="#topic-353.3">353.3 cloud-init</a></li>
        <li><a href="#topic-353.4">353.4 Vagrant</a></li>
      </ul>
    </li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
    <li><a href="#acknowledgments">Acknowledgments</a></li>
  </ol>
</details><br>

* * *

<a name="about-the-project"></a>

## √Ä propos du projet

> Ce projet vise √† aider les √©tudiants ou les professionnels √† apprendre les principaux concepts de Gnulinux
> et logiciel gratuit \\
> Certaines distributions Gnunux comme Debian et RPM seront couvertes \\
> L'installation et la configuration de certains packages seront √©galement couvertes \\
> En faisant cela, vous pouvez donner √† toute la communaut√© une chance de b√©n√©ficier de vos modifications. \\
> L'acc√®s au code source est une condition pr√©alable √† cela. \\
> Utilisez Vagrant pour les machines UP et ex√©cutez des laboratoires et pratiquez le contenu de cet article. \\
> J'ai publi√© dans Folder Vagrant A Vagrantfile avec ce qui est n√©cessaire \\
> Pour que vous puissiez t√©l√©charger un environnement pour les √©tudes

* * *

<p align="right">(<a href="#readme-top">back to top</a>)</p>

<a name="getting-started"></a>

## Commencer

Pour commencer l'apprentissage, consultez la documentation ci-dessus.

<a name="prerequisites"></a>

### Condition pr√©alable

-   [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
-   [Passe de travail VMware](https://blogs.vmware.com/workstation/2024/05/vmware-workstation-pro-now-available-free-for-personal-use.html)
-   [Utilitaire VMware vaginal](https://developer.hashicorp.com/vagrant/install/vmware)
-   [Vagabond](https://developer.hashicorp.com/vagrant/install)

<a name="installation"></a>

### Installation

Cloner le repo

```sh
git clone https://github.com/marcossilvestrini/learning-lpic-3-305-300.git
cd learning-lpic-3-305-300
```

Personnaliser un mod√®le_Vagrantfile-topic-xxx_. Ce fichier contient une configuration VMS pour les laboratoires. Exemple:

-   D√©poser[Vagrantfile-topic-351](vagrant/Vagrantfile-topic-351)
    -   vm.clone_directory = "&lt;your_driver_letter>:\\<folder>\\&lt;To_machine>\\# {Vm_name} -instance-1 "
        Exemple: vm.clone_directory = "E:\\Serveurs\\Vmware\\# {Vm_name} -instance-1 "
    -   vm.vmx["memsize"]= ""
    -   vm.vmx["Numvcpus"]= ""
    -   vm.vmx["CPUID.CorRrespersoCout"]= ""

Personnaliser la configuration du r√©seau dans les fichiers[configs / r√©seau](configs/network/).

* * *

<a name="usage"></a>

## Usage

Utilisez ce r√©f√©rentiel pour obtenir un apprentissage sur l'examen LPIC-3 305-300

### Pour de haut en bas

Changer un_Vagrantfile-topic-xxx_mod√®le et copier pour un nouveau fichier avec nom_Vagabond_

```sh
cd vagrant && vagrant up
cd vagrant && vagrant destroy -f
```

### Pour red√©marrer les machines virtuelles

```sh
cd vagrant && vagrant reload
```

**Important:**_Si vous red√©marrez les machines virtuelles sans vagabond, le dossier partag√© ne monte pas apr√®s le d√©marrage._

### Utiliser PowerShell pour le haut et le bas

Si vous utilisez la plate-forme Windows, je cr√©e un script PowerShell pour les machines virtuelles de haut en bas.

```powershell
vagrant/up.ps1
vagrant/destroy.ps1
```

### Sch√©ma d'infrastructure Sujet 351

![topic-351](images/infraestructure-topic-351.png)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="roadmap"></a>

## Feuille de route

-   [x] Cr√©er un r√©f√©rentiel
-   [x] Cr√©er des scripts pour l'approvisionnement des laboratoires
-   [x] Cr√©er des exemples sur le sujet 351
-   [ ] Cr√©er des exemples sur le sujet 352
-   [ ] Cr√©er des exemples sur le sujet 353
-   [ ] T√©l√©charger simul√© iTexam

* * *

<a name="freedoms"></a>

## Quatre libert√©s essentielles

> 0\. La libert√© d'ex√©cuter le programme comme vous le souhaitez, √† quelque fin que ce soit (libert√© 0). \\
> 1.La libert√© d'√©tudier le fonctionnement du programme et de le changer pour qu'il ait \\
> votre informatique comme vous le souhaitez (Freedom 1). \\
> L'acc√®s au code source est une condition pr√©alable √† cela. \\
> 2\. La libert√© de redistribuer des copies afin que vous puissiez aider les autres (Freedom 2). \\
> 3\. Freedom pour distribuer des copies de vos versions modifi√©es √† d'autres (Freedom 3).

* * *

## Inspecter les commandes

```sh
type COMMAND
apropos COMMAND
whatis COMMAND --long
whereis COMMAND
COMMAND --help, --h
man COMMAND
```

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351"></a>

## Sujet 351: Virtualisation compl√®te

![LPIC3-305-300](images/virtualization-351.png)

* * *

<a name="topic-351.1"></a>

### 351.1 Concepts de virtualisation et th√©orie

**Poids:**6

**Description:**Les candidats doivent conna√Ætre et comprendre les concepts g√©n√©raux, la th√©orie et la terminologie de la virtualisation. Cela comprend la terminologie Xen, Qemu et Libvirt.

**Zones de connaissances cl√©s:**

-   üñ•Ô∏è comprendre la terminologie de la virtualisation
-   ‚öñÔ∏è comprendre les avantages et les inconv√©nients de la virtualisation
-   üõ†Ô∏è Comprenez les diff√©rentes variations des hyperviseurs et des moniteurs de machines virtuelles
-   üîÑ Comprendre les principaux aspects de la migration des machines physiques vers virtuelles
-   üöÄ Comprendre les principaux aspects de la migration des machines virtuelles entre les syst√®mes h√¥tes
-   üì∏ Comprendre les caract√©ristiques et les implications de la virtualisation pour une machine virtuelle, comme les instantan√©s, la pause, le clonage et les limites de ressources
-   üåê Conscience d'Ovirt, Proxmox, Systemd-Machine et VirtualBox
-   üîó Sensibilisation du vSwitch ouvert

#### 351.1 objets cit√©s

```sh
Hypervisor
Hardware Virtual Machine (HVM)
Paravirtualization (PV)
Emulation and Simulation
CPU flags
/proc/cpuinfo
Migration (P2V, V2V)
```

#### Hyperviseurs

##### Hyperviseur de type 1 (hyperviseur √† m√©tal nu)

###### D√©finition de type 1

S'ex√©cute directement sur le mat√©riel physique de l'h√¥te, fournissant une couche de base pour g√©rer les machines virtuelles sans avoir besoin d'un syst√®me d'exploitation h√¥te.

###### Caract√©ristiques de type 1

-   ‚ö° Haute performance et efficacit√©.
-   ‚è±Ô∏è latence et les frais g√©n√©raux inf√©rieurs.
-   üè¢ Souvent utilis√© dans les environnements d'entreprise et les centres de donn√©es.

###### Exemples de type 1

-   VMware ESXi: un hyperviseur robuste et largement utilis√© dans les param√®tres d'entreprise.
-   Microsoft Hyper-V: int√©gr√© √† Windows Server, offrant des performances et des fonctionnalit√©s de gestion solides.
-   Xen: un hyperviseur open source utilis√© par de nombreux fournisseurs de services cloud.
-   KVM (machine virtuelle bas√©e sur le noyau): int√©gr√© dans le noyau Linux, offrant des performances √©lev√©es pour les syst√®mes bas√©s sur Linux.

##### Hyperviseur de type 2 (hyperviseur h√©berg√©)

###### D√©finition de type 2

Ex√©cute sur un syst√®me d'exploitation conventionnel, en s'appuyant sur le syst√®me d'exploitation h√¥te pour la gestion des ressources et la prise en charge des appareils.

###### Caract√©ristiques de type 2

-   üõ†Ô∏è Plus facile √† configurer et √† utiliser, en particulier sur les ordinateurs personnels.
-   üîß Plus flexible pour le d√©veloppement, les tests et les d√©ploiements √† plus petite √©chelle.
-   üê¢ Typiquement moins efficace que les hyperviseurs de type 1 en raison des frais g√©n√©raux suppl√©mentaires du syst√®me d'exploitation h√¥te.

###### Exemples de type 2

-   VMware Workstation: un puissant hyperviseur pour ex√©cuter plusieurs syst√®mes d'exploitation sur un seul bureau.
-   Oracle VirtualBox: un hyperviseur open source connu pour sa flexibilit√© et sa facilit√© d'utilisation.
-   Parallels Desktop: Con√ßu pour que les utilisateurs de Mac puissent ex√©cuter Windows et d'autres syst√®mes d'exploitation aux c√¥t√©s de macOS.
-   Qemu (√©mulateur rapide): un √©mulateur et virtualiseur open source, souvent utilis√©s en conjonction avec KVM.

##### Diff√©rences cl√©s entre les hyperviseurs de type 1 et de type 2

-   Environnement de d√©ploiement:
    -   Les hyperviseurs de type 1 sont couramment d√©ploy√©s dans les centres de donn√©es et les environnements d'entreprise en raison de leur interaction directe avec le mat√©riel et des performances √©lev√©es.
    -   Les hyperviseurs de type 2 conviennent plus √† un usage personnel, au d√©veloppement, aux tests et aux t√¢ches de virtualisation √† petite √©chelle.
-   Performance:
    -   Les hyperviseurs de type 1 offrent g√©n√©ralement de meilleures performances et une latence plus faible car elles ne comptent pas sur un syst√®me d'exploitation h√¥te.
    -   Les hyperviseurs de type 2 peuvent subir une certaine d√©gradation des performances en raison des frais g√©n√©raux de fonctionnement au-dessus d'un syst√®me d'exploitation h√¥te.
-   Gestion et facilit√© d'utilisation:
    -   Les hyperviseurs de type 1 n√©cessitent une configuration et une gestion plus complexes, mais fournissent des fonctionnalit√©s avanc√©es et une √©volutivit√© pour les d√©ploiements √† grande √©chelle.
    -   Les hyperviseurs de type 2 sont plus faciles √† installer et √† utiliser, ce qui les rend id√©aux pour les utilisateurs individuels et les petits projets.

##### Types de migration

Dans le contexte des hyperviseurs, qui sont des technologies utilis√©es pour cr√©er et g√©rer les machines virtuelles, les termes migration P2V et migration V2V sont courants dans les environnements de virtualisation.  
Ils se r√©f√®rent aux processus de migration des syst√®mes entre diff√©rents types de plates-formes.

##### P2V - Migration physique √† virtuelle

La migration P2V fait r√©f√©rence au processus de migration d'un serveur physique vers une machine virtuelle.  
En d'autres termes, un syst√®me d'exploitation et ses applications, fonctionnant sur du mat√©riel physique d√©di√©, sont "convertis" et d√©plac√©s vers une machine virtuelle qui s'ex√©cute sur un hyperviseur (comme VMware, Hyper-V, KVM, etc.).

-   Exemple: vous avez un serveur physique ex√©cutant un syst√®me Windows ou Linux, et vous souhaitez le d√©placer vers un environnement virtuel, comme une infrastructure cloud ou un serveur de virtualisation interne.  
    Le processus consiste √† copier l'ensemble de l'√©tat du syst√®me, y compris le syst√®me d'exploitation, les pilotes et les donn√©es, pour cr√©er une machine virtuelle √©quivalente qui peut s'ex√©cuter comme si elle se trouvait sur le mat√©riel physique.

##### V2V - Migration virtuelle √† virtuelle

La migration V2V fait r√©f√©rence au processus de migration d'une machine virtuelle d'un hyperviseur √† un autre.  
Dans ce cas, vous avez d√©j√† une machine virtuelle ex√©cut√©e dans un environnement virtualis√© (comme VMware), et vous souhaitez le d√©placer vers un autre environnement virtualis√© (par exemple, √† Hyper-V ou vers un nouveau serveur VMware).

-   Exemple: vous avez une machine virtuelle en cours d'ex√©cution sur un serveur de virtualisation VMware, mais vous d√©cidez de le migrer vers une plate-forme Hyper-V. Dans ce cas, la migration V2V convertit la machine virtuelle d'un format ou d'un hyperviseur √† un autre, garantissant qu'il peut continuer √† fonctionner correctement.

#### HVM et paravirtualisation

##### Virtualisation assist√©e par mat√©riel (HVM)

###### HVM Definition

HVM exploite les extensions mat√©rielles fournies par les processeurs modernes pour virtualiser le mat√©riel, permettant la cr√©ation et la gestion des machines virtuelles avec des frais g√©n√©raux minimaux.

###### Caract√©ristiques cl√©s HVM

-   üñ•Ô∏è**Support mat√©riel**: N√©cessite une prise en charge du processeur pour les extensions de virtualisation telles que Intel VT-X ou AMD-V.
-   üõ†Ô∏è**Virtualisation compl√®te:**Les VM peuvent ex√©cuter des syst√®mes d'exploitation invit√©s non modifi√©s, car l'hyperviseur fournit une √©mulation compl√®te de l'environnement mat√©riel.
-   ‚ö°**Performance:**Offre g√©n√©ralement des performances presque natives en raison de l'ex√©cution directe du code invit√© sur le CPU.
-   üîí**Isolement:**Fournit une forte isolement entre les machines virtuelles car chaque machine virtuelle fonctionne comme si elle avait son propre mat√©riel d√©di√©.

###### HVM Examples

VMware Esxi, Microsoft Hyper-V, KVM (machine virtuelle bas√©e sur le noyau).

###### HVM Advantages

-   ‚úÖ**Compatibilit√©:**Peut ex√©cuter n'importe quel syst√®me d'exploitation sans modification.
-   ‚ö°**Performance:**Haute performances en raison de la prise en charge mat√©rielle.
-   üîí**S√©curit√©:**Des fonctionnalit√©s d'isolement et de s√©curit√© am√©lior√©es fournies par le mat√©riel.

###### HVM Disadvantages

-   üõ†Ô∏è**D√©pendance mat√©rielle:**N√©cessite des fonctionnalit√©s mat√©rielles sp√©cifiques, limitant la compatibilit√© avec les syst√®mes plus anciens.
-   üîß**Complexit√©:**Peut impliquer une configuration et une gestion plus complexes.

##### Paravirtualisation

###### D√©finition de paravirtualisation

La paravirtualisation consiste √† modifier le syst√®me d'exploitation invit√© pour √™tre conscient de l'environnement virtuel, ce qui lui permet d'interagir plus efficacement avec l'hyperviseur.

###### Paravirtualisation des caract√©ristiques cl√©s

-   üõ†Ô∏è**Modification des invit√©s:**N√©cessite des modifications apport√©es au syst√®me d'exploitation des invit√©s pour communiquer directement avec l'hyperviseur √† l'aide d'hypercaux.
-   ‚ö°**Performance:**Peut √™tre plus efficace que la virtualisation compl√®te traditionnelle car elle r√©duit les frais g√©n√©raux associ√©s √† l'√©mulation de mat√©riel.
-   üîó**Compatibilit√©:**Limit√© aux syst√®mes d'exploitation qui ont √©t√© modifi√©s pour la paravirtualisation.

###### Exemples de paravirtualisation

Xen avec des invit√©s paravirtualis√©s, des outils VMware dans certaines configurations et certaines configurations KVM.

###### Avantages de paravirtualisation

-   ‚ö°**Efficacit√©:**R√©duit les frais g√©n√©raux du mat√©riel de virtualisation, offrant potentiellement de meilleures performances pour certaines charges de travail.
-   ‚úÖ**Utilisation des ressources:**Utilisation plus efficace des ressources syst√®me en raison de la communication directe entre le syst√®me d'exploitation invit√© et l'hyperviseur.

###### Inconv√©nients de paravirtualisation

-   üõ†Ô∏è**Modification du syst√®me d'exploitation invit√©:**N√©cessite des modifications au syst√®me d'exploitation invit√©, limitant la compatibilit√© aux syst√®mes d'exploitation pris en charge.
-   üîß**Complexit√©:**N√©cessite une complexit√© suppl√©mentaire dans le syst√®me d'exploitation invit√© pour les impl√©mentations hypercall.

##### Diff√©rences cl√©s

###### Exigences du syst√®me d'exploitation invit√©

-   **HVM:**Peut ex√©cuter des syst√®mes d'exploitation invit√©s non modifi√©s.
-   **Paravirtualisation:**Exige que les syst√®mes d'exploitation invit√©s soient modifi√©s pour travailler avec l'hyperviseur.

###### Performance

-   **HVM:**Offre g√©n√©ralement des performances presque natives en raison de l'ex√©cution assist√©e par le mat√©riel.
-   **Paravirtualisation:**Peut offrir des performances efficaces en r√©duisant les frais g√©n√©raux de l'√©mulation mat√©rielle, mais repose sur un syst√®me d'exploitation invit√© modifi√©.

###### D√©pendance mat√©rielle

-   **HVM:**N√©cessite des fonctionnalit√©s CPU sp√©cifiques (Intel VT-X, AMD-V).
-   **Paravirtualisation:**Ne n√©cessite pas de fonctionnalit√©s CPU sp√©cifiques mais n√©cessite un syst√®me d'exploitation invit√© modifi√©.

###### Isolement

-   **HVM:**Fournit une forte isolement √† l'aide de fonctionnalit√©s mat√©rielles.
-   **Paravirtualisation:**S'appuie sur l'isolement logiciel, qui peut ne pas √™tre aussi robuste que l'isolement mat√©riel.

###### Complexit√©

-   **HVM:**G√©n√©ralement plus simple √† d√©ployer car il prend en charge le syst√®me d'exploitation non modifi√©.
-   **Paravirtualisation:**N√©cessite une configuration et des modifications suppl√©mentaires pour le syst√®me d'exploitation invit√©, augmentant la complexit√©.

#### NUMA (acc√®s √† la m√©moire non uniforme)

NUMA (Acc√®s √† la m√©moire non uniforme) est une architecture de m√©moire utilis√©e dans les syst√®mes multiprocesseurs pour optimiser l'acc√®s √† la m√©moire par les processeurs.  
Dans un syst√®me NUMA, la m√©moire est distribu√©e de mani√®re in√©gale entre les processeurs, ce qui signifie que chaque processeur a un acc√®s plus rapide √† une partie de la m√©moire (sa "m√©moire locale") qu'√† la m√©moire qui est physiquement plus loin (appel√©e "m√©moire distante") et associ√©e √† d'autres processeurs.

##### Caract√©ristiques cl√©s de l'architecture NUMA

1.  **M√©moire locale et distante**: Chaque processeur a sa propre m√©moire locale, √† laquelle elle peut acc√©der plus rapidement. Cependant, il peut √©galement acc√©der √† la m√©moire d'autres processeurs, bien que cela prenne plus de temps.
2.  **Latence diff√©renci√©e**: La latence de l'acc√®s √† la m√©moire varie selon que le processeur acc√©de √† sa m√©moire locale ou √† la m√©moire d'un autre n≈ìud. L'acc√®s √† la m√©moire locale est plus rapide, tandis que l'acc√®s √† la m√©moire d'un autre n≈ìud (t√©l√©commande) est plus lent.
3.  **√âvolutivit√©**: L'architecture NUMA est con√ßue pour am√©liorer l'√©volutivit√© des syst√®mes avec de nombreux processeurs. √Ä mesure que davantage de processeurs sont ajout√©s, la m√©moire est √©galement distribu√©e, en √©vitant le goulot d'√©tranglement qui se produirait dans une architecture d'acc√®s √† la m√©moire uniforme (UMA).

##### Avantages de Numa

-   ‚ö° Meilleures performances dans les grands syst√®mes: √âtant donn√© que chaque processeur a une m√©moire locale, il peut fonctionner plus efficacement sans concurrencer autant avec d'autres processeurs pour l'acc√®s √† la m√©moire.
-   üìà √âvolutivit√©: NUMA permet des syst√®mes avec de nombreux processeurs et de grandes quantit√©s de m√©moire pour √©voluer plus efficacement par rapport √† une architecture UMA.

##### D√©savantage

-   üõ†Ô∏è Complexit√© de programmation: les programmeurs doivent √™tre conscients quelles r√©gions de m√©moire sont locales ou √©loign√©es, optimisant l'utilisation de la m√©moire locale pour obtenir de meilleures performances.
-   üê¢ P√©nances de performances potentielles: Si un processeur acc√®de fr√©quemment √† la m√©moire distante, les performances peuvent souffrir en raison d'une latence plus √©lev√©e.
    Cette architecture est courante dans les syst√®mes multiprocesseurs haute performance, tels que les serveurs et les superordinateurs, o√π l'√©volutivit√© et l'optimisation de la m√©moire sont essentielles.

#### Opensource Solutions

-   üåê Ovirt:<https://www.ovirt.org/>

-   üåê Proxmox:<https://www.proxmox.com/en/proxmox-virtual-environment/overview>

-   üåê Oracle VirtualBox:<https://www.virtualbox.org/>

-   üåê Open VSwitch:<https://www.openvswitch.org/>

#### Types de virtualisation

##### Virtualisation mat√©rielle (virtualisation du serveur)

###### D√©finition HV

R√©sum√© mat√©riel physique pour cr√©er des machines virtuelles (VM) qui ex√©cutent des syst√®mes d'exploitation et des applications distincts.

###### Cas d'utilisation HV

Centres de donn√©es, cloud computing, consolidation de serveurs.

###### Exemples HV

VMware Esxi, Microsoft Hyper-V, KVM.

##### Virtualisation du syst√®me d'exploitation (conteneurisation)

###### D√©finition de conteneurisation

Permet √† plusieurs instances d'espace utilisateur isol√©es (conteneurs) d'ex√©cuter sur un seul noyau de syst√®me d'exploitation.

###### Cas d'utilisation de la conteneurisation

Environnements d'architecture, de d√©veloppement et de test des microservices.

###### Exemples de conteneurisation

Docker, Kubernetes, LXC.

##### Virtualisation du r√©seau

###### D√©finition de virtualisation du r√©seau

Combine les ressources mat√©rielles et r√©seau logicielles dans une seule entit√© administrative bas√©e sur un logiciel.

###### Cas d'utilisation de la virtualisation du r√©seau

R√©seaux d√©finis par logiciel (SDN), virtualisation des fonctions du r√©seau (NFV).

###### Exemples de virtualisation du r√©seau

VMware NSX, Cisco ACI, OpenStack Neutron.

##### Virtualisation de stockage

###### D√©finition de virtualisation de stockage

Poolets Stockage physique de plusieurs appareils dans une seule unit√© de stockage virtuel qui peut √™tre g√©r√© de mani√®re centralis√©e.

###### D√©finition de virtualisation de stockage Cas d'utilisation

Gestion des donn√©es, optimisation du stockage, reprise apr√®s sinistre.

###### Exemples de d√©finition de virtualisation de stockage

Contr√¥leur de volume IBM SAN, VMware VSAN, NetApp ONTAP.

##### Virtualisation de bureau

###### D√©finition de virtualisation de bureau

Permet √† un syst√®me d'exploitation de bureau d'ex√©cuter une machine virtuelle h√©berg√©e sur un serveur.

###### Cas d'utilisation de d√©finition de virtualisation de bureau

Infrastructure de bureau virtuelle (VDI), solutions de travail √† distance.

###### Exemples de d√©finition de virtualisation de bureau

Citrix Virtual Apps and Desktops, VMware Horizon, Microsoft Remote Desktop Services.

##### Virtualisation de l'application

###### D√©finition de virtualisation de l'application

S√©pare les applications du mat√©riel et du syst√®me d'exploitation sous-jacents, leur permettant d'ex√©cuter dans des environnements isol√©s.

###### Application Virtualization D√©finition des cas d'utilisation

D√©ploiement simplifi√© des applications, tests de compatibilit√©.

###### Exemples de d√©finition de virtualisation de l'application

VMware Thinapp, Microsoft App-V, Citrix XenApp.

##### Virtualisation des donn√©es

###### D√©finition de virtualisation des donn√©es

Int√®gre les donn√©es de diverses sources sans la consolider physiquement, fournissant une vue unifi√©e pour l'analyse et les rapports.

###### Data Virtualization D√©finition des cas d'utilisation

Business Intelligence, int√©gration de donn√©es en temps r√©el.

###### Exemples de d√©finition de virtualisation des donn√©es

D√âNODO, Red Hat JBoss Data Virtualization, IBM Infosphere.

##### Avantages de la virtualisation

-   ‚ö° Efficacit√© des ressources: meilleure utilisation des ressources physiques.
-   üí∞ √âconomies de co√ªts: r√©duction des co√ªts mat√©riels et op√©rationnels.
-   üìà √âvolutivit√©: facile √† √©voluer ou √† la baisse selon la demande.
-   üîß Flexibilit√©: prend en charge une vari√©t√© de charges de travail et d'applications.
-   üîÑ R√©cup√©ration en cas de catastrophe: processus de sauvegarde et de r√©cup√©ration simplifi√©s.
-   üîí Isolement: une s√©curit√© am√©lior√©e par l'isolement des environnements.

#### √âmulation

L'√©mulation implique la simulation du comportement du mat√©riel ou des logiciels sur une plate-forme diff√©rente de celle initialement pr√©vue.

Ce processus permet aux logiciels con√ßus pour un syst√®me d'ex√©cuter sur un autre syst√®me qui peut avoir une architecture ou un environnement de fonctionnement diff√©rent.

Bien que l'√©mulation offre une polyvalence en permettant l'ex√©cution de syst√®mes ou d'applications d'exploitation invit√©s non modifi√©s, il est souvent livr√© avec des frais g√©n√©raux de performances.

Cette surcharge survient parce que le syst√®me √©mul√© doit interpr√©ter et traduire les instructions destin√©es au syst√®me d'origine en instructions compatibles avec le syst√®me h√¥te. En cons√©quence, l'√©mulation peut √™tre plus lente que l'ex√©cution native, ce qui le rend moins efficace pour les t√¢ches √† forte intensit√© de ressources.

Malgr√© cet inconv√©nient, l'√©mulation reste pr√©cieuse pour ex√©cuter des logiciels h√©rit√©s, tester des applications sur diff√©rentes plates-formes et faciliter le d√©veloppement de la plate-forme multiplate.

#### systemd-machant

Le service SystemD-Maching est d√©di√© √† la gestion des machines virtuelles et des conteneurs au sein de l'√©cosyst√®me SystemD.
 Il fournit des fonctionnalit√©s essentielles pour contr√¥ler, surveiller et maintenir des instances virtuelles, offrant une int√©gration et une efficacit√© robustes dans les environnements Linux.

<p align="right">(<a href="#topic-351.1">back to sub Topic 351.1</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351.2"></a>

### 351.2 alternant

![xen-architecture](images/xen-achitecture.png)

![xen-architecture](images/xen-achitecture2.png)

**Poids:**3

**Description:**Les candidats doivent pouvoir installer, configurer, maintenir, migrer et d√©panner les installations Xen. L'accent est mis sur Xen version 4.x.

**Zones de connaissances cl√©s:**

-   Comprendre l'architecture de Xen, y compris le r√©seautage et le stockage
-   Configuration de base des n≈ìuds et domaines Xen
-   Gestion de base des n≈ìuds et domaines Xen
-   D√©pannage de base des installations Xen
-   Avarines hors de la pilule
-   Sensibilisation de Xenstore
-   Conscience des param√®tres de d√©marrage Xen
-   Conscience de l'utilitaire XM

#### Alterner

![panda](images/xen-panda.png)

Xen est un hyperviseur open source de type 1 (bare-metal), qui permet √† plusieurs syst√®mes d'exploitation d'ex√©cuter simultan√©ment le m√™me mat√©riel physique.  
Xen fournit une couche entre le mat√©riel physique et les machines virtuelles (VM), permettant un partage et un isolement efficaces des ressources.

-   **Architecture:**Xen fonctionne avec un syst√®me √† deux niveaux o√π le domaine 0 (DOM0) est le domaine privil√©gi√© avec un acc√®s mat√©riel direct et g√®re l'hyperviseur. D'autres machines virtuelles, appel√©es Domain U (DOMU), ex√©cutent des syst√®mes d'exploitation invit√©s et sont g√©r√©es par DOM0.
-   **Types de virtualisation:**Xen prend en charge √† la fois la paravirtualisation (PV), qui n√©cessite un syst√®me d'exploitation invit√© modifi√©, et la virtualisation assist√©e par le mat√©riel (HVM), qui utilise des extensions mat√©rielles (par exemple, Intel VT-X ou AMD-V) pour ex√©cuter des syst√®mes d'exploitation invit√©s non modifi√©s.
    Xen est largement utilis√© dans les environnements cloud, notamment par Amazon Web Services (AWS) et d'autres fournisseurs de cloud √† grande √©chelle.

#### Xensource

Xensource √©tait la soci√©t√© fond√©e par les d√©veloppeurs originaux de l'hyperviseur Xen √† l'Universit√© de Cambridge pour commercialiser Xen.  
La soci√©t√© a fourni des solutions d'entreprise bas√©es sur Xen et a offert des outils et un support suppl√©mentaires pour am√©liorer les capacit√©s de Xen √† usage d'entreprise.

-   **Acquisition par Citrix**: En 2007, Xensource a √©t√© acquis par Citrix Systems, Inc. Citrix a utilis√© la technologie Xen comme base de son produit Citrix Xenserver, qui est devenu une plate-forme de virtualisation populaire de qualit√© d'entreprise bas√©e sur Xen.
-   **Transition**: Apr√®s l'acquisition, le projet Xen s'est poursuivi en tant que projet open source, tandis que Citrix s'est concentr√© sur des offres commerciales comme Xenserver, en tirant parti de la technologie XenSource.

#### Projet Xen

Le projet Xen fait r√©f√©rence √† la communaut√© open source et √† l'initiative responsable du d√©veloppement et du maintien de l'hyperviseur Xen apr√®s sa commercialisation.  
Le projet Xen fonctionne sous la Fondation Linux, en mettant l'accent sur la construction, l'am√©lioration et le soutien de Xen comme un effort collaboratif et ax√© sur la communaut√©.

-   **Objectifs:**Le projet Xen vise √† faire progresser l'hyperviseur en am√©liorant ses performances, sa s√©curit√© et son ensemble de fonctionnalit√©s pour une large gamme de cas d'utilisation, notamment le cloud computing, la virtualisation ax√©e sur la s√©curit√© (par exemple, Qubes OS) et les syst√®mes int√©gr√©s.
-   **Contributeurs:**Le projet comprend des contributeurs de diverses organisations, notamment les principaux fournisseurs de cloud, les fournisseurs de mat√©riel et les d√©veloppeurs ind√©pendants.
-   **Pilule et h√©dools:**Le projet Xen comprend √©galement des outils tels que XAPI (Xenapi), qui est utilis√© pour g√©rer les installations d'hyperviseur Xen, et divers autres utilitaires pour la gestion et l'optimisation du syst√®me.

#### Xenstore

Xen Store est un √©l√©ment essentiel de l'hyperviseur Xen.  
Essentiellement, Xen Store est une base de donn√©es de valeur cl√© distribu√©e utilis√©e pour la communication et le partage d'informations entre l'hyperviseur Xen et les machines virtuelles (√©galement appel√©es domaines).

Voici quelques aspects cl√©s de Xen Store:

-   **Communication inter-domaine:**Xen Store permet la communication entre les domaines, tels que DOM0 (le domaine privil√©gi√© qui contr√¥le les ressources mat√©rielles) et Domus (domaines utilisateur, qui sont les VM). Cela se fait via des entr√©es de valeur cl√©, o√π chaque domaine peut lire ou √©crire des informations.

-   **Gestion de la configuration:**Il est utilis√© pour stocker et acc√©der aux informations de configuration, telles que les p√©riph√©riques virtuels, les r√©seaux et les param√®tres de d√©marrage. Cela facilite la gestion et la configuration dynamiques des machines virtuelles.

-   **√âv√©nements et notifications:**Xen Store prend √©galement en charge les notifications d'√©v√©nements. Lorsqu'une cl√© ou valeur particuli√®re dans le magasin Xen est modifi√©e, les domaines int√©ress√©s peuvent √™tre inform√©s pour r√©agir √† ces modifications. Ceci est utile pour surveiller et g√©rer les ressources.

-   API simple: Xen Store fournit une API simple pour la lecture et l'√©criture de donn√©es, ce qui permet aux d√©veloppeurs d'int√©grer facilement leurs applications avec le syst√®me de virtualisation Xen.

#### Pilule

XAPI, ou Xenapi, est l'interface de programmation d'application (API) utilis√©e pour g√©rer l'hyperviseur Xen et ses machines virtuelles (VM).  
XAPI est un composant cl√© de XenServer (maintenant connu sous le nom d'hyperviseur Citrix) et fournit un moyen standardis√© d'interagir avec l'hyperviseur Xen pour effectuer des op√©rations telles que la cr√©ation, la configuration, la surveillance et le contr√¥le des machines virtuelles.

Voici quelques aspects importants de XAPI:

-   **Gestion de la machine virtuelle:**XAPI permet aux administrateurs de cr√©er, de supprimer, de d√©marrer et d'arr√™ter les machines virtuelles.

-   **Automation:**Avec XAPI, il est possible d'automatiser la gestion des ressources virtuelles, y compris la mise en r√©seau, le stockage et l'informatique, ce qui est crucial pour les grands environnements cloud.

-   **Int√©gration:**XAPI peut √™tre int√©gr√© √† d'autres outils et scripts pour fournir une administration plus efficace et personnalis√©e de l'environnement XEN.

-   **Contr√¥le d'acc√®s:**XAPI fournit √©galement des m√©canismes de contr√¥le d'acc√®s pour s'assurer que seuls les utilisateurs autoris√©s peuvent effectuer des op√©rations sp√©cifiques dans l'environnement virtuel.

XAPI est l'interface qui permet le contr√¥le et l'automatisation de l'hyperviseur Xen, ce qui facilite la gestion des environnements virtualis√©s.

#### R√©sum√© Xen

-   **Intercalpring:**La technologie de base de l'hyperviseur permettant aux machines virtuelles de fonctionner sur du mat√©riel physique.
-   **Xensource:**La soci√©t√© qui a commercialis√© Xen, acquise plus tard par Citrix, conduisant au d√©veloppement de Citrix Xenserver.
-   **Projet Xen:**L'initiative et la communaut√© open source qui continue de d√©velopper et de maintenir l'hyperviseur Xen sous la Fondation Linux.
-   **Xenstore:**Xen Store agit comme un interm√©diaire de communication et de configuration entre l'hyperviseur Xen et les machines virtuelles, rationalisant le fonctionnement et la gestion des environnements virtualis√©s.
-   **Pilule**est l'interface qui permet le contr√¥le et l'automatisation de l'hyperviseur Xen, ce qui facilite la gestion des environnements virtualis√©s.

#### Domain0 (Dom0)

Domain0, ou Dom0, est le domaine de contr√¥le dans une architecture XEN. Il g√®re d'autres domaines (DOMUS) et a un acc√®s direct au mat√©riel.  
DOM0 ex√©cute les pilotes de p√©riph√©riques, permettant √† Domus, qui manque d'acc√®s mat√©riel direct, √† communiquer avec les appareils. En r√®gle g√©n√©rale, il s'agit d'une instance compl√®te d'un syst√®me d'exploitation, comme Linux, et est essentiel pour le fonctionnement de l'hyperviseur Xen.

#### Domaine (maison)

Domus sont des domaines non privil√©gi√©s qui ex√©cutent des machines virtuelles.  
Ils sont g√©r√©s par DOM0 et n'ont pas acc√®s direct au mat√©riel. DOMUS peut √™tre configur√© pour ex√©cuter diff√©rents syst√®mes d'exploitation et est utilis√© √† diverses fins, tels que les serveurs d'applications et les environnements de d√©veloppement. Ils comptent sur DOM0 pour l'interaction mat√©rielle.

#### PV-DomU (Paravirtualized DomainU)

PV-DOMUS utilise une technique appel√©e paravirtualisation. Dans ce mod√®le, le syst√®me d'exploitation DOMU est modifi√© pour savoir qu'il s'ex√©cute dans un environnement virtualis√©, lui permettant de communiquer directement avec l'hyperviseur pour des performances optimis√©es.  
Il en r√©sulte des frais g√©n√©raux plus faibles et une meilleure efficacit√© par rapport √† la virtualisation compl√®te.

#### HVM-DomU (Hardware Virtual Machine DomainU)

HVM-DOMUS sont des machines virtuelles qui utilisent une virtualisation compl√®te, permettant aux syst√®mes d'exploitation non modifi√©s de s'ex√©cuter. L'hyperviseur Xen fournit une √©mulation mat√©rielle pour ces domus, leur permettant d'ex√©cuter tout syst√®me d'exploitation qui prend en charge l'architecture mat√©rielle sous-jacente.  
Bien que cela offre une plus grande flexibilit√©, cela peut entra√Æner des frais g√©n√©raux plus √©lev√©s par rapport √† PV-DOMUS.

#### R√©seau Xen

Appareils r√©seau paravirtualis√©s![pv-networking](images/xen-networking2.png)

Pontage![pv-networking](images/xen-networking1.png)

#### 351.2 objets cit√©s

```sh
Domain0 (Dom0), DomainU (DomU)
PV-DomU, HVM-DomU
/etc/xen/
xl
xl.cfg 
xl.conf # Xen global configurations
xentop
oxenstored # Xenstore configurations
```

#### 351.2 Notes

```sh

# Xen Settings
/etc/xen/
/etc/xen/xl.conf - Main general configuration file for Xen
/etc/xen/oxenstored.conf - Xenstore configurations

# VM Configurations
/etc/xen/xlexample.pvlinux
/etc/xen/xlexample.hvm

# Service Configurations
/etc/default/xen
/etc/default/xendomains

# xen-tools configurations
/etc/xen-tools/
/usr/share/xen-tools/

# docs
xl(1)
xl.conf(5)
xlcpupool.cfg(5)
xl-disk-configuration(5)
xl-network-configuration(5)
xen-tscmode(7)

# initialized domains auto
/etc/default/xendomains
   XENDOMAINS_AUTO=/etc/xen/auto

/etc/xen/auto/


# set domain for up after xen reboot
## create folder auto
cd /etc/xen && mkdir -p auto && cd auto

# create simbolic link
ln -s /etc/xen/lpic3-pv-guest /etc/xen/auto/lpic3-pv-guest
```

#### 351.2 Commandes importantes

##### xen-cr√©ation-image

```sh
# create a pv image
xen-create-image \
  --hostname=lpic3-pv-guest \
  --memory=1gb \
  --vcpus=2 \
  --lvm=vg_xen \
  --bridge=xenbr0 \
  --dhcp \
  --pygrub \
  --password=vagrant \
  --dist=bookworm
```

##### Xen-list-iMages

```sh
# list image
xen-list-image
```

##### Xen-Delete-Image

```sh
# delete a pv image
xen-delete-image lpic3-pv-guest --lvm=vg_xen
```

##### Xenstore-LS

```sh
# list xenstore infos
xenstore-ls
```

##### xl

```sh
# view xen information
xl infos

# list Domains
xl list
xl list lpic3-hvm-guest
xl list lpic3-hvm-guest -l

# uptime Domains
xl uptime

# pause Domain
xl pause 2
xl pause lpic3-hvm-guest

# save state Domains
xl -v save lpic3-hvm-guest ~root/image-lpic3-hvm-guest.save

# restore Domain
xl restore /root/image-lpic3-hvm-guest.save

# get Domain name
xl domname 2

# view dmesg information
xl dmesg

# monitoring domain
xl top
xentop
xen top

# Limit mem Dom0
xl mem-set 0 2048

# Limite cpu (not permanent after boot)
xl vcpu-set 0 2

# create DomainU - virtual machine
xl create /etc/xen/lpic3-pv-guest.cfg

# create DomainU virtual machine and connect to guest
xl create -c /etc/xen/lpic3-pv-guest.cfg

##----------------------------------------------
# create DomainU virtual machine HVM

## create logical volume
lvcreate -l +20%FREE -n lpic3-hvm-guest-disk  vg_xen

## create a ssh tunel for vnc
ssh -l vagrant -L 5900:localhost:5900  192.168.0.130

## configure /etc/xen/lpic3-hvm-guest.cfg
## set boot for cdrom: boot = "d"

## create domain hvm
xl create /etc/xen/lpic3-hvm-guest.cfg

## open vcn conection in your vnc client with localhost
## for view install details

## after installation finished, destroy domain: xl destroy <id_or_name>

## set /etc/xen/lpic3-hvm-guest.cfg: boot for hard disc: boot = "c"

## create domain hvm
xl create /etc/xen/lpic3-hvm-guest.cfg

## access domain hvm
xl console <id_or_name>
##----------------------------------------------

# connect in domain guest
xl console <id>|<name> (press enter)
xl console 1
xl console lpic3-pv-guest

#How do I exit domU "xl console" session
#Press ctrl+] or if you're using Putty press ctrl+5.

# Poweroff domain
xl shutdown lpic3-pv-guest

# destroy domain
xl destroy lpic3-pv-guest

# reboot domain
xl reboot lpic3-pv-guest

# list block devices
xl block-list 1
xl block-list lpic3-pv-guest

# detach block devices
xl block-detach lpic3-hvm-guest hdc
xl block-detach 2 xvdc

# attach block devices

## hard disk devices
xl block-attach lpic3-hvm-guest-ubuntu 'phy:/dev/vg_xen/lpic3-hvm-guest-disk2,xvde,w'

## cdrom
xl block-attach lpic3-hvm-guest 'file:/home/vagrant/isos/ubuntu/seed.iso,xvdc:cdrom,r'
xl block-attach 2 'file:/home/vagrant/isos/ubuntu/seed.iso,xvdc:cdrom,r'

# insert and eject cdrom devices
xl cd-insert lpic3-hvm-guest-ubuntu xvdb  /home/vagrant/isos/ubuntu/ubuntu-24.04.1-live-server-amd64.iso
xl cd-eject lpic3-hvm-guest-ubuntu xvdb
```

#### 251.2 Notes

##### vif

Dans Xen, ¬´VIF¬ª signifie l'interface virtuelle et est utilis√©e pour configurer le r√©seau pour les machines virtuelles (domaines).

En sp√©cifiant les directives ¬´VIF¬ª dans les fichiers de configuration du domaine, les administrateurs peuvent d√©finir les interfaces r√©seau, attribuer des adresses IP, configurer des VLAN et configurer d'autres param√®tres de r√©seautage pour les machines virtuelles fonctionnant sur des h√¥tes Xen. Par exemple: vif =[¬´Bridge = xenbr0¬ª], dans ce cas, il connecte l'interface r√©seau de la machine virtuelle au pont Xen nomm√© ¬´Xenbr0¬ª.

````sh

<p align="right">(<a href="#topic-351.2">back to sub Topic 351.2</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

---

<a name="topic-351.3"></a>

### 351.3 QEMU

![xen-kvm-qemu](/images/xen-kvm-qemu.png)

**Weight:** 4

**Description:** Candidates should be able to install, configure, maintain, migrate and troubleshoot QEMU installations.

**Key Knowledge Areas:**

* Understand the architecture of QEMU, including KVM, networking and storage
* Start QEMU instances from the command line
* Manage snapshots using the QEMU monitor
* Install the QEMU Guest Agent and VirtIO device drivers
* Troubleshoot QEMU installations, including networking and storage
* Awareness of important QEMU configuration parameters

#### 351.3 Cited Objects

```sh
Kernel modules: kvm, kvm-intel and kvm-amd
/dev/kvm
QEMU monitor
qemu
qemu-system-x86_64
ip
brctl
tunctl
````

#### 351.3 Commandes importantes

##### 351.3 autres commandes

##### V√©rifiez le module KVM

```sh
# check if kvm is enabled
egrep -o '(vmx|svm)' /proc/cpuinfo
lscpu |grep Virtualization
lsmod|grep kvm
ls -l /dev/kvm
hostnamectl
systemd-detect-virt
```

```sh
# check if kvm is enabled
egrep -o '(vmx|svm)' /proc/cpuinfo
lscpu |grep Virtualization
lsmod|grep kvm
ls -l /dev/kvm

# check kernel infos
uname -a

# check root device
findmnt /

# mount a qcow2 image
## Example 1:
mkdir -p /mnt/qemu
guestmount -a os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 -i /mnt/qemu/

## Example 2:
sudo guestfish --rw -a os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2
run
list-filesystems

# run commands in qcow2 images
## Example 1:
virt-customize -a  os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2  --run-command 'echo hello >/root/hello.txt'
## Example 2:
sudo virt-customize -a os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  --run-command 'echo -e "auto ens3\niface ens3 inet dhcp" > /etc/network/interfaces.d/ens3.cfg'

# generate mac 
printf 'DE:AD:BE:EF:%02X:%02X\n' $((RANDOM%256)) $((RANDOM%256))
```

##### IP

```sh
# list links
ip link show

# create bridge
ip link add br0 type bridge
```

##### BRCTL

```sh
# list links
ip link show

# create bridge
ip link add br0 type bridge
```

##### Qemu-IMG

```sh
# create image
qemu-img create -f qcow2 vm-disk-debian-12.qcow2 20G

# convert vmdk to qcow2 image
qemu-img convert \
  -f vmdk \
  -O qcow2 os-images/Debian_12.0.0_VMM/Debian_12.0.0_VMM_LinuxVMImages.COM.vmdk os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -p \
  -m16

# check image
qemu-img info os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2
```

##### Qemu-system-x86_64

```sh
# create vm with ISO
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm -hda vm-disk-debian-12.qcow2 \
  -cdrom /home/vagrant/isos/debian/debian-12.8.0-amd64-DVD-1.iso  \
  -boot d \
  -m 2048 \
  -smp cpus=2 \
  -k pt-br

# create vm with ISO using vnc in no gui servers \ ssh connections

## create ssh tunel in host
 ssh -l vagrant -L 5902:localhost:5902  192.168.0.131

## create vm 
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -k pt-br \
  -vnc :2 \
  -device qemu-xhci \
  -device usb-tablet \
  -device ide-cd,bus=ide.1,drive=cdrom,bootindex=1 \
  -drive id=cdrom,media=cdrom,if=none,file=/home/vagrant/isos/debian/debian-12.8.0-amd64-DVD-1.iso \
  -hda vm-disk-debian-12.qcow2 \
  -boot order=d \
  -vga std \
  -display none \
  -monitor stdio

# create vm with OS Image - qcow2

## create vm
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -k pt-br \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2

## create vm with custom kernel params
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -kernel /vmlinuz \
  -initrd /initrd.img \
  -append "root=/dev/mapper/debian--vg-root ro fastboot console=ttyS0" \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -k pt-br \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2

## create vm with and attach disk
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -hdb vmdisk-debian12.qcow2 \
  -drive file=vmdisk-extra-debian12.qcow2,index=2,media=disk,if=ide \
  -netdev bridge,id=net0,br=qemubr0 \
  -device virtio-net-pci,netdev=net0
  
## create vm network netdev user
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -netdev user,id=mynet0,net=192.168.0.150/24,dhcpstart=192.168.0.155,hostfwd=tcp::2222-:22 \
  -device virtio-net-pci,netdev=mynet0

## create vm network netdev tap (Private Network)
ip link add br0 type bridge ; ifconfig br0 up
qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -vnc :2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -netdev tap,id=br0 \
  -device e1000,netdev=br0,mac=DE:AD:BE:EF:1A:24

## create vm with public bridge
#create a public bridge : https://www.linux-kvm.org/page/Networking

qemu-system-x86_64 \
  -name lpic3-debian-12 \
  -enable-kvm \
  -m 2048 \
  -smp cpus=2 \
  -hda os-images/Debian_12.0.0_VMM/Debian_12.0.0.qcow2 \
  -k pt-br \
  -vnc :2 \
  -device qemu-xhci \
  -device usb-tablet \
  -vga std \
  -display none \
  -netdev bridge,id=net0,br=qemubr0 \
  -device virtio-net-pci,netdev=net0

## get a ipv4 ip - open ssh in vm and:
dhcpclient ens4
```

#### Moniteur Qemu

Pour initier le moniteur Qemu dans l'utilisation de la ligne de commande**-Monitor Stdio**param**Qemu-system-x86_64**

```sh
qemu-system-x86_64 -monitor stdio
```

Exit Qemu-monitor:

```sh
ctrl+alt+2
```

```sh
# Managment
info status # vm info
info cpus # cpu information
info network # network informations
stop # pause vm
cont # start vm in status pause
system_powerdown # poweroff vm
system_reset # restart monitor


# Blocks
info block # block info
boot_set d # force boot iso
change ide1-cd0  /home/vagrant/isos/debian/debian-12.8.0-amd64-DVD-1.iso  # attach cdrom
eject ide1-cd0 # detach cdrom

# Snapshots
info snapshots # list snapshots
savevm snapshot-01  # create snapshot
loadvm snapshot-01 # restore snapshot
delvm snapshot-01
```

#### Agent invit√©

Pour activer, utiliser:

```sh
qemu-system-x86_x64
 -chardev socket,path=/tmp/qga.sock,server=on,wait=off,id=qga0 \
 -device virtio-serial \
 -device virtserialport,chardev=qga0,name=org.qemu.guest_agent.0
```

<p align="right">(<a href="#topic-351.3">back to sub Topic 351.3</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351.4"></a>

### 351.4 Gestion de la machine virtuelle LibVirt

![libvirt](images/libvirt.png)

![libvirt-network](images/libvirt-default-network.jpg)

**Poids:**9

**Description:**Les candidats devraient √™tre en mesure de g√©rer les h√¥tes de virtualisation et les machines virtuelles (¬´Domains LibVirt¬ª) √† l'aide de libvirt et d'outils connexes.

**Zones de connaissances cl√©s:**

-   Comprendre l'architecture de libvirt
-   G√©rer les connexions et les n≈ìuds libvirt
-   Cr√©er et g√©rer les domaines Qemu et Xen, y compris les instantan√©s
-   G√©rer et analyser la consommation de ressources de domaines
-   Cr√©er et g√©rer les pools de stockage et les volumes
-   Cr√©er et g√©rer les r√©seaux virtuels
-   Migrer les domaines entre les n≈ìuds
-   Comprendre comment Libvirt interagit avec Xen et Qemu
-   Comprendre comment LibVirt interagit avec les services de r√©seau tels que DNSMASQ et RADVD
-   Comprendre les fichiers de configuration XML LibVirt
-   Conscience de Virtlogd et Virtlockd

#### 351.4 objets cit√©s

```sh
libvirtd
/etc/libvirt/
/var/lib/libvirt
/var/log/libvirt
virsh (including relevant subcommands) 
```

#### 351.4 Commandes importantes

##### Vif

```sh
# using env variable for set virsh uri (local or remotly)
export LIBVIRT_DEFAULT_URI=qemu:///system
export LIBVIRT_DEFAULT_URI=xen+ssh://vagrant@192.168.0.130
export LIBVIRT_DEFAULT_URI='xen+ssh://vagrant@192.168.0.130?keyfile=/home/vagrant/.ssh/skynet-key-ecdsa'

# COMMONS

# get helps
virsh help
virsh help pool-create

# view version
virsh version

# view system info
sudo virsh sysinfo

# view node info
virsh nodeinfo

# hostname
virsh hostname

# check vcn allocated port
virsh vncdisplay <domain_id>
virsh vncdisplay <domain_name>
virsh vncdisplay rocky9-server01 

# HYPERVISIONER

# view libvirt hypervisioner connection
virsh uri

# list valid hypervisioners
virt-host-validate
virt-host-validate qemu

# test connetion uri(vm test)
virsh -c test:///default list

# connect remotly
virsh -c xen+ssh://vagrant@192.168.0.130
virsh -c xen+ssh://vagrant@192.168.0.130 list
virsh -c qemu+ssh://vagrant@192.168.0.130/system list

# connect remotly without enter password
virsh -c 'xen+ssh://vagrant@192.168.0.130?keyfile=/home/vagrant/.ssh/skynet-key-ecdsa'

# STORAGE

# list storage pools
virsh pool-list --details

# list all storage pool
virsh pool-list --all --details

# get a pool configuration
virsh pool-dumpxml default

# get pool info
virsh pool-info default

# create a storage pool
virsh pool-define-as --name default --type dir --target /var/lib/libvirt/images

# create a storage pool with dumpxml
virsh pool-create --overwrite --file configs/kvm/libvirt/pool.xml

# start storage pool
virsh pool-start default

# set storage pool for autostart
virsh pool-autostart default

# stop storage pool
virsh pool-destroy linux

# delete xml storage pool file
virsh pool-undefine linux

# edit storage pool
virsh pool-edit linux

# list volumes
virsh vol-list linux

# get volume infos
virsh vol-info Debian_12.0.0.qcow2 os-images
virsh vol-info --pool os-images Debian_12.0.0.qcow2 

# get volume xml
virsh vol-dumpxml rocky9-disk1 default

# create volume
virsh vol-create-as default --format qcow2 disk1 10G

# delete volume
virsh vol-delete  disk1 default

# DOMAINS \ INSTANCES \ VIRTUAL MACHINES

# list domain\instance\vm
virsh list
virsh list --all

# create domain\instance\vm
virsh create configs/kvm/libvirt/rocky9-server03.xml

# view domain\instance\vm info
virsh dominfo rocky9-server01

# view domain\instance\vm xml
virsh dumpxml rocky9-server01

# edit domain\instance\vm xml
virsh edit rocky9-server01

# stop domain\instance\vm
virsh shutdown rocky9-server01 # gracefully
virsh destroy 1
virsh destroy rocky9-server01

# suspend domain\instance\vm
virsh suspend rocky9-server01

# resume domain\instance\vm
virsh resume rocky9-server01

# start domain\instance\vm
virsh start rocky9-server01

# remove domain\instance\vm
virsh undefine rocky9-server01

# remove domain\instance\vm and storage volumes
virsh undefine rocky9-server01 --remove-all-storage

# save domain\instance\vm
virsh save rocky9-server01 rocky9-server01.qcow2

# restore domain\instance\vm
virsh restore rocky9-server01.qcow2

# list snapshots
virsh snapshot-list rocky9-server01

# create snapshot
virsh snapshot-create rocky9-server01

# restore snapshot
virsh snapshot-revert rocky9-server01 1748983520

# view snapshot xml
virsh snapshot-info rocky9-server01 1748983520

# dumpxml snapshot
virsh snapshot-dumpxml rocky9-server01 1748983520

# xml snapshot path
/var/lib/libvirt/qemu/snapshot/rocky9-server01/

# view snapshot info
virsh snapshot-info rocky9-server01 1748983671

# edit snapshot
virsh snapshot-edit rocky9-server01 1748983520

# delete snapshot
virsh snapshot-delete rocky9-server01 1748983520

# DEVICES

# list block devices
virsh domblklist rocky9-server01 --details

# add cdrom media 
virsh change-media rocky9-server01 sda /home/vagrant/isos/rocky/Rocky-9.5-x86_64-minimal.iso
virsh attach-disk rocky9-server01 /home/vagrant/isos/rocky/Rocky-9.5-x86_64-minimal.iso sda --type cdrom --mode readonly

# remove cdrom media
virsh change-media rocky9-server01 sda --eject

# add new disk
virsh attach-disk rocky9-server01  /var/lib/libvirt/images/rocky9-disk2  vdb --persistent

# remove disk
virsh detach-disk rocky9-server01 vdb --persistent

# RESOURCES (CPU and Memory)

# get cpu infos
virsh vcpuinfo rocky9-server01 --pretty
virsh dominfo rocky9-server01 | grep 'CPU'

# get vcpu count
virsh vcpucount rocky9-server01

# set vcpus maximum config
virsh setvcpus rocky9-server01 --count 4 --maximum --config
virsh shutdown rocky9-server01
virsh start rocky9-server01

# set vcpu current config
virsh setvcpus rocky9-server01 --count 4 --config

# set vcpu current live
virsh setvcpus rocky9-server01 --count 3 --current
virsh setvcpus rocky9-server01 --count 3 --live

# configure vcpu afinity config
virsh vcpupin rocky9-server01 0 7 --config
virsh vcpupin rocky9-server01 1 5-6 --config

# configure vcpu afinity current
virsh vcpupin rocky9-server01 0 7
virsh vcpupin rocky9-server01 1 5-6

# set maximum memory config
virsh setmaxmem rocky9-server01 3000000 --config
virsh shutdown rocky9-server01
virsh start rocky9-server01

# set current memory config
virsh setmem rocky9-server01 2500000 --current

# NETWORK

# get netwwork bridges
brctl show

# get iptables rules for libvirt
sudo iptables -L -n -t  nat

# list network
virsh net-list --all

# set default network
virsh net-define /etc/libvirt/qemu/networks/default.xml

# get network infos
virsh net-info default

# get xml network
virsh net-dumpxml default

# xml file
cat /etc/libvirt/qemu/networks/default.xml

# dhcp config
sudo cat /etc/libvirt/qemu/networks/default.xml | grep -A 10 dhcp
sudo cat /var/lib/libvirt/dnsmasq/default.conf

# get domain ipp address
virsh net-dhcp-leases default
virsh net-dhcp-leases default --mac 52\:54\:00\:89\:19\:86

# edit network
virsh net-edit default

# get domain network detais
virsh domiflist debian-server01

# path for network filter files
/etc/libvirt/nwfilter/

# list network filters
virsh nwfilter-list

# create network filter - block icmp traffic
virsh nwfilter-define block-icmp.xml
# virsh edit Debian-Server
    #  <interface type='network'>
    #        ...
    #        <filterref filter='block-icmp'/>
    #        ...
    # </interface>
# virsh destroy debian-server01
# virsh start debian-server01

# delete network filter
virsh nwfilter-undefine block-icmp

# get xml network filter
virsh nwfilter-dumpxml block-icmp
```

###### verrouillage

```sh
# list os variants
virt-install --os-variant list
osinfo-query os

# create domain\instance\vm with iso file
virsh vol-create-as default --format qcow2 rocky9-disk1 20G
virt-install --name rocky9-server01 \
--vcpus 2 \
--cpu host \
--memory 2048 \
--disk vol=default/rocky9-disk1 \
--cdrom /home/vagrant/isos/rocky/Rocky-9.5-x86_64-minimal.iso \
--os-variant=rocky9 \
--graphics vnc,listen=0.0.0.0,port=5905

# create debian domain\instance\vm with qcow2 file
virt-install --name debian-server01 \
--vcpus 2 \
--ram 2048 \
--disk vol=os-images/Debian_12.0.0.qcow2 \
--import \
--osinfo detect=on \
--graphics vnc,listen=0.0.0.0,port=5906 \
--network network=default \
--noautoconsole

# create rocky9 domain\instance\vm with qcow2 file
virt-install --name rocky9-server02 \
--vcpus 2 \
--ram 2048 \
--disk path=os-images/RockyLinux_9.4_VMG/RockyLinux_9.4.qcow2,format=qcow2,bus=virtio \
--import \
--osinfo detect=on \
--graphics vnc,listen=0.0.0.0,port=5907 \
--network bridge=qemubr0,model=virtio \
--noautoconsole

# open domain\instance\vm gui console
virt-viewer debian-server01

# check metadata domain\instance\vm file (if uri is qemu:////system)
less /etc/libvirt/qemu/debian-server01.xml
```

<p align="right">(<a href="#topic-351.4">back to sub Topic 351.4</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-351.5"></a>

### 351.5 Gestion de l'image du disque de machine virtuelle

![disk-managment](images/virtual-machine-disk.png)

**Poids:**3

**Description:**Les candidats devraient √™tre en mesure de g√©rer des images de disque de machines virtuelles. Cela comprend la conversion d'images de disque entre diff√©rents formats et hyperviseurs et acc√©der aux donn√©es stock√©es dans une image.

**Zones de connaissances cl√©s:**

-   Comprendre les fonctionnalit√©s de divers formats d'image disque virtuels, tels que les images brutes, QCOW2 et VMDK
-   G√©rer les images de disque de machine virtuelle √† l'aide de Qemu-IMG
-   Les partitions de montage et les fichiers d'acc√®s contenus dans les images de disque de machine virtuelle √† l'aide de libguestfish
-   Copier le contenu du disque physique sur une image de disque de machine virtuelle
-   Migrer le contenu du disque entre divers formats d'image de disque de machine virtuelle
-   Conscience du format de virtualisation ouverte (OVF)

#### 351,5 objets cit√©s

```sh
qemu-img
guestfish (including relevant subcommands)
guestmount
guestumount
virt-cat
virt-copy-in
virt-copy-out
virt-diff
virt-inspector
virt-filesystems
virt-rescue
virt-df
virt-sparsify
virt-p2v
virt-p2v-make-disk
virt-v2v
```

#### 351,5 Commandes importantes

##### 351.5.1 QemU-IMG

```sh
# Display detailed information about a disk image
qemu-img info UbuntuServer_24.04.qcow2

# Create a new 22G raw disk image (default format is raw)
qemu-img create new-disk 22G

# Create a new 22G disk image in qcow2 format
qemu-img create -f qcow2 new-disk2 22G

# Convert a VDI image to raw format using 5 threads and show progress
qemu-img convert -f vdi -O raw Ubuntu-Server.vdk new-Ubuntu.raw -m5 -p

# Convert vmdk to qcow2 image
qemu-img convert \
-f vmdk \
-O qcow2 os-images/UbuntuServer_24.04_VM/UbuntuServer_24.04_VM_LinuxVMImages.COM.vmdk \
os-images/UbuntuServer_24.04_VM/UbuntuServer_24.04.qcow2 \
-p \
-m16

# Resize a raw image to 30G
qemu-img resize -f raw new-disk 30G

# Resize a qcow2 image to 15G(actual size 30Gdisk 30G)
qemu-img resize -f raw --shrink new-disk 15G

# Snapshots

# List all snapshots in the image
qemu-img snapshot -l new-disk2.qcow2

# Create a snapshot named SNAP1
qemu-img snapshot -c SNAP1 disk

# Apply a snapshot by ID or name
qemu-img snapshot -a 123456789 disk

# Delete the snapshot named SNAP1
qemu-img snapshot -d SNAP1 disk
```

##### poisson-client

```sh
# set enviroment variables for guestfish
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg

# Launch guestfish with a disk image
guestfish -a UbuntuServer_24.04.qcow2
#run
#list-partitions

# Run the commands in a script file
guestfish -a UbuntuServer_24.04.qcow2 -m /dev/sda -i < script.ssh

# Interactively run commands
guestfish --rw -a UbuntuServer_24.04.qcow2 <<'EOF'
run
list-filesystems
EOF

# Copy a file from the guest image to the host
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg
sudo guestfish --rw -a UbuntuServer_24.04.qcow2 -i <<'EOF'
copy-out /etc/hostname /tmp/
EOF

# Copy a file from the host into the guest image
echo "new-hostname" > /tmp/hostname
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg
sudo guestfish --rw -a UbuntuServer_24.04.qcow2 -i <<'EOF'
copy-in /tmp/hostname /etc/
EOF

# View contents of a file in the guest image
guestfish --ro -a UbuntuServer_24.04.qcow2 -i <<'EOF'
cat /etc/hostname
EOF

# List files in the guest image
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg
guestfish --rw -a UbuntuServer_24.04.qcow2 -i <<'EOF'
ls /home/ubuntu
EOF

# Edit a file in the guest image
export LIBGUESTFS_BACKEND_SETTINGS=force_tcg
guestfish --rw -a UbuntuServer_24.04.qcow2 -i <<'EOF'
edit /etc/hosts
EOF
```

###### calendrier

```sh
# Mount a disk image to a directory
guestmount -a UbuntuServer_24.04.qcow2 -m /dev/ubuntu-vg/ubuntu-lv /mnt/ubuntu
# domain
guestmount -d rocky9-server02 -m /dev/ubuntu-vg/ubuntu-lv /mnt/ubuntu 

# Mount a specific partition from a disk image
guestmount -a UbuntuServer_24.04.qcow2 -m /dev/sda2 /mnt/ubuntu
# domain
guestmount -d debian-server01 --ro -m  /dev/debian-vg/root /mnt/debian
```

###### invit√©

```sh
# Umount a disk image to a directory
sudo guestunmount /mnt/ubuntu
```

##### vir-DF

```sh
# Show free and used space on virtual machine filesystems
virt-df UbuntuServer_24.04.qcow2 -h
virt-df -d rocky9-server02 -h
```

##### Virt-FileSystems

```sh
# List filesystems, partitions, and logical volumes in a VM disk image (disk image)
virt-filesystems -a UbuntuServer_24.04.qcow2 --all --long -h

# List filesystems, partitions, and logical volumes in a VM disk image (domain)
virt-filesystems -d debian-server01 --all --long -h
```

##### vrac-inspecteur

```sh
# Inspect and report on the operating system in a VM disk image
virt-inspector -a UbuntuServer_24.04.qcow2 #(disk)
virt-inspector -d debian-server01 #(domain) 
```

##### vir-cat

```sh
# Display the contents of a file inside a VM disk image
virt-cat -a UbuntuServer_24.04.qcow2 /etc/hosts
virt-cat -d debian-server01 /etc/hosts #(domain)
```

##### vrombier

```sh
# Show differences between two VM disk images
virt-diff -a UbuntuServer_24.04.qcow2 -A Rocky-Linux.qcow2
```

##### verrouiller

```sh
# Make a VM disk image smaller by removing unused space
virt-sparsify UbuntuServer_24.04.qcow2 UbuntuServer_24.04-sparse.qcow2
```

##### verti-resize

```sh
# Resize a VM disk image or its partitions
virt-filesystems -a UbuntuServer_24.04.qcow2 --all --long -h #(check size of partitions)
qemu-img create -f qcow2 UbuntuServer_24.04-expanded.qcow2 100G #(create new disk image with 100G)
virt-resize --expand /dev/ubuntu-vg/ubuntu-lv \
UbuntuServer_24.04.qcow2 UbuntuServer_24.04-expanded.qcow2

```

##### verdat-copy-in

```sh
# Copy files from the host into a VM disk image

virt-copy-in -a UbuntuServer_24.04.qcow2 ~vagrant/test-virt-copy-in.txt /home/ubuntu
```

##### verrure-copie

```sh
# Copy files from a VM disk image to the host
virt-copy-out -a UbuntuServer_24.04.qcow2 /home/ubuntu/.bashrc /tmp
```

##### verrouillage

```sh
# List files and directories inside a VM disk image
virt-ls -a UbuntuServer_24.04.qcow2 /home/ubuntu
```

##### verrure

```sh
# Launch a rescue shell on a VM disk image for recovery
virt-rescue -a UbuntuServer_24.04.qcow2
```

##### Verrouillage

```sh
# Prepare a VM disk image for cloning by removing system-specific data
virt-sysprep -a UbuntuServer_24.04.qcow2
```

##### Virt-V2V

```sh
# Convert a VM from a foreign hypervisor to run on KVM
virt-v2v -i disk input-disk.img -o local -os /var/tmp
```

##### Virt-P2v

```sh
# Convert a physical machine to use KVM
```

##### Virt-P2v-Make-Disk

```sh
# Create a bootable disk image for physical to virtual conversion
sudo virt-p2v-make-disk -o output.img
```

#### 351,5 notes

##### OVF: format de virtualisation ouvrir

OVF: un format ouvert qui d√©finit une norme pour l'emballage et la distribution de machines virtuelles dans diff√©rents environnements.

Le package g√©n√©r√© a l'extension .ova et contient les fichiers suivants:

-   .ovf: fichier XML avec des m√©tadonn√©es d√©finissant l'environnement de la machine virtuelle
-   Fichiers d'image: .vmdk, .vhd, .vhdx, .qcow2, .raw
-   Fichiers suppl√©mentaires: m√©tadonn√©es, instantan√©s, configuration, hachage

<p align="right">(<a href="#topic-351.5">back to sub Topic 351.5</a>)</p>
<p align="right">(<a href="#topic-351">back to Topic 351</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-352"></a>

## Sujet 352: Virtualisation des conteneurs

* * *

<a name="topic-352.1"></a>

### 352.1 Concepts de virtualisation des conteneurs

![virtualization-container](images/virtualization-container.png)

```mermaid
timeline
    title Time Line Containers Evolution
    1979 : chroot
    2000 : FreeBSD Jails
    2002 : Linux Namespaces
    2005 : Solaris Containers
    2007 : cgroups
    2008 : LXC
    2013 : Docker
    2015 : Kubernetes
```

* * *

**Poids:**7

**Description:**Les candidats doivent comprendre le concept de virtualisation des conteneurs. Cela comprend la compr√©hension des composants Linux utilis√©s pour impl√©menter la virtualisation des conteneurs ainsi que l'utilisation d'outils Linux standard pour d√©panner ces composants.

**Zones de connaissances cl√©s:**

-   Comprendre les concepts du syst√®me et du conteneur d'application
-   Comprendre et analyser les espaces de noms du noyau
-   Comprendre et analyser les groupes de contr√¥le
-   Comprendre et analyser les capacit√©s
-   Comprendre le r√¥le de SecComp, Selinux et Apparmor pour la virtualisation des conteneurs
-   Comprenez comment LXC et Docker levier des espaces de noms, les CGROUP, les capacit√©s, SecComp et Mac
-   Comprendre le principe de Runc
-   Comprendre le principe de Cri-O et de Containerd
-   Conscience des sp√©cifications de l'ex√©cution et de l'image OCI
-   Sensibilisation de l'interface d'ex√©cution des conteneurs de Kubernetes (CRI)
-   Conscience de Podman, Buildah et Scopeo
-   Conscience des autres approches de virtualisation des conteneurs dans Linux et d'autres syst√®mes d'exploitation gratuits, tels que RKT, OpenVZ, Systemd-Nspawn ou BSD

* * *

#### 352.1 objets cit√©s

```sh
nsenter
unshare
ip (including relevant subcommands)
capsh
/sys/fs/cgroups
/proc/[0-9]+/ns
/proc/[0-9]+/status
```

* * *

#### üß† Comprendre les conteneurs

![container](images/containers1.png)

Les conteneurs sont une technologie de virtualisation l√©g√®re qui emballe les applications ainsi que leurs d√©pendances requises - code, biblioth√®ques, variables d'environnement et fichiers de configuration - en unit√©s isol√©es, portables et reproductibles.

> En termes simples: un conteneur est une bo√Æte autonome qui ex√©cute votre application de la m√™me mani√®re, n'importe o√π.

##### üí° Qu'est-ce qu'un conteneur?

Contrairement aux machines virtuelles (VM), les conteneurs ne virtualisent pas le mat√©riel. Au lieu de cela, ils virtualisent le syst√®me d'exploitation. Les conteneurs partagent le m√™me noyau Linux avec l'h√¥te, mais chacun fonctionne dans un espace utilisateur enti√®rement isol√©.

üìå CONTENSEURS VS Machines virtuelles:

| Fonctionnalit√©            | Conteneurs                                 | Machines virtuelles                                          |
| ------------------------- | ------------------------------------------ | ------------------------------------------------------------ |
| Noyau d'os                | Partag√© avec l'h√¥te                        | Chaque machine virtuelle a son propre syst√®me d'exploitation |
| Heure de d√©marrage        | Rapide (secondes ou moins)                 | Lent (minutes)                                               |
| Taille de l'image         | L√©ger (MBS)                                | Lourd (GBS)                                                  |
| Efficacit√© des ressources | Haut                                       | Inf√©rieur                                                    |
| M√©canisme d'isolement     | Fonctionnalit√©s du noyau (espaces de noms) | Hyperviseur                                                  |

##### üîë Caract√©ristiques cl√©s des conteneurs

üîπ**L√©ger**: Partagez le noyau du syst√®me d'exploitation h√¥te, en r√©duisant les frais g√©n√©raux et en activant le d√©marrage rapide.

üîπ**Portable**: Ex√©cutez de mani√®re coh√©rente dans diff√©rents environnements (Dev, Staging, Prod, Cloud, sur-Prem).

üîπ**Isol√©**: Utilisez des espaces de noms pour l'isolement de processus, de r√©seau et de syst√®me de fichiers.

üîπ**Efficace**: Activer une densit√© plus √©lev√©e et une meilleure utilisation des ressources que les machines virtuelles traditionnelles.

üîπ**√âvolutif**: Ajustement parfait pour les microservices et l'architecture native du cloud.

##### üß± Types de conteneurs

1.  Conteneurs syst√®me
    -   Con√ßu pour ex√©cuter l'int√©gralit√© du syst√®me d'exploitation, ressembler √† des machines virtuelles.
    -   Prise en charge de plusieurs processus et services syst√®me (init, syslog).
    -   Id√©al pour les applications h√©rit√©es ou monolithiques.
    -   Exemple: LXC, libvirt-lxc.

2.  Conteneurs de demande
    -   Con√ßu pour ex√©cuter un seul processus.
    -   Apatride, √©ph√©m√®re et √©volutif horizontalement.
    -   Utilis√© largement dans les environnements DevOps et Kubernetes modernes.
    -   Exemple: Docker, Containerd, Cri-O.

##### üöÄ Runtime des conteneurs populaires

| Temps d'ex√©cution | Description                                                                    |
| ----------------- | ------------------------------------------------------------------------------ |
| **Docker**        | CLI / d√©mon le plus largement adopt√© pour la construction et la course √† pied. |
| **conteneur**     | Docker de runtime l√©ger et Kubernetes.                                         |
| **Critiquer**     | Kubernetes Native Runtime pour les conteneurs OCI.                             |
| **LXC**           | Conteneurs de syst√®me Linux traditionnels, plus proche de la Fond OS.          |
| **Rkt**           | Ex√©cution ax√©e sur la s√©curit√© (obsol√®te).                                     |

##### üîê Les internes et les √©l√©ments de s√©curit√© des conteneurs

| Composant              | R√¥le                                                                  |
| ---------------------- | --------------------------------------------------------------------- |
| **Espaces de noms**    | Isoler les processus, les utilisateurs, les montures, les r√©seaux.    |
| **troupes**            | Contr√¥ler et limiter l'utilisation des ressources (CPU, m√©moire, IO). |
| **Capacit√©s**          | Contr√¥le des privil√®ges √† grain fin √† l'int√©rieur des conteneurs.     |
| **seccompente**        | Restreint les syst√®mes autoris√©s √† r√©duire la surface d'attaque.      |
| **Apparmor / selinux** | Application obligatoire du contr√¥le d'acc√®s au niveau du noyau.       |

* * *

#### üß† Comprendre le chroot - Modifier le r√©pertoire racine dans Unix / Linux

![chroot](images/chroot.png)

##### Qu'est-ce que le chroot?

Chroot (abr√©viation de Change Root) est un appel et une commande syst√®me sur les syst√®mes d'exploitation de type UNIX qui modifient le r√©pertoire racine apparent (/) pour le processus de fonctionnement actuel et ses enfants. Cela cr√©e un environnement isol√©, commun√©ment appel√© une prison de chroot.

##### üß± But et cas d'utilisation

-   üîí Isoler les demandes de s√©curit√© (emprisonnement).
-   üß™ Cr√©ez des environnements de test sans avoir un impact sur le reste du syst√®me.
-   üõ†Ô∏è R√©cup√©ration du syst√®me (par exemple, d√©marrer dans Livecd et chroot dans le syst√®me install√©).
-   üì¶ Construire des packages de logiciels dans un environnement contr√¥l√©.

##### üìÅ Structure minimale requise

L'environnement de chroot doit avoir ses propres fichiers et structure essentiels:

```sh
/mnt/myenv/
‚îú‚îÄ‚îÄ bin/
‚îÇ   ‚îî‚îÄ‚îÄ bash
‚îú‚îÄ‚îÄ etc/
‚îú‚îÄ‚îÄ lib/
‚îú‚îÄ‚îÄ lib64/
‚îú‚îÄ‚îÄ usr/
‚îú‚îÄ‚îÄ dev/
‚îú‚îÄ‚îÄ proc/
‚îî‚îÄ‚îÄ tmp/
```

Utilisez LDD pour identifier les biblioth√®ques requises:

```sh
ldd /bin/bash
```

##### üö® limitations et consid√©rations de s√©curit√©

-   Le chroot n'est pas une limite de s√©curit√© comme les conteneurs ou les machines virtuelles.
-   Un utilisateur privil√©gi√© (racine) √† l'int√©rieur de la prison peut potentiellement √©clater.
-   Aucune isolation d'espaces de noms de processus, d'appareils ou de ressources au niveau du noyau.

Pour une isolement plus fort, consid√©rez des alternatives comme:

-   Conteneurs Linux (LXC, Docker)
-   Machines virtuelles (KVM, QEMU)
-   Espaces de noms et groupes de noyau

##### üß™ Test de chroot avec debootstrap

```sh
# download debain files
sudo debootstrap stable ~vagrant/debian http://deb.debian.org/debian
sudo chroot ~vagrant/debian bash
```

##### : üß™ Chroot Lab

Utilisez ce script pour le laboratoire:[chroot.sh](scripts/container/chroot.sh)

Sortir:

![chroot-labt](images/chroot-lab.png)

* * *

#### üß† Comprendre les espaces de noms Linux

![linux-namespaces](images/linux-namespaces2.png)

Les espaces de noms sont une fonctionnalit√© de noyau Linux de base qui permette l'isolement au niveau du processus. Ils cr√©ent des ¬´vues¬ª distinctes des ressources syst√®me mondiales - telles que les ID de processus, le r√©seau, les syst√®mes de fichiers et les utilisateurs - afin que chaque groupe de processus pense qu'il s'ex√©cute dans son propre syst√®me.

> En termes simples: les espaces de noms trompent un processus en pensant qu'il poss√®de la machine, m√™me s'il ne fait que le partager.

C'est le fondement de l'isolement des conteneurs.

##### üîç Qu'est-ce que les espaces de noms isolent?

Chaque type d'espace de noms isole une ressource syst√®me sp√©cifique. Ensemble, ils constituent le bac √† sable dans lequel un conteneur op√®re:

| Espace de noms  | Isolats ...                              | Exemple du monde r√©el                                                           |
| --------------- | ---------------------------------------- | ------------------------------------------------------------------------------- |
| **Piquer**      | ID de processus                          | Les processus √† l'int√©rieur d'un conteneur voient un autre espace PID           |
| **Monter**      | Points de montage du syst√®me de fichiers | Chaque conteneur voit son propre syst√®me de fichiers racine                     |
| **R√©seau**      | Pile de r√©seau                           | Les conteneurs ont des IP, des interfaces et des itin√©raires isol√©s             |
| **Uts**         | Nom d'h√¥te et nom de domaine             | Chaque conteneur d√©finit son propre nom d'h√¥te                                  |
| **IPC**         | M√©moire partag√©e et s√©maphores           | Emp√™che la communication interpr√®te entre les conteneurs                        |
| **Utilisateur** | ID utilisateur et groupe                 | Active la fausse racine (UID 0) √† l'int√©rieur du conteneur                      |
| **Cgroup (V2)** | Adh√©sion au groupe t√©moin                | Li√©s aux contr√¥les des ressources comme le processeur et les limites de m√©moire |

##### üß™ Analogie visuelle

![linux-namespaces](images/linux-namespaces.png)

Imaginez un immeuble de bureaux partag√©:

-   Tous les locataires partagent la m√™me fondation (noyau Linux).
-   Chaque entreprise a son propre bureau (espace de noms): diff√©rents serrures, meubles, lignes t√©l√©phoniques et nom de l'entreprise.
-   Pour chaque locataire, cela ressemble √† leur propre b√¢timent.

C'est exactement ainsi que les conteneurs √©prouvent le syst√®me - isol√©, mais efficace.

##### üîß Comment les conteneurs utilisent des espaces de noms

Lorsque vous ex√©cutez un conteneur (par exemple, avec Docker ou Podman), le runtime cr√©e un nouvel ensemble d'espaces de noms:

```bash
docker run -it --rm alpine sh
```

Cette commande donne le processus:

-   Un nouvel espace de noms PID ‚Üí C'est le processus 1 √† l'int√©rieur du conteneur.
-   Un nouvel espace de noms de r√©seau ‚Üí son propre Ethernet virtuel.
-   Un espace de noms de montage ‚Üí Un syst√®me de fichiers racine sp√©cifique au conteneur.
-   Autres espaces de noms en fonction de la configuration (utilisateur, IPC, etc.)

Le r√©sultat: un environnement d'ex√©cution l√©ger et isol√© qui se comporte comme un syst√®me s√©par√©.

##### ‚öôÔ∏è Caract√©ristiques du noyau compl√©mentaire

Les espaces de noms masquent les ressources des conteneurs. Mais pour contr√¥ler combien ils peuvent utiliser et ce qu'ils peuvent faire, nous avons besoin de m√©canismes suppl√©mentaires:

###### üî© cgroups (groupes de contr√¥le)

Les CGRoupes permettent au noyau de limiter, de hi√©rarchiser et de surveiller l'utilisation des ressources entre les groupes de processus.

| Ressource    | Exemples de cas d'utilisation                |
| ------------ | -------------------------------------------- |
| Processeur   | Limiter le temps du processeur par conteneur |
| M√©moire      | Cap RAM usage                                |
| E / S disque | Op√©rations de lecture / √©criture de gaz      |
| R√©seau (V2)  | Restrictions de bande passante               |

üõ°Ô∏è emp√™che le probl√®me du "voisin bruyant" en emp√™chant un conteneur de consommer toutes les ressources syst√®me.

###### üß± Capacit√©s

Linux traditionnel utilise un mod√®le de privil√®ge binaire: Root (UID 0) peut tout faire, tout le monde est limit√©.

| Capacit√©               | Permet ...                                                       |
| ---------------------- | ---------------------------------------------------------------- |
| `CAP_NET_BIND_SERVICE` | Liaison aux ports privil√©gi√©s (par exemple 80, 443)              |
| `CAP_SYS_ADMIN`        | Un puissant fourre-tout pour les t√¢ches d'administration syst√®me |
| `CAP_KILL`             | Envoi de signaux aux processus arbitraires                       |

En abandonnant les capacit√©s inutiles, les conteneurs peuvent fonctionner avec seulement ce dont ils ont besoin - en r√©duisant les risques.

##### üîê M√©canismes de s√©curit√©

Utilis√© en conjonction avec des espaces de noms et des CGROUP pour verrouiller ce qu'un processus conteneuris√© peut faire:

| Fonctionnalit√©  | Description                                                                    |
| --------------- | ------------------------------------------------------------------------------ |
| **seccompente** | Ligne blanche ou bloquer les appels du syst√®me Linux (syst√®me)                 |
| **Apparmor**    | Appliquer les profils de s√©curit√© par application par application              |
| **Selinux**     | Appliquer le contr√¥le d'acc√®s obligatoire avec des politiques syst√®me √©troites |

##### üß† R√©sum√© pour les d√©butants

> ‚úÖ Les espaces de noms isolent ce qu'un conteneur peut voir  
> ‚úÖ Cgroups contr√¥lent ce qu'il peut utiliser  
> ‚úÖ Les capacit√©s et les modules de s√©curit√© d√©finissent ce qu'il peut faire

Ensemble, ces caract√©ristiques du noyau forment l'√©pine dorsale technique de l'isolement des conteneurs - permettant un d√©ploiement d'applications √† haute densit√©, s√©curis√© et efficace sans machines virtuelles compl√®tes.

##### üß™ Espaces de noms de laboratoire

Utilisez ce script pour le laboratoire:[namespace.sh](scripts/container/namespace.sh)

Sortir:

![namespaces](images/namespace-lab.png)

* * *

#### üß© Comprendre les groupes (groupes de contr√¥le)

![cgroups](images/cgroups1.png)

##### üìå D√©finition

Les groupes de contr√¥le (CGROUP) sont une fonctionnalit√© du noyau Linux introduit en 2007 qui vous permettent de limiter, de rendre compte et d'isoler l'utilisation des ressources (CPU, m√©moire, E / S de disque, etc.) de groupes de processus.

Les CGROUP sont fortement utilis√©s par les temps de r√©cipient de bas niveau tels que RUNC et CRUN, et exploit√©s par des moteurs √† conteneurs comme Docker, Podman et LXC pour appliquer les limites des ressources et fournir l'isolement entre les conteneurs.

Les espaces de noms isolent, le contr√¥le des CGROUP.

Les espaces de noms cr√©ent des environnements distincts pour les processus (comme le PID, le r√©seau ou les montures), tandis que CGROUPS limite et surveillez l'utilisation des ressources (CPU, m√©moire, E / S) pour ces processus.

‚öôÔ∏è Capacit√©s cl√©s

| Fonctionnalit√©                | Description                                                                |
| ----------------------------- | -------------------------------------------------------------------------- |
| **Limitation des ressources** | Imposer des limites √† la quantit√© de ressources qu'un groupe peut utiliser |
| **Priorisation**              | Allouer plus de priorit√© CPU / IO √† certains groupes plut√¥t que d'autres   |
| **Comptabilit√©**              | Suivre l'utilisation des ressources par groupe                             |
| **Contr√¥le**                  | Suspendre, reprendre ou tuer les processus en vrac                         |
| **Isolement**                 | Emp√™cher la famine des ressources entre les groupes                        |

##### üì¶ Sous-syst√®mes (contr√¥leurs)

CGroup fonctionne via des contr√¥leurs, chacun responsable de la gestion d'un type de ressource:

| Sous-syst√®me | Description                                        |
| ------------ | -------------------------------------------------- |
| `cpu`        | Contr√¥le la planification du processeur            |
| `cpuacct`    | G√©n√®re des rapports d'utilisation du processeur    |
| `memory`     | Limites et utilisation de la m√©moire des comptes   |
| `blkio`      | Limite les E / S de l'appareil de bloc             |
| `devices`    | Contr√¥le l'acc√®s aux appareils                     |
| `freezer`    | Suspend / reprend l'ex√©cution des t√¢ches           |
| `net_cls`    | Paquets de balises pour la mise en forme du trafic |
| `ns`         | G√®re l'acc√®s √† l'espace de noms (rare)             |

##### üìÇ Disposition du syst√®me de fichiers

Les CGROUP sont expos√©s via le syst√®me de fichiers virtuel sous / SYS / FS / CGROUP.

Selon la version:

-   **Cgroups v1**: Hi√©rarchies s√©par√©es pour chaque contr√¥leur (par exemple, m√©moire, CPU, etc.)
-   **Cgroups v2**: hi√©rarchie unifi√©e sous un seul point de montage

Mont√© sous:

```sh
/sys/fs/cgroup/
```

Hi√©rarchie Cgroups V1 typique:

```sh
/sys/fs/cgroup/
‚îú‚îÄ‚îÄ memory/
‚îÇ   ‚îú‚îÄ‚îÄ mygroup/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory.limit_in_bytes
‚îú‚îÄ‚îÄ cpu/
‚îÇ   ‚îî‚îÄ‚îÄ mygroup/
‚îî‚îÄ‚îÄ ...
```

Dans CGroups v2, toutes les ressources sont g√©r√©es sous une hi√©rarchie unifi√©e:

```sh
/sys/fs/cgroup/
‚îú‚îÄ‚îÄ cgroup.procs
‚îú‚îÄ‚îÄ cgroup.controllers
‚îú‚îÄ‚îÄ memory.max
‚îú‚îÄ‚îÄ cpu.max
‚îî‚îÄ‚îÄ ...
```

##### üß™ Utilisation commune (exemples V1 et V2)

V1 - Cr√©er et affecter la limite de m√©moire:

```sh
# Mount memory controller (if needed)
mount -t cgroup -o memory none /sys/fs/cgroup/memory

# Create group
mkdir /sys/fs/cgroup/memory/mygroup

# Set memory limit (100 MB)
echo 104857600 | tee /sys/fs/cgroup/memory/mygroup/memory.limit_in_bytes

# Assign a process (e.g., current shell)
echo $$ | tee /sys/fs/cgroup/memory/mygroup/tasks
```

V2 - Hi√©rarchie unifi√©e:

```sh
# Create subgroup
mkdir /sys/fs/cgroup/mygroup

# Enable controllers
echo +memory +cpu > /sys/fs/cgroup/cgroup.subtree_control

# Move shell into group
echo $$ > /sys/fs/cgroup/mygroup/cgroup.procs

# Set limits
echo 104857600 > /sys/fs/cgroup/mygroup/memory.max
echo "50000 100000" > /sys/fs/cgroup/mygroup/cpu.max  # 50ms quota per 100ms period
```

üß≠ Inspection du processus et du groupe

| Commande                | Description                                    |
| ----------------------- | ---------------------------------------------- |
| `cat /proc/self/cgroup` | Affiche l'appartenance CGROUP actuelle         |
| `cat /proc/PID/cgroup`  | Cgroup d'un autre processus                    |
| `cat /proc/PID/status`  | Informations sur la m√©moire et le groupe       |
| `ps -o pid,cmd,cgroup`  | Afficher la cartographie de processus √† groupe |

##### üì¶ Utilisation dans les conteneurs

Des moteurs √† conteneurs comme Docker, Podman et Containerd Delegate Resource Control aux CGroups (via Runc ou Crun), en autorisant:

-   CPU par contact et limites de m√©moire
-   Contr√¥le √† grains fins sur Blkio et les appareils
-   Comptabilit√© des ressources en temps r√©el

Exemple de docker:

```sh
docker run --memory=256m --cpus=1 busybox
```

Dans les coulisses, cela cr√©e des r√®gles de CGROUP pour la m√©moire et les limites du processeur pour le processus de conteneur.

##### üß† R√©sum√© des concepts

| Concept         | Explication                                                                   |
| --------------- | ----------------------------------------------------------------------------- |
| **Contr√¥leurs** | Modules comme`cpu`,`memory`,`blkio`, etc. Appliquer des limites et des r√®gles |
| **T√¢ches**      | Pids (processus) affect√©s au groupe t√©moin                                    |
| **Hi√©rarchie**  | Les Cgroups sont structur√©s dans un arbre parent-enfant                       |
| **D√©l√©gation**  | Systemd et les services utilisateur peuvent g√©rer les sous-arbres de CGROUPS  |

##### üß™ LAB COMPROUPS

Utilisez ce script pour le laboratoire:[cgroups.sh](scripts/container/cgroups.sh)

Sortie de la m√©moire limite douce:

![cgroups-soft-limit](images/cgroups-soft-limit.png)

* * *

#### üõ°Ô∏è Comprendre les capacit√©s

‚ùì Quelles sont les capacit√©s Linux?

Traditionnellement dans Linux, l'utilisateur racine a un acc√®s illimit√© au syst√®me. Les capacit√©s Linux ont √©t√© introduites pour d√©composer ces privil√®ges tout-puissants en autorisations plus petites et discr√®tes, permettant aux processus d'effectuer des op√©rations privil√©gi√©es sp√©cifiques sans n√©cessiter un acc√®s racine complet.

Cela am√©liore la s√©curit√© du syst√®me en appliquant le principe du moindre privil√®ge.

| üîê Capacit√©            | üìã Description                                                       |
| ---------------------- | -------------------------------------------------------------------- |
| `CAP_CHOWN`            | Modifier le propri√©taire du fichier ind√©pendamment des autorisations |
| `CAP_NET_BIND_SERVICE` | Se lier aux ports inf√©rieurs √† 1024 (par exemple, 80, 443)           |
| `CAP_SYS_TIME`         | D√©finir l'horloge du syst√®me                                         |
| `CAP_SYS_ADMIN`        | ‚ö†Ô∏è tr√®s puissant - comprend la monture, le BPF et plus               |
| `CAP_NET_RAW`          | Utilisez des prises brutes (par exemple, ping, traceroute)           |
| `CAP_SYS_PTRACE`       | Tracez d'autres processus (d√©bogage)                                 |
| `CAP_KILL`             | Envoyer des signaux √† n'importe quel processus                       |
| `CAP_DAC_OVERRIDE`     | Modifier les fichiers et les r√©pertoires sans autorisation           |
| `CAP_SETUID`           | Modifier l'ID utilisateur (UID) du processus                         |
| `CAP_NET_ADMIN`        | G√©rer les interfaces r√©seau, le routage, etc.                        |

üîê Certains types de capacit√©s Linux

| Type de capacit√©      | Description                                                                       |
| --------------------- | --------------------------------------------------------------------------------- |
| **Capinhos (h√©rit√©)** | Capacit√©s h√©rit√©es du processus parent.                                           |
| **CAPPRM (autoris√©)** | Capacit√©s que le processus est autoris√©e √† avoir.                                 |
| **CAPEFF (efficace)** | Capacit√©s que le processus utilise actuellement.                                  |
| **Capbnd (limite)**   | Restreint l'ensemble maximum de capacit√©s efficaces qu'un processus peut obtenir. |
| **Capamb (ambiant)**  | Permet √† un processus de d√©finir explicitement ses propres capacit√©s efficaces.   |

üì¶ Capacit√©s dans les conteneurs et les gousses
Les conteneurs ne fonctionnent g√©n√©ralement pas en tant que racine compl√®te, mais re√ßoivent plut√¥t un ensemble limit√© de capacit√©s par d√©faut en fonction de l'ex√©cution.

Des capacit√©s peuvent √™tre ajout√©es ou supprim√©es dans Kubernetes √† l'aide du SecurityContext.

üìÑ Kubernetes Exemple:

```yaml
securityContext:
  capabilities:
    drop: ["ALL"]
    add: ["NET_BIND_SERVICE"]
```

üîê Cela garantit que le conteneur commence par z√©ro privil√®ges et ne re√ßoit que ce qui est n√©cessaire.

##### üß™ Capacit√©s de laboratoire

Utilisez ce script pour le laboratoire:[capabilities.sh](scripts/container/capabilities.sh)

Sortir:

![capabilities-lab](images/capabilities-lab.png)

* * *

#### 352.1 Commandes importantes

##### sans partage

```sh
# create a new namespaces and run a command in it
unshare --mount --uts --ipc --user --pid --net  --map-root-user --mount-proc --fork chroot ~vagrant/debian bash
# mount /proc for test
#mount -t proc proc /proc
#ps -aux
#ip addr show
#umount /proc
```

##### LSN

```sh
# show all namespaces
lsns

# show only pid namespace
lsns -s <pid>
lsns -p 3669

ls -l /proc/<pid>/ns
ls -l /proc/3669/ns

ps -o pid,pidns,netns,ipcns,utsns,userns,args -p <PID>
ps -o pid,pidns,netns,ipcns,utsns,userns,args -p 3669
```

##### nsenter

```sh
# execute a command in namespace
sudo nsenter -t <PID> -n  ip link show
sudo nsenter -t 3669 -n ip link show
```

##### 252.1 IP

```sh
# create a new network namespace
sudo ip netns add lxc1

# list network list
ip netns list

# exec command in network namespace
sudo ip netns exec lxc1 ip addr show
```

##### stat

```sh
# get cgroup version
stat -fc %T /sys/fs/cgroup
```

##### SystemCTL et SystemD

```sh
# get cgroups of system
systemctl status
systemd-cgls
```

##### cgcreate

```sh
cgcreate -g memory,cpu:lsf
```

##### cgClassify

```sh
cgclassify -g memory,cpu:lsf <PID>
```

##### setCap Cap_net_raw = ep / usr / bin / tcpdump

```sh

```

##### getCap / usr / bin / tcpdump

```sh

```

##### Capsh - Capacit√© enveloppe de coquille

* * *

<p align="right">(<a href="#topic-352.1">back to sub topic 352.1</a>)</p>
<p align="right">(<a href="#topic-352">back to topic 352</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-352.2"></a>

### 352.2 LXC

**Poids:**6

**Description:**Les candidats doivent pouvoir utiliser des conteneurs syst√®me √† l'aide de LXC et LXD. La version de LXC couverte est de 3,0 ou plus.

**Zones de connaissances cl√©s:**

-   Comprendre l'architecture de LXC et LXD
-   G√©rer les conteneurs LXC en fonction des images existantes √† l'aide de LXD, y compris la mise en r√©seau et le stockage
-   Configurer les propri√©t√©s du conteneur LXC
-   Limiter l'utilisation des ressources de conteneur LXC
-   Utiliser les profils LXD
-   Comprendre les images LXC
-   Conscience des outils LXC traditionnels

#### 352.2 objets cit√©s

```sh
lxd
lxc (including relevant subcommands)
```

#### 352.2 Commandes importantes

##### foo

```sh
foo
```

<p align="right">(<a href="#topic-352.2">back to sub topic 352.2</a>)</p>
<p align="right">(<a href="#topic-352">back to topic 352</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-352.3"></a>

### 352.3 Docker

**Poids:**9

**Description:**Le candidat doit √™tre en mesure de g√©rer les n≈ìuds Docker et les conteneurs Docker. Cela inclut la compr√©hension de l'architecture de Docker ainsi que la compr√©hension de la fa√ßon dont Docker interagit avec le syst√®me Linux du n≈ìud.

**Zones de connaissances cl√©s:**

-   Comprendre l'architecture et les composants de Docker
-   G√©rer les conteneurs Docker en utilisant des images √† partir d'un registre Docker
-   Comprendre et g√©rer les images et les volumes pour les conteneurs Docker
-   Comprendre et g√©rer la journalisation des conteneurs Docker
-   Comprendre et g√©rer le r√©seautage pour Docker
-   Utilisez dockerfiles pour cr√©er des images de conteneurs
-   Ex√©cutez un registre Docker √† l'aide de l'image du registre Docker

#### 352.3 objets cit√©s

```sh
dockerd
/etc/docker/daemon.json
/var/lib/docker/
docker
Dockerfile
```

#### 352.3 Commandes importantes

##### docker

```sh
# Examples of docker
```

<p align="right">(<a href="#topic-352.3">back to sub topic 352.3</a>)</p>
<p align="right">(<a href="#topic-352">back to topic 352</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-352.4"></a>

### 352.4 plates-formes d'orchestration en conteneurs

**Poids:**3

**Description:**Les candidats doivent comprendre l'importance de l'orchestration des conteneurs et les concepts cl√©s Docker Swarm et Kubernetes fournissent pour impl√©menter l'orchestration des conteneurs.

**Zones de connaissances cl√©s:**

-   Comprendre la pertinence de l'orchestration des conteneurs
-   Comprendre les concepts cl√©s de Docker Compose et Docker Swarm
-   Comprendre les concepts cl√©s de Kubernetes et Helm
-   Sensibilisation de l'OpenShift, du Rancher et de la Mesosph√®re DC / OS

<p align="right">(<a href="#topic-352.4">back to sub topic 352.4</a>)</p>
<p align="right">(<a href="#topic-352">back to topic 352</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-353"></a>

## Sujet 353: d√©ploiement et provisionnement VM

* * *

<a name="topic-353.1"></a>

### 353.1 outils de gestion du cloud

**Poids:**2

**Description:**Les candidats doivent comprendre les offres communes dans les nuages ‚Äã‚Äãpublics et avoir une connaissance de base des fonctionnalit√©s des outils de gestion du cloud couramment disponibles.

**Zones de connaissances cl√©s:**

-   Comprendre les offres communes dans les nuages ‚Äã‚Äãpublics
-   Connaissance de base des fonctionnalit√©s d'OpenStack
-   Connaissance des fonctionnalit√©s de base de Terraform
-   Conscience de Cloudstack, Eucalyptus et Opennebula

#### 353.1 objets cit√©s

```sh
IaaS, PaaS, SaaS
OpenStack
Terraform
```

#### 353.1 Commandes importantes

##### foo

```sh
# examples
```

<p align="right">(<a href="#topic-353.1">back to sub topic 353.1</a>)</p>
<p align="right">(<a href="#topic-353">back to topic 353</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-353.2"></a>

### 353.2 Packer

**Poids:**2

**Description:**Les candidats devraient pouvoir utiliser Packer pour cr√©er des images syst√®me. Cela comprend l'ex√©cution de Packer dans divers environnements cloud publics et priv√©s ainsi que la construction d'images de conteneurs pour LXC / LXD.

**Zones de connaissances cl√©s:**

-   Comprendre les fonctionnalit√©s et les fonctionnalit√©s de Packer
-   Cr√©er et maintenir des fichiers de mod√®le
-   Cr√©er des images √† partir de fichiers de mod√®le √† l'aide de diff√©rents constructeurs

#### 353,2 objets cit√©s

```sh
packer
```

#### 353.2 Commandes importantes

##### emballeur

```sh
# examples
```

<p align="right">(<a href="#topic-353.2">back to sub topic 353.2</a>)</p>
<p align="right">(<a href="#topic 353">back to topic 353</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-353.3"></a>

### 353.3 Cloud-init

**Poids:**3

**Description:**Les candidats doivent utiliser le cloud-init pour configurer des machines virtuelles cr√©√©es √† partir d'images standardis√©es. Cela comprend l'ajustement des machines virtuelles pour correspondre √† leurs ressources mat√©rielles disponibles, en particulier, l'espace disque et les volumes.  
De plus, les candidats devraient pouvoir configurer des instances pour permettre des connexions SSH s√©curis√©es et installer un ensemble sp√©cifique de packages logiciels.  
De plus, les candidats devraient √™tre en mesure de cr√©er de nouvelles images syst√®me avec un support Cloud-INT.

**Zones de connaissances cl√©s:**

-   Comprendre les fonctionnalit√©s et les concepts du cloud-init, y compris les donn√©es utilisateur, l'initialisation et la configuration du cloud-init
-   Utilisez le cloud-init pour cr√©er, redimensionner et monter des syst√®mes de fichiers, configurer les comptes d'utilisateurs, y compris les informations d'identification de connexion telles que les cl√©s SSH et installer des packages de logiciels √† partir du r√©f√©rentiel de la distribution
-   Int√©grer le cloud-init dans les images du syst√®me
-   Utilisez la source de donn√©es de configuration pour les tests

#### 353.3 objets cit√©s

```sh
cloud-init
user-data
/var/lib/cloud/
```

#### 353.3 Commandes importantes

##### foo

```sh
# examples
```

<p align="right">(<a href="#topic-353.3">back to sub topic 353.3</a>)</p>
<p align="right">(<a href="#topic 353">back to topic 353</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<a name="topic-353.4"></a>

### 353,4 vagabond

**Poids:**3

**Description:**Le candidat doit √™tre en mesure d'utiliser Vagrant pour g√©rer les machines virtuelles, y compris l'approvisionnement de la machine virtuelle.

**Zones de connaissances cl√©s:**

-   Comprendre l'architecture et les concepts vagabonds, y compris le stockage et le r√©seautage
-   R√©cup√©rez et utilisez des bo√Ætes √† partir de l'atlas
-   Cr√©er et ex√©cuter Vagrantfiles
-   Acc√©der aux machines virtuelles vagues
-   Partagez et synchronisez le dossier entre une machine virtuelle Vagrant et le syst√®me h√¥te
-   Comprendre l'approvisionnement vagabond, c'est-√†-dire les provisions de fichiers et de shell
-   Comprendre la configuration multi-machine

#### 353.4 objets cit√©s

```sh
vagrant
Vagrantfile
```

#### 353.4 Commandes importantes

##### vagabond

```sh
# examples
```

<p align="right">(<a href="#topic-353.4">back to sub topic 353.4</a>)</p>
<p align="right">(<a href="#topic 353">back to topic 353</a>)</p>
<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

## Contributif

Les contributions font de la communaut√© open source un endroit incroyable
Apprenez, inspirez et cr√©ez. Toutes les contributions que vous faites sont**tr√®s appr√©ci√©**.

Si vous avez une suggestion qui rendrait cela meilleur, veuillez enfiler le repo et
Cr√©ez une demande de traction. Vous pouvez √©galement ouvrir simplement un probl√®me avec la balise "am√©lioration".
N'oubliez pas de donner une √©toile au projet! Merci encore!

1.  Fourk le projet
2.  Cr√©ez votre branche de fonctionnalit√© (`git checkout -b feature/AmazingFeature`)
3.  Engagez vos modifications (`git commit -m 'Add some AmazingFeature'`)
4.  Pousser √† la branche (`git push origin feature/AmazingFeature`)
5.  Ouvrir une demande de traction

* * *

## Licence

-   Ce projet est sous licence en vertu de la licence MIT \* voir le fichier licence.md pour plus de d√©tails

* * *

## Contact

Marcos Silvestrini -[marcos.silvestrini@gmail.com](mailto:marcos.silvestrini@gmail.com)\\[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/mrsilvestrini.svg?style=social&label=Follow%20%40mrsilvestrini)](https://twitter.com/mrsilvestrini)

Lien du projet:<https://github.com/marcossilvestrini/learning-lpic-3-305-300>

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

## Remerciements

-   [Richard Stallman's](http://www.stallman.org/)
-   [GNOU](<>)
    -   [FAQ GNU / Linux par Richard Stallman](https://www.gnu.org/gnu/gnu-linux-faq.html)
    -   [GNOU](https://www.gnu.org/)
    -   [Syst√®me d'exploitation GNU](https://www.gnu.org/gnu/thegnuproject.html)
    -   [Compilateur GCC](https://gcc.gnu.org/wiki/History)
    -   [Goudron de gnu](https://www.gnu.org/software/tar/)
    -   [GNU FAIT](https://www.gnu.org/software/make/)
    -   [GNU Emacs](https://en.wikipedia.org/wiki/Emacs)
    -   [Forfaits GNU](https://www.gnu.org/software/)
    -   [Collection GNU / Linux](https://directory.fsf.org/wiki/Collection:GNU/Linux)
    -   [T√©l√©chargeur de d√©marrage GNU GRUB](https://www.gnu.org/software/grub/)
    -   [GNU HURD](https://www.gnu.org/software/hurd/hurd/what_is_the_gnu_hurd.html)
-   [Noyau](<>)
    -   [Noyau](https://www.kernel.org/)
    -   [Pages de noyau Linux](https://www.kernel.org/doc/man-pages/)
    -   [Compilez votre noyau](https://wiki.linuxquestions.org/wiki/How_to_build_and_install_your_own_Linux_kernel)
-   [Base standard Linux](<>)
    -   [Base standard Linux](https://en.wikipedia.org/wiki/Linux_Standard_Base)
    -   [Norme de hi√©rarchie du syst√®me de fichiers](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard)
    -   [Structure de hi√©rarchie de fichiers](https://refspecs.linuxfoundation.org/FHS_3.0/fhs-3.0.pdf)
-   [Logiciel gratuit](<>)
    -   [FSF](https://www.fsf.org)
    -   [R√©pertoire de logiciel gratuit](https://directory.fsf.org/wiki/Free_Software_Directory:Free_software_replacements)
-   [Licence](<>)
    -   [Logiciel gratuit](https://www.gnu.org/philosophy/free-sw.html)
    -   [Copyleft](https://www.gnu.org/licenses/copyleft.en.html)
    -   [GPL](https://www.gnu.org/licenses/quick-guide-gplv3.html)
    -   [Licence publique g√©n√©rale GNU moindre](https://www.gnu.org/licenses/lgpl-3.0.html)
    -   [BSD](https://opensource.org/licenses/BSD-3-Clause)
    -   [Initiative open source](https://opensource.org/)
    -   [Communes cr√©atives](https://creativecommons.org/)
    -   [Licence LTS](https://en.wikipedia.org/wiki/Long-term_support)
-   [Distros](<>)
    -   [Directives de Debian Free Software](https://www.debian.org/social_contract#guidelines)
    -   [Liste de la distribution Linux](https://en.wikipedia.org/wiki/List_of_Linux_distributions)
    -   [Destrowatch](https://distrowatch.com/)
    -   [Comparaison Distributions Linux](https://en.wikipedia.org/wiki/Comparison_of_Linux_distributions)
-   [Environnements de bureau](<>)
    -   [X11 org](https://www.x.org/wiki/)
    -   [Wayland](https://wayland.freedesktop.org/)
    -   [GNU GNOME](https://www.gnu.org/press/gnome-1.0.html)
    -   [GNOME](https://www.gnome.org/)
    -   [Xfce](https://xfce.org/)
    -   [O√π le plasma](https://kde.org/plasma-desktop/)
    -   [Harmonie](https://en.wikipedia.org/wiki/Harmony_(toolkit))
-   [Protocoles](<>)
    -   [Http](<>)
        -   [W3techs](https://w3techs.com/)
        -   [Apache](https://www.apache.org/)
        -   [Apache Directives][def]
        -   [Codes d'√©tat HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)
        -   [Ciphers forts pour Apache, Nginx et LightTPD](https://cipherlist.eu/)
        -   [Tutoriels SSL](https://www.golinuxcloud.com/blog/)
        -   [SSL Config Mozilla](https://ssl-config.mozilla.org/)
    -   [xrdp](https://bytexd.com/xrdp-centos/)
    -   [NTP](https://www.ntppool.org/en/)
-   [DNS](<>)
    -   [Lier](https://www.isc.org/bind/)
    -   [Lier la journalisation](https://www.zytrax.com/books/dns/ch7/logging.html)
    -   [Liste des types d'enregistrements DNS](https://en.wikipedia.org/wiki/List_of_DNS_record_types)
    -   [Liste des types d'enregistrements DNS](https://en.wikipedia.org/wiki/List_of_DNS_record_types)
-   [Gestionnaire de packages](<>)
    -   [T√©l√©charger des packages](https://pkgs.org/)
    -   [Installer des packages](https://installati.one/)
    -   [Guide des packages d'installation](https://installati.one/)
-   [Script shell](<>)
    -   [Bourne √† nouveau Shell](https://www.gnu.org/software/bash/manual/)
    -   [Case](https://bash.cyberciti.biz/guide/Shebang)
    -   [Variables d'environnement](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)
    -   [GNU GLOBBING](https://man7.org/linux/man-pages/man7/glob.7.html)
    -   [Global](https://linuxhint.com/bash_globbing_tutorial/)
    -   [Citation](https://www.gnu.org/software/bash/manual/html_node/Quoting.html)
    -   [Expressions r√©guli√®res](https://www.gnu.org/software/grep/manual/html_node/Regular-Expressions.html)
    -   [Commande non trouv√©e](https://command-not-found.com/)
    -   [G√©n√©rateur d'invite de bash](https://bash-prompt-generator.org/)
    -   [Expliquer](https://explainshell.com/)
    -   [Tutoriel VIM](https://www.openvim.com/)
    -   [Tutoriel de script de shell Linux](https://bash.cyberciti.biz/guide/Main_Page)
    -   [Commandes Exemples](https://www.geeksforgeeks.org/)
-   [Autres outils](<>)
    -   [Bugzila](https://bugzilla.kernel.org/)
    -   [Badges Github](https://github.com/alexandresanlim/Badges4-README.md-Profile)
-   [D√©finitions de virtualisation](<>)
    -   [Chapeau rouge](https://www.redhat.com/pt-br/topics/virtualization/what-is-virtualization/)
    -   [AWS](https://aws.amazon.com/pt/what-is/virtualization/)
    -   [Ibm](https://www.ibm.com/topics/virtualization)
    -   [OpenSource.com](https://opensource.com/resources/virtualization)
-   [Alterner](<>)
    -   [Xenserver](https://www.xenserver.com/)
    -   [Wiki xenproject](https://wiki.xenproject.org/wiki/Main_Page)
    -   [Interfaces r√©seau](https://wiki.xenproject.org/wiki/Xen_Networking#Virtual_Network_Interfaces)
    -   [Outils Xen](https://xen-tools.org/software/)
    -   [Blog LPI: Virtualisation Xen et cloud computing # 01: Introduction](https://www.lpi.org/pt-br/blog/2020/10/01/xen-virtualization-and-cloud-computing-01-introduction/)
    -   [Blog LPI: Xen Virtualisation et cloud computing # 02: Comment Xen fait le travail](https://www.lpi.org/blog/2020/10/08/xen-virtualization-and-cloud-computing-02-how-xen-does-job/)
    -   [Blog LPI: Virtualisation Xen et cloud computing # 04: conteneurs, OpenStack et autres plateformes connexes](https://www.lpi.org/pt-br/blog/2020/10/22/xen-virtualization-and-cloud-computing-04-containers-openstack-and-other-related/)
    -   [Virtualisation Xen et cloud computing # 05: The Xen Project, Unikernell et l'avenir](https://www.lpi.org/pt-br/blog/2020/10/29/xen-virtualization-and-cloud-computing-05-xen-project-unikernels-and-future/)
    -   [Guide des d√©butants du projet Xen](https://wiki.xenproject.org/wiki/Xen_Project_Beginners_Guide#Installing_the_Xen_Project_Software)
    -   [Livre fou](https://wiki.xenproject.org/wiki/Book/HelloXenProject/0-Contents)
-   [Unicernel](https://www.lpi.org/blog/2020/10/29/xen-virtualization-and-cloud-computing-05-xen-project-unikernels-and-future/)
    -   [Force unique](https://github.com/unikraft/unikraft)
    -   [Mirageos](https://mirage.io/docs/hello-world)
    -   [Mauvais](https://galois.com/project/halvm/)
    -   [Unique](https://github.com/solo-io/unik/blob/master/docs/providers/virtualbox.md)
-   [Kvm](<>)
    -   [Officier Doc](https://linux-kvm.org/page/Main_Page)
    -   [KVM (machines virtuelles du noyau par redhat)](https://www.redhat.com/pt-br/topics/virtualization/what-is-KVM)
    -   [Outils de gestion KVM](https://www.linux-kvm.org/page/Management_Tools)
    -   [R√©seau KVM](https://www.linux-kvm.org/page/Networking)
-   [Qemu](<>)
    -   [Officier Doc](https://www.qemu.org/)
    -   [T√©l√©charger des images Osboxes](https://www.osboxes.org/)
    -   [T√©l√©charger des images LinuxImages](https://www.linuxvmimages.com/)
    -   [Urbain](https://en.wikibooks.org/wiki/QEMU/Devices/Virtio)
    -   [Agent invit√©](https://wiki.qemu.org/Features/GuestAgent)
-   [Libvirt](<>)
    -   [Officier Doc](https://libvirt.org/)
    -   [Activation du socket syst√®me](https://libvirt.org/manpages/libvirtd.html#system-socket-activation)
    -   [Relations](https://libvirt.org/uri.html)
    -   [Stockage](https://libvirt.org/storage.html)
    -   [R√©seau](https://wiki.libvirt.org/Networking.html)
    -   [VirtualNetwork](https://wiki.libvirt.org/VirtualNetworking.html)
    -   [verrogd](https://libvirt.org/manpages/virtlogd.html)
    -   [Virtlockd](https://libvirt.org/manpages/virtlockd.html)
    -   [manager](https://virt-manager.org/)
-   [Gestion du disque](<>)
    -   [Images disque](https://qemu-project.gitlab.io/qemu/system/images.html)
    -   [copie-√©criture](https://sempreupdate.com.br/linux/tutoriais/sistema-de-arquivos-copy-on-write-saiba-o-que-e-e-quais-as-vantagens-e-desvantagens/)
    -   [RAM X QCOW2](https://docs.redhat.com/en/documentation/red_hat_virtualization/4.3/html/technical_reference/qcow2)
    -   [Libguestfs](https://libguestfs.org/)
-   [Virtualisation et contenerisation](<>)
    -   [Conteneurs AWS Doc](https://aws.amazon.com/pt/containers/)
    -   [Conteneurs Doc GCP](https://cloud.google.com/learn/what-are-containers?hl=pt-br)
    -   [Conteneur IBM Doc](https://www.ibm.com/br-pt/topics/containers)
    -   [Conteneurs Docs Red Hat](https://www.redhat.com/en/topics/containers/whats-a-linux-container)
    -   [Espaces de noms](https://manpages.ubuntu.com/manpages/noble/man7/namespaces.7.html)
    -   [Espaces de noms les plus importants](https://www.redhat.com/en/blog/7-linux-namespaces)
    -   [Cours de Cgroups](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/resource_management_guide/ch01)
    -   [Homme cgroupes](https://manpages.ubuntu.com/manpages/noble/man7/cgroups.7.html)
    -   [Capacit√©s Doc](https://linux-audit.com/kernel/capabilities/linux-capabilities-101/)
    -   [Capacit√©s de l'homme](https://manpages.ubuntu.com/manpages/noble/man7/capabilities.7.html)
    -
-   [Docs OpenStack](<>)
    -   [Redhat](https://www.redhat.com/pt-br/topics/openstack)
-   [VSWitch ouvert](<>)
    -   [Ovs doc 4linux](https://blog.4linux.com.br/open-vswitch-o-que-e-o-que-come-onde-vive)
-   [Examen LPIC-3 305-300](<>)
    -   [LPIC-3 305-300 Objectifs](https://www.lpi.org/our-certifications/exam-305-objectives/)
    -   [LPIC-3 305-300 Wiki](https://wiki.lpi.org/wiki/LPIC-305_Objectives_V3.0)
    -   [LPIC-3 305-300 Mat√©riel d'apprentissage](https://cursos.linuxsemfronteiras.com.br/courses/preparatorio-para-certificacao-lpic-3-305/)
    -   [LPIC-3 305-300 Examen simul√© par iTexams](https://www.itexams.com/info/305-300)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

* * *

<!-- MARKDOWN LINKS & IMAGES-->

<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->

[contributors-shield]: https://img.shields.io/github/contributors/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[contributors-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/graphs/contributors

[forks-shield]: https://img.shields.io/github/forks/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[forks-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/network/members

[stars-shield]: https://img.shields.io/github/stars/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[stars-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/stargazers

[issues-shield]: https://img.shields.io/github/issues/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[issues-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/issues

[license-shield]: https://img.shields.io/github/license/marcossilvestrini/learning-lpic-3-305-300.svg?style=for-the-badge

[license-url]: https://github.com/marcossilvestrini/learning-lpic-3-305-300/blob/master/LICENSE

[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555

[linkedin-url]: https://linkedin.com/in/marcossilvestrini

[def]: https://httpd.apache.org/docs/2.4/mod/directives.html
